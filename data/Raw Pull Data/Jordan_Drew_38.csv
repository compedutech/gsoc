pr_id,pr_title,pr_body,is_merged,pr_number,pr_url,pr_html_url,pr_state,additions,deletions,pr_changed_files,pr_commits_count,pr_comments_count,pr_review_comments_count,pr_labels_count,pr_assignees_count,pr_labels,pr_created_at,pr_closed_at,time_taken,time_delta,pr_review_comments,pr_commits,contributor,contributor_id,contributor_email,contributor_type,contributions,contributor_public_repos,contributor_private_repos,contributor_followings,contributor_followers
967515821,[GSOC] implement example of state-space model for connectivity,"# PR Description

## Google Summer of Code (2022) project 

Closes #99 

WIP: Linear Dynamic System (state-space model using EM algorithm to find autoregressive coefficients) to infer functional connectivity by interpreting autoregressive coefficients as connectivity strength. The model uses M/EEG data as input, and outputs time-varying autoregressive coefficients for source space labels.

### Completed during GSoC

- [x] A user-friendly API that allows the user to work easily with MEG and/or EEG EEG data, following MNE-Python's local standards and conventions for usability as much as possible:
    - Most of the code complexity is hidden from the user in the backend for the simplest interface:
    
```
data_path = mne.datasets.sample.data_path()

raw = mne.io.read_raw_fif(raw_fname).crop(tmax=60)
events = mne.find_events(raw, ...)

event_dict = {...}
epochs = mne.Epochs(raw, events, tmin=-0.2, tmax=0.7, event_id=event_dict,
                    preload=True).pick_types(meg=True,eeg=True)


fwd_fname = sample_folder / '....'
fwd = mne.read_forward_solution(fwd_fname)

cov_fname = sample_folder / 'sample_audvis-cov.fif'
cov = mne.read_cov(cov_fname)

label_names = ['Aud-lh', 'Aud-rh', 'Vis-lh', 'Vis-rh']
labels = [mne.read_label(sample_folder / 'labels' / f'{label}.label',
                          subject='sample') for label in label_names]

model = LDS(lam0=0, lam1=100)
model.add_subject('sample', condition, epochs, labels, fwd, cov)
model.fit(...)
model.fit()
At = model.A
assert At.shape == (len(labels), len(labels), len(epochs.times))
```

- [x] Preprocess it to a format meant to increase the SNR of the data
- [x] Downsample the data for faster processing.
- [x] Utilizes forward and covariance matrices in the API well as the labels for the regions of interest (ROIs) of the dataset
- [x] PCA is used to reduce the dimensionality of the data
- [x] `model.fit` using the Expectation Maximization algorithm to fit the autoregressive coefficients of the state-space model, mapping the sensor data to each ROI, and computing the connectivity strength between ROI pairs
- [x] Plotting of the time-varying coefficients in a matrix format to observe the strength of connection between each pair of ROIs
- [x] An example script showing how to use the function and interpret its outputs using the MNE-Python `sample` dataset
- [x] Basic unit tests have been written and partially incorporated

Check-out this [link](https://blogs.python-gsoc.org/en/jadrew43s-blog/) to see my weekly progress. All of the code in this PR is new to MNE-Python's repositories. 

### Todo after GSoC

- [ ] Finish unit tests
- [ ] Finish replacing redundant functionality with MNE-Python equivalents (e.g., `scale_data` with `compute_whitener` and dot products)
- [ ] The current implementation uses `autograd` as a dependency, which is no longer actively developed (but still maintained). The code should be updated to incorporate `JAX`, which includes the features of `autograd` and is being actively developed (`autograd_linalg` should be replaced by `scipy.linalg`).

## Merge checklist

Maintainer, please confirm the following before merging:

- [ ] All comments resolved
- [ ] This is not your own PR
- [ ] All CIs are happy
- [ ] PR title starts with [MRG]
- [ ] [whats_new.rst](https://github.com/mne-tools/mne-connectivity/blob/main/doc/whats_new.rst) is updated
- [ ] PR description includes phrase ""closes <#issue-number>""
",False,100,https://api.github.com/repos/mne-tools/mne-connectivity/pulls/100,https://github.com/mne-tools/mne-connectivity/pull/100,open,2478,0,12,18,21,51,0,0,[],2022-06-14 23:52:22+00:00,,0.0,,"[{'comment_id': 900102379, 'comment_body': ""I'm in favor of naming these 1 letter variable names explicitly: e.g. `forward_lin_projection`, and then specify in the docstring that this corresponds to the `G` variable in the publication.\r\n\r\nAnd so on for the other variables too."", 'comment_created': datetime.datetime(2022, 6, 17, 13, 10, 53, tzinfo=datetime.timezone.utc), 'commenter': 'adam2392', 'type': 'User'}, {'comment_id': 900104014, 'comment_body': ""Not needed since we'll use Python3.7+"", 'comment_created': datetime.datetime(2022, 6, 17, 13, 12, 58, tzinfo=datetime.timezone.utc), 'commenter': 'adam2392', 'type': 'User'}, {'comment_id': 900520565, 'comment_body': ""To me the `ROIToSourceMap` and `scale_sensor_data` should just be hidden steps in the `model.fit` step, no? Does a user really need access to these?\r\n\r\nThis would allow `MEGLDS` (we should probably rename it just `LDS` or something since it's not MEG-specific at all) to have all the properties of ROIToSourceMap internally"", 'comment_created': datetime.datetime(2022, 6, 17, 21, 54, 12, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 900520886, 'comment_body': ""This should be a util function in mne-python if really necessary. It's not (MEG)LDS-specific. I even think we can avoid having it altogether in the first version of this algorithm, right?"", 'comment_created': datetime.datetime(2022, 6, 17, 21, 54, 59, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 900521453, 'comment_body': 'This is exactly what you would get if you whitened the gain matrix with a diagonal noise cov...', 'comment_created': datetime.datetime(2022, 6, 17, 21, 56, 25, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 900521554, 'comment_body': '... and rescaled a covariance by a diagonal noise cov whitener...', 'comment_created': datetime.datetime(2022, 6, 17, 21, 56, 43, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 900522127, 'comment_body': ""... and scaled data by a diagonal noise cov whitener.\r\n\r\nSo in the end I don't think we need this function at all. Instead we can do something like:\r\n```\r\ncov = mne.make_ad_hoc_cov(...)\r\nwhitener = compute_whitener(cov, ...)\r\n```\r\nand then internally whenever we need scaled data in (MEG)LDS we just `whitener @ data` or `whitener @ cov @ whitener.T` on the fly (or store as a private attr to avoid recomputation.\r\n\r\nCan you play around with these functions to see if you agree it should be equivalent?"", 'comment_created': datetime.datetime(2022, 6, 17, 21, 58, 17, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 900528422, 'comment_body': ""Let's remove this for now, does not seem relevant/needed"", 'comment_created': datetime.datetime(2022, 6, 17, 22, 17, 6, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 900532558, 'comment_body': ""We probably don't really need this function. Or at least it doesn't need to be public. In other words, it should have an underscore prefix `_` prepended to the name to make it clear that it's not intended for users to ever see or use it."", 'comment_created': datetime.datetime(2022, 6, 17, 22, 30, 15, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 900532589, 'comment_body': 'See how we construct examples/tutorials with proper titles etc.', 'comment_created': datetime.datetime(2022, 6, 17, 22, 30, 22, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 900532591, 'comment_body': 'See how we do it in other examples/tutorials in MNE-Python using `data_path`', 'comment_created': datetime.datetime(2022, 6, 17, 22, 30, 22, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 900532681, 'comment_body': ""To me this is where it should be `model = LDS(fwd, labels, noise_cov, epochs, subjects_dir)`. Or if we're planning on the model being fit over multiple subjects:\r\n```\r\nmodel = LDS(..., subjects_dir=None)\r\nmodel.add_subject(subject, fwd, labels, noise_cov, epochs)\r\nmodel.fit()\r\n```\r\nand at the `add_subject` stage it could do all the whitening/rescaling etc. And at the `.fit` stage it does all the heavy lifting. And the `...` above could be all the stuff about lambdas, `random_state` support, etc. We'd have to see what the minimum set of parameters is we need for the `LDS` class..."", 'comment_created': datetime.datetime(2022, 6, 17, 22, 30, 43, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 902020713, 'comment_body': 'should this be in the docstring of the example script of of `mne_util.py`?', 'comment_created': datetime.datetime(2022, 6, 20, 22, 3, 36, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 902030743, 'comment_body': 'The docstring sections on tutorials seem to just be authors and a statement about the license. Happy to edit the author statement, but unsure if I should include ""License BSD (3-Clause))"" like in other tutorials. Thoughts?', 'comment_created': datetime.datetime(2022, 6, 20, 22, 33, 11, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 902031246, 'comment_body': ""I have the normal definition for `data_path` (as defined in tutorials) commented out on the next line, I just use this^^ definition so it doesn't keep downloading to my machine."", 'comment_created': datetime.datetime(2022, 6, 20, 22, 34, 48, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 902867112, 'comment_body': 'no. The docstring of `class ROIToSourceMap` should document the parameters of its `__init__` method, and you could include a `Notes` section there that connected the (explicit/descriptive) method and property names to the (brief/opaque) variable names used in the literature. Something like:\r\n\r\n```\r\nNotes\r\n-----\r\n\r\n- The property ``whatever`` corresponds to the matrix ``G`` in :footcite:`BibtexKey`\r\n```', 'comment_created': datetime.datetime(2022, 6, 21, 17, 8, 48, tzinfo=datetime.timezone.utc), 'commenter': 'drammock', 'type': 'User'}, {'comment_id': 902868267, 'comment_body': 'license is implied / specified at the repo level, no need to include it in a tutorial doc.', 'comment_created': datetime.datetime(2022, 6, 21, 17, 10, 9, tzinfo=datetime.timezone.utc), 'commenter': 'drammock', 'type': 'User'}, {'comment_id': 902868972, 'comment_body': ""that's not how `data_path` works :)  it's smart enough to look to see if you already have the dataset downloaded; it won't re-download every time."", 'comment_created': datetime.datetime(2022, 6, 21, 17, 10, 56, tzinfo=datetime.timezone.utc), 'commenter': 'drammock', 'type': 'User'}, {'comment_id': 902953820, 'comment_body': ""It looks like it wants to download to a different folder than where I keep this project. Can I change the default download path? I don't want to change `_download_mne_dataset()` but is there a global variable I can change?"", 'comment_created': datetime.datetime(2022, 6, 21, 18, 46, 11, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 902963427, 'comment_body': ""If you need to work with a local copy because you're modifying the data files multiple times, then you can specify the `path` argument.\r\n\r\nSee: https://mne.tools/stable/generated/mne.datasets.sample.data_path.html\r\n\r\nIf you're not touching the data files, I would just leave as is actually cuz you can easily access the path."", 'comment_created': datetime.datetime(2022, 6, 21, 18, 58, tzinfo=datetime.timezone.utc), 'commenter': 'adam2392', 'type': 'User'}, {'comment_id': 902970492, 'comment_body': 'Yup just found that page :) thanks', 'comment_created': datetime.datetime(2022, 6, 21, 19, 6, 15, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 903084093, 'comment_body': ""@drammock There are several variables defined after the `__init__` but still within the class, i.e. line 90 above. Can I still include them in the class's note section?"", 'comment_created': datetime.datetime(2022, 6, 21, 21, 27, 30, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 903095160, 'comment_body': ""You can add more bullets to the notes section, just copy the example I gave for `G` (the dash at the start of the line will make it a bulleted list, so just add more lines like that).  I'm actually not 100% sure whether the best place is a big bulleted list in `Notes` of the class docstring, or separate entries in docstrings for each property.  It shouldn't be too much work to switch it later, so for now just put everything in the class docstring `Notes` section."", 'comment_created': datetime.datetime(2022, 6, 21, 21, 45, 43, tzinfo=datetime.timezone.utc), 'commenter': 'drammock', 'type': 'User'}, {'comment_id': 905508707, 'comment_body': ""Ok cool thanks! What is the standard method for building the citations? I have used Zotero for papers in the past, I'm wondering if you all have a method you rely on. I'll try to get `references.bib` uploaded tonight so you can see what the Zotero output looks like."", 'comment_created': datetime.datetime(2022, 6, 23, 22, 6, 30, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 905598202, 'comment_body': ""Exporting a bibtex entry from Zotero is usually fine, though there may be extraneous fields you'll want to delete (like file path to your pdf copy). See how they look in `doc/references.bib` in the MNE-Python repo "", 'comment_created': datetime.datetime(2022, 6, 23, 23, 50, 50, tzinfo=datetime.timezone.utc), 'commenter': 'drammock', 'type': 'User'}, {'comment_id': 907881149, 'comment_body': ""With `scales = 1` the `scale_sensor_data()` is not necessary. Should we leave it as part of LDS in case users want to change the scales? \r\n\r\nAlso, I'm not following how the steps in `scale_sensor_data()` is the same as the whitening functions you've mentioned. Can you checkout my previous push with commit `does scale_sensor_data() == whitening?` for lines 64-72 in `state_space_connectivity.py()` to see if I'm following you correctly?"", 'comment_created': datetime.datetime(2022, 6, 27, 23, 28, 47, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 911471447, 'comment_body': ""The matrices that are returned from the above two functions are used as part of `subject_data` to add subject's data to the model. If we are to have the function `model.add_subject` here then we will also need these functions to return these variables. I am open to discussing other methods of going about this. "", 'comment_created': datetime.datetime(2022, 6, 30, 21, 52, 36, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 917055105, 'comment_body': ""You shouldn't need to worry about dropping 2 here (presumably due to `bads`?), it should be done internally in `model.add_subject` or `model.fit` when it actually uses the cov"", 'comment_created': datetime.datetime(2022, 7, 8, 18, 33, 51, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 917055438, 'comment_body': 'Move all of these files from `examples` into `mne_connectivity/state_space/*.py` for example', 'comment_created': datetime.datetime(2022, 7, 8, 18, 34, 24, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 917056507, 'comment_body': ""I don't think it should need to take `num_rois` or `timepts`. It can get this information from the first `add_subject` call, and then after the first `add_subject` any subsequent `add_subject` call should check to make sure the same time points and ROIs are used"", 'comment_created': datetime.datetime(2022, 7, 8, 18, 36, 11, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 917056781, 'comment_body': 'Better to use `with plt.rc_context` here so the global state is not changed\r\n\r\nhttps://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.rc_context.html', 'comment_created': datetime.datetime(2022, 7, 8, 18, 36, 43, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 917057017, 'comment_body': ""We should probably call the attribute `model.At` if it's really time-varying `A`"", 'comment_created': datetime.datetime(2022, 7, 8, 18, 37, 10, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 917058096, 'comment_body': '... specifically I guess this `run_pca_on_subject` function should take into account channel mismatches between `cov`, `fwd`, and `epochs`', 'comment_created': datetime.datetime(2022, 7, 8, 18, 39, 8, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 917058766, 'comment_body': 'This should be private (prefixed by `_`) to make it clear users should never touch it', 'comment_created': datetime.datetime(2022, 7, 8, 18, 40, 13, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 917058844, 'comment_body': 'Private', 'comment_created': datetime.datetime(2022, 7, 8, 18, 40, 22, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 917058900, 'comment_body': 'etc.', 'comment_created': datetime.datetime(2022, 7, 8, 18, 40, 27, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 917059470, 'comment_body': ""All of this stuff looks like it should be private to me. That also means we don't need a `@property` or a `@whatever.setter` for each of them. We just use private attributes directly"", 'comment_created': datetime.datetime(2022, 7, 8, 18, 41, 21, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 917065518, 'comment_body': ""... probably here you do\r\n```\r\nepochs = epochs.copy().pick('data', exclude='bads')\r\nsnsr_cov = pick_channels_cov(cov, epochs.ch_names, ordered=True).data\r\nfwd = convert_forward_solution(fwd, force_fixed=True)\r\nfwd_src_snsr = pick_channels_forward(fwd, epochs.ch_names, ordered=True)['sol']['data']\r\ndata = epochs.get_data()\r\ninfo = epochs.info\r\ndel cov, fwd, epochs\r\n```\r\nnow you are guaranteed that `data`, `fwd_src_snsr`, and `snsr_cov `all have the same set of channels in the same order. Then to rescale them all you can just do:\r\n```\r\nrescale_cov = make_ad_hoc_cov(epochs.info)\r\nscaler = compute_whitener(info, rescale_cov)\r\ndel rescale_cov\r\nfwd_src_snsr = scaler @ fwd_src_snsr\r\nsnsr_cov = scaler @ snsr_cov @ scaler.T\r\ndata = scaler @ epochs.get_data()  # @ nicely knows to operate over the last two dims of (epochs, chs, time)\r\n```\r\nor something similar. Basically you let the MNE `pick_channels_*` make sure all channels are present and ordered properly, then make use of MNE functions to do the rescaling."", 'comment_created': datetime.datetime(2022, 7, 8, 18, 51, 23, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 923884515, 'comment_body': 'The initialization of the model uses these variables to set up the default values for several model parameters. See `models.py` line 97. Would it make sense to move those initializations to `add_subject()`? I think it makes more sense to leave them in the initialization of the model. ', 'comment_created': datetime.datetime(2022, 7, 18, 21, 40, 5, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 923887355, 'comment_body': '`At` is already frequently used in `models.py` when updating `A`. I thought about using something like `connectivity` or `dyn_conn` but I think it would make the code really messy and hard to read. We can talk about it more in our meeting?', 'comment_created': datetime.datetime(2022, 7, 18, 21, 44, 30, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 923889690, 'comment_body': ""So I should be able to remove all of these `@property` and `@some.setter` lines right? Does that mean there needs to be additional checks on variable format somewhere? I don't think any of these have any checks as they are written here. I'd also like to talk about which vars and functions to make private. In my mind, all of the vars and functions in `models.py` should be private."", 'comment_created': datetime.datetime(2022, 7, 18, 21, 48, 28, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 923893375, 'comment_body': 'Perhaps At_? To have fitted attributes have an underscore at the end. In line with scikit learn convention. \n\nOr A_t_? ', 'comment_created': datetime.datetime(2022, 7, 18, 21, 54, 58, tzinfo=datetime.timezone.utc), 'commenter': 'adam2392', 'type': 'User'}, {'comment_id': 927187769, 'comment_body': ""Previously you said to play around with these functions to see if it would produce the same outputs as the `scale_sensor_data` function. The original function was using a scale of 1 for each of the channel types so nothing was changed. This scaling does alter the data, primarily in a significant increase in the number of principal components (PCA run immediately after scaling function). Also, the output of the model is much noisier using `@ scaler`, I think as expected with the significant increase in PCs. Let's talk to make sure this is working as expected. \r\n"", 'comment_created': datetime.datetime(2022, 7, 22, 0, 2, 41, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 927188548, 'comment_body': 'Output using mne functions for scaling seems much noisier.\r\n![image](https://user-images.githubusercontent.com/39603454/180334645-499c773d-20aa-4de5-ac65-3628f9eac992.png)\r\n', 'comment_created': datetime.datetime(2022, 7, 22, 0, 4, 39, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 927188842, 'comment_body': 'Compared to no scaling (or previous function with scale of 1 for each channel type).\r\n![image](https://user-images.githubusercontent.com/39603454/180334746-1d6af98f-1413-4254-bbf6-713a61412db4.png)\r\n', 'comment_created': datetime.datetime(2022, 7, 22, 0, 5, 37, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 930025057, 'comment_body': ""> The original function was using a scale of 1 for each of the channel types so nothing was changed.\r\n\r\nThis is the equivalent of using `mne.make_ad_hoc_cov(..., std=1.)`. So if you want this mode you can get it easily with a very small change. Can you verify this looks the same as the old code?\r\n\r\n> This scaling does alter the data, primarily in a significant increase in the number of principal components (PCA run immediately after scaling function).\r\n\r\nThis is to be expected. If you do no scaling and have MEG *and* EEG data, your EEG data will be ~6 orders of magnitude larger or so, so you will end up only looking at EEG data.\r\n\r\n> Also, the output of the model is much noisier using @ scaler, I think as expected with the significant increase in PCs. Let's talk to make sure this is working as expected.\r\n\r\nWhat's not clear to me: is it also noisier (and noisier in the same way) when using the *old* code path but *enabling* scaling?\r\n\r\nIn other words: is this a problem with the new way of scaling, or is this a problem that has always existed with scaling the data, regardless of scaling method?\r\n\r\nWe can chat about this today"", 'comment_created': datetime.datetime(2022, 7, 26, 14, 18, 23, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 956575693, 'comment_body': '@jadrew43 this is line 114 and it\'s not related to scaling. There is some stuff below that is. Can you confirm by commenting on the correct lines/lines in the ""Files"" tab of this PR? This not being the correct line makes me wonder if you forgot a push, and I don\'t want to look prematurely.\r\n\r\nThe best way to help me check would be to make it very easy to run and check the results. For example, add a commented-out (to make CIs happy) `# np.testing.assert_allclose(data, data_mne, rtol=..., atol=...)` statement that fails (when you uncomment it) where you scale `data` the old way and `data_mne` the new way. Then I need a minimal script (hopefully runs in just a few seconds, and on `sample` data for example) I can run on this branch that will fail when I uncomment the `assert_allclose` line', 'comment_created': datetime.datetime(2022, 8, 27, 12, 14, 9, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 957773317, 'comment_body': ""The updated files are in `state_space/`, I haven't done anything in `examples/` in weeks so I went ahead and deleted the files I had in there. Running `state_space_connectivity.py` takes about 90 seconds, adding `_mne_scale_sensor_data()` adds about 60 seconds. Happy to chat about building tests that take much less time. The commented testing line is L228 in `mne_util.py`."", 'comment_created': datetime.datetime(2022, 8, 29, 20, 36, 51, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 958761843, 'comment_body': 'Hmm, I don\'t think this is quite a minimal reproducible bit of code.\r\n\r\nWhen I tried to run the code, it failed because I needed `autograd`. I installed that, then it complained that I needed `autograd_linalg`. I can\'t find this on `pip` or `conda`, so I tried swapping in `from scipy.linalg import solve_triangular`.\r\n\r\nNext I tried to run the script and got:\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: \'/home/larsoner/mne_data/MNE-sample-data/MEG/sample/labels/AUD-lh.label\'\r\n```\r\nThis makes sense, usually labels are in the SUBJECTS_DIR. But even then there aren\'t auditory labels in `sample`. So let\'s change it to something that should work I think:\r\n```\r\nregexp = \'^(G_temp_sup-Plan_tempo.*|Pole_occipital)\'\r\nlabels = mne.read_labels_from_annot(\r\n    \'sample\', \'aparc.a2009s\', regexp=regexp, subjects_dir=subjects_dir)\r\nlabel_names = [label.name for label in labels]\r\nassert len(label_names) == 4\r\n```\r\nThen I got a bit farther, until I hit:\r\n```\r\nAttributeError: python: undefined symbol: mkl_get_max_threads\r\n```\r\nSo I then disabled all the thread-setting stuff, and then get to:\r\n```\r\n  File ""/home/larsoner/python/scipy/scipy/linalg/_basic.py"", line 336, in solve_triangular\r\n    raise ValueError(\'expected square matrix\')\r\nValueError: expected square matrix\r\n```\r\nSo my swap of `scipy.linalg` is not correct.\r\n\r\nI don\'t think I can proceed until I can install `autograd_linalg` somehow, how do you recommend doing this?', 'comment_created': datetime.datetime(2022, 8, 30, 17, 39, 59, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}, {'comment_id': 958772623, 'comment_body': 'Try this:\r\npip install --src deps -e git+ssh://git@github.com/nfoti/autograd_linalg.git@master#egg=autograd_linalg', 'comment_created': datetime.datetime(2022, 8, 30, 17, 52, 8, tzinfo=datetime.timezone.utc), 'commenter': 'jadrew43', 'type': 'User'}, {'comment_id': 958773526, 'comment_body': 'I pushed a commit for a `solve_triangular` that allows the code to run on my machine. I can look into the issue now!', 'comment_created': datetime.datetime(2022, 8, 30, 17, 53, 10, tzinfo=datetime.timezone.utc), 'commenter': 'larsoner', 'type': 'User'}]","[{'commit_sha': '05eb0a255034388c81584d7b4616adf87c7c67ed', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fb40a1da57f81388179459c7e81aaf0c7c6a8278', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c65356487b42da61c5bda03a85825d51083c1f9d', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': '60a04adfc15a4adbb877ba319a6978571426c50f', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': '97b982693632f22b9ae87977ad6f1bc831f93f05', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': '87269d74cd6ab59332a4b9f8cd20649ff2d8f143', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': '93755acd5669fc503d1402f473895f70a5922e89', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'aca431c773272d1f95f87465a8965d04ec0ebbeb', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': '463b804f998af30ad4827b48d610910e296c2682', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5164d6c8700d34ce2252bc82cc2cf8287a3d4e45', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': '460fbbbb10e8ec91901a0b64204877930fd162ed', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e60d17518c58238e4e5a876fe522aca6a7fbf800', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': '71c65398127194e0ba5dab6923c25e7b55e07db1', 'committer_username': 'larsoner', 'committer_name': 'Eric Larson', 'committer_email': 'larson.eric.d@gmail.com', 'commit_date': datetime.datetime(2012, 9, 17, 22, 6, 38, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b05fd713adb4e87c305a3bdee0db4a606e393779', 'committer_username': 'larsoner', 'committer_name': 'Eric Larson', 'committer_email': 'larson.eric.d@gmail.com', 'commit_date': datetime.datetime(2012, 9, 17, 22, 6, 38, tzinfo=datetime.timezone.utc)}, {'commit_sha': '39c1d1b217e5af882f995ad33f0bf10bc82ef264', 'committer_username': 'larsoner', 'committer_name': 'Eric Larson', 'committer_email': 'larson.eric.d@gmail.com', 'commit_date': datetime.datetime(2012, 9, 17, 22, 6, 38, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e7ffafa4f6614647eaed370c5ebcff4f0401aaa5', 'committer_username': 'larsoner', 'committer_name': 'Eric Larson', 'committer_email': 'larson.eric.d@gmail.com', 'commit_date': datetime.datetime(2012, 9, 17, 22, 6, 38, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e63252ae2dd9160db516539cff29cb5c706ed3a8', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4ea5b8a89a55253173bafce259356aa7286d8763', 'committer_username': 'jadrew43', 'committer_name': 'Drew, J.', 'committer_email': None, 'commit_date': datetime.datetime(2018, 5, 24, 21, 11, 41, tzinfo=datetime.timezone.utc)}]","Drew, J.",39603454,,User,,6,,0,0

Project_ID,Name,Full_name,Language,Forks,Stars,Watchers,contributors,commits,issues,branches,PRs_count,contributor pullrequests
369628653,mne-connectivity,mne-tools/mne-connectivity,Python,34,67,12,21,232,49,7,8,"[{'id': 967515821, 'number': 100, 'closed': None, 'created': datetime.datetime(2022, 6, 14, 23, 52, 22, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 2478, 'deletions': 0, 'state': 'open'}]"
