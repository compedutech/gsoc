pr_id,pr_title,pr_body,is_merged,pr_number,pr_url,pr_html_url,pr_state,additions,deletions,pr_changed_files,pr_commits_count,pr_comments_count,pr_review_comments_count,pr_labels_count,pr_assignees_count,pr_labels,pr_created_at,pr_closed_at,time_taken,time_delta,pr_review_comments,pr_commits,contributor,contributor_id,contributor_email,contributor_type,contributions,contributor_public_repos,contributor_private_repos,contributor_followings,contributor_followers
969412673,Run postprocess before consolidate in MZZ,This addresses issue https://github.com/fsspec/kerchunk/issues/176 ,True,180,https://api.github.com/repos/fsspec/kerchunk/pulls/180,https://github.com/fsspec/kerchunk/pull/180,closed,26,21,2,3,7,2,0,0,[],2022-06-16 14:41:57+00:00,2022-06-21 15:31:18+00:00,434961.0,"5 days, 0:49:21","[{'comment_id': 902654963, 'comment_body': 'Please alter the docstring for postprocess, to add its signature `postprocess(dict)->dict`.', 'comment_created': datetime.datetime(2022, 6, 21, 13, 56, 24, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 902753074, 'comment_body': 'Okay, have updated in latest commit ', 'comment_created': datetime.datetime(2022, 6, 21, 15, 16, 39, tzinfo=datetime.timezone.utc), 'commenter': 'peterm790', 'type': 'User'}]","[{'commit_sha': '9a84c93f3da1bf7d0f8c8756c93fe4773d578201', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0ca5bc7d7fb0a5133a04c66e351a348d40b15c9a', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '355e8f2fe115640df9d2537ba78790cbf8943380', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}]",Peter Marsh,52179978,petermarsh790@gmail.com,User,,23,,4,7
977503862,Speed up combine,"- get file sizes directly, where possible
- use np arrays and searchsorted instead of list index

@peterm790 , I get a combine time for the two intermediate JSON files of about 3s with this and https://github.com/fsspec/filesystem_spec/pull/985",True,183,https://api.github.com/repos/fsspec/kerchunk/pulls/183,https://github.com/fsspec/kerchunk/pull/183,closed,45,13,5,3,0,0,0,0,[],2022-06-23 18:34:16+00:00,2022-06-24 17:23:47+00:00,82171.0,22:49:31,[],"[{'commit_sha': '1e47dcad0f3c0b95d92633cdf75cb3164016721c', 'committer_username': 'martindurant', 'committer_name': 'Martin Durant', 'committer_email': None, 'commit_date': datetime.datetime(2013, 11, 26, 16, 3, 34, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6ca34a3de3d984e422c9663490085aad90e8a32d', 'committer_username': 'martindurant', 'committer_name': 'Martin Durant', 'committer_email': None, 'commit_date': datetime.datetime(2013, 11, 26, 16, 3, 34, tzinfo=datetime.timezone.utc)}, {'commit_sha': '409327841d472d675d1976a0ea73547cb5aa78b3', 'committer_username': 'martindurant', 'committer_name': 'Martin Durant', 'committer_email': None, 'commit_date': datetime.datetime(2013, 11, 26, 16, 3, 34, tzinfo=datetime.timezone.utc)}]",Martin Durant,6042212,,User,,160,,0,403
998075196,add tutorial to docs and ERA5 example,I have moved the existing tutorial to 'Quick Start' and used the [extended ERA5 tutorial](https://github.com/peterm790/ERA5_Kerchunk_tutorial/blob/master/ERA5_tutorial_extended.ipynb) as the new tutorial.  I have also added a new ERA5 notebook to the examples as well as the associated reference json. Both these files are ~14MB. ,False,193,https://api.github.com/repos/fsspec/kerchunk/pulls/193,https://github.com/fsspec/kerchunk/pull/193,closed,5787,5,6,8,2,0,0,0,[],2022-07-15 19:10:32+00:00,2022-08-05 18:09:53+00:00,1810761.0,"20 days, 22:59:21",[],"[{'commit_sha': '3a00f54cb638bff955d0409eff1d5ffacf1c1cd9', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '83ec4d724183ddf943478e6a683a074e49edf559', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '290656c5327f9cab0f0e881ff3dd81432f87e541', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a1fcbe37105d766ce5b28e4eb810ba7d8a2e2b91', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f4b7bd6340f4c5c2f60c3beeaec0604d05593fbe', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f1c4577ba0c555131850401c2ee7da5a9f4355a9', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'de69eec41651349e746a67afe53c9164ba01ba1b', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '161c54bfb1f32c9d72f0b83d1209646d9506a43b', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}]",Peter Marsh,52179978,petermarsh790@gmail.com,User,,23,,4,7
999463603,Add merge_vars convenience function to combine,"I have had a go at creating a convenience function to merge variables across datasets with identical coordinates. 

The initial code to open the files using `fsspec` is from `MultiZarrToZarr.fss` and seems to work fine although I have not provided a `target_options` argument as I wasn't sure how to do this. 

I am checking that the dimensions of the variables from the files are identical by comparing the shape of the array with the largest number of dimensions, which I assume to be the variable. This seems to work fine for the datasets I have tested against but it is maybe not the clearest. Another option is to have the user specify the `identical_dims` and have a check to ensure these are in fact identical.  Or is it okay to have no checks? 

Fundamentally all this is doing is merging the dictionary items across the jsons and so could have strange results if the dimensions are not identical. ",True,196,https://api.github.com/repos/fsspec/kerchunk/pulls/196,https://github.com/fsspec/kerchunk/pull/196,closed,35,0,2,6,1,7,0,0,[],2022-07-18 14:19:25+00:00,2022-07-21 20:51:26+00:00,282721.0,"3 days, 6:32:01","[{'comment_id': 924666192, 'comment_body': 'Could use docs. Files is a list of dicts or list of str?', 'comment_created': datetime.datetime(2022, 7, 19, 15, 38, 21, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 924670108, 'comment_body': 'Perhaps rather make filesystems (url_to_fs) and use fs.open? Or are you hoping to allow for glob characters here? If so, you could just `fo_list = fsspec.open(files)` and deal with it a few lines below; note that we should pass some `storage_options`.', 'comment_created': datetime.datetime(2022, 7, 19, 15, 41, 12, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 924673311, 'comment_body': 'Probably just assume the user knows what they are doing? There might be a reason to have var1(x1, y1) and var2(x2, y2), i.e., completely independent from oneanother, in the same dataset.', 'comment_created': datetime.datetime(2022, 7, 19, 15, 44, 10, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 926932773, 'comment_body': '```suggestion\r\n        merged = fo_list[0].copy()\r\n```', 'comment_created': datetime.datetime(2022, 7, 21, 17, 24, 38, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 926933069, 'comment_body': '```suggestion\r\n    return merged\r\n\r\n```', 'comment_created': datetime.datetime(2022, 7, 21, 17, 25, 2, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 926933986, 'comment_body': ""I added `.copy()` here, so that the original input dictionary doesn't get changed. It is unlikely to be used again, but still..."", 'comment_created': datetime.datetime(2022, 7, 21, 17, 26, 5, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 926934160, 'comment_body': ""```suggestion\r\n    assert list(merge['refs']) == ['item1', 'item2']\r\n\r\n```"", 'comment_created': datetime.datetime(2022, 7, 21, 17, 26, 17, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}]","[{'commit_sha': 'f86e52186b9f63bf203ce16bbb0d8f3d69f18b10', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7555e916ceed1a32cb4bfae3ac68e3417564b5bb', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '30e163c811bd4596e70f5e7bb7e2a1212dca1a32', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7ba33ed95677a01cd67c8ea29bc6bf8decea345a', 'committer_username': 'martindurant', 'committer_name': 'Martin Durant', 'committer_email': None, 'commit_date': datetime.datetime(2013, 11, 26, 16, 3, 34, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd45815749f639517e2a59ca202d1c616b9b0a480', 'committer_username': 'martindurant', 'committer_name': 'Martin Durant', 'committer_email': None, 'commit_date': datetime.datetime(2013, 11, 26, 16, 3, 34, tzinfo=datetime.timezone.utc)}, {'commit_sha': '599f39438cd2fb5d3059e213f81f205e586075b9', 'committer_username': 'martindurant', 'committer_name': 'Martin Durant', 'committer_email': None, 'commit_date': datetime.datetime(2013, 11, 26, 16, 3, 34, tzinfo=datetime.timezone.utc)}]",Peter Marsh,52179978,petermarsh790@gmail.com,User,,23,,4,7
1017398394,update HRRR case study in docs,This updates the HRRR case study to use the new scan_grib method,True,206,https://api.github.com/repos/fsspec/kerchunk/pulls/206,https://github.com/fsspec/kerchunk/pull/206,closed,3,4,1,2,1,0,0,0,[],2022-08-04 11:19:19+00:00,2022-08-04 13:33:32+00:00,8053.0,2:14:13,[],"[{'commit_sha': '57c36014abae87b0c5dec7e9690b89c233bd51d8', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '386753cd35048d79fb369c6f5980cec4b5c4518e', 'committer_username': 'martindurant', 'committer_name': 'Martin Durant', 'committer_email': None, 'commit_date': datetime.datetime(2013, 11, 26, 16, 3, 34, tzinfo=datetime.timezone.utc)}]",Peter Marsh,52179978,petermarsh790@gmail.com,User,,23,,4,7
1018906833,Update tutorial,"I have moved the existing tutorial to 'Quick Start' and used the [extended ERA5 tutorial](https://github.com/peterm790/ERA5_Kerchunk_tutorial/blob/master/ERA5_tutorial_extended.ipynb) to create a newer more thorough tutorial. 

This time created directly as a reStructuredText document to avoid pandoc. I will close https://github.com/fsspec/kerchunk/pull/193 if this builds successfully.",True,208,https://api.github.com/repos/fsspec/kerchunk/pulls/208,https://github.com/fsspec/kerchunk/pull/208,closed,589,5,6,6,1,13,0,0,[],2022-08-05 17:28:54+00:00,2022-08-17 13:03:21+00:00,1020867.0,"11 days, 19:34:27","[{'comment_id': 939167237, 'comment_body': 'You can do all this in one line, but its a bit messy:\r\n```\r\nds = xr.open_dataset(""reference://"", engine=""zarr"", backend_kwargs={\r\n    ""consolidated"": False,\r\n    ""storage_options"": {""fo"": \'01_air_pressure_at_mean_sea_level.json\', ""remote_protocol"": ""s3"",\r\n        ""remote_options"": {""anon"": True}}\r\n)\r\n```\r\n(the complexity of this line is why people would eventually want to hide such a dataset prescription inside an Intake catalogue).', 'comment_created': datetime.datetime(2022, 8, 5, 19, 33, 39, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943933977, 'comment_body': 'Now that it\'s in one line, this ""mapping"" is created by xarray/zarr. Perhaps just note that the location of the references file and details for the filesystem holding the original data all need to be given.', 'comment_created': datetime.datetime(2022, 8, 11, 20, 57, 26, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943934500, 'comment_body': '```suggestion\r\nMultiZarrtoZarr provides a number of convenience methods to combine reference files. The simplest is to concatenate along a specified dimension using the ``concat_dims`` argument, `""time0""` in this instance. Specifying the identical coordinate across the files using the ``identical_dims`` argument is not strictly necessary but will speed up computation times.\r\n```', 'comment_created': datetime.datetime(2022, 8, 11, 20, 58, 14, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943935432, 'comment_body': 'Explain that we created this dimension by supplying literal values.', 'comment_created': datetime.datetime(2022, 8, 11, 20, 59, 37, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943935793, 'comment_body': 'and now this is the result of applying the regex to each input file name', 'comment_created': datetime.datetime(2022, 8, 11, 21, 0, 10, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943936210, 'comment_body': '```suggestion\r\nSimilarly we can map each file to a new variable using the special ``var`` key in coo_map. Here we use the same ``regex`` function but instead map these as new variables.\r\n```', 'comment_created': datetime.datetime(2022, 8, 11, 21, 0, 48, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943936495, 'comment_body': ""```suggestion\r\nAnother special character in ``coo_map`` is ``attr:``. This allows the user to access values from the each dataset's global attributes.\r\n```"", 'comment_created': datetime.datetime(2022, 8, 11, 21, 1, 15, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943937618, 'comment_body': 'but both datasets had the save value of the attribute, so is this actually what we expect?', 'comment_created': datetime.datetime(2022, 8, 11, 21, 2, 45, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943938281, 'comment_body': '```suggestion\r\nThe special value ``vattr:{var}:{attr}`` allows access to variable attributes. Here renaming the variable to instead use its short name. \r\n```', 'comment_created': datetime.datetime(2022, 8, 11, 21, 3, 46, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943938508, 'comment_body': 'This line should follow the example block below', 'comment_created': datetime.datetime(2022, 8, 11, 21, 4, 6, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 944388199, 'comment_body': 'updated to now read:\r\n\r\nThe ``.json`` reference files we have generated can now be used to open virtual datasets through xarray or zarr. It is necessary to specify location of the reference ``json`` files, using the ``target_options`` argument, and the source data using the ``remote_options`` and ``remote_protocol`` arguments. Here specifying that the source data is stored on ``AWS S3`` and can be accessed anonymously.', 'comment_created': datetime.datetime(2022, 8, 12, 11, 51, 18, tzinfo=datetime.timezone.utc), 'commenter': 'peterm790', 'type': 'User'}, {'comment_id': 944388996, 'comment_body': 'In this case the `json_list` contains two files of consecutive time steps and the same variable  ', 'comment_created': datetime.datetime(2022, 8, 12, 11, 52, 41, tzinfo=datetime.timezone.utc), 'commenter': 'peterm790', 'type': 'User'}, {'comment_id': 944390288, 'comment_body': 'I have added this section after out conversation yesterday to make it clear the reference files do not need to be regenerated if the source files move. Although I am not sure how well this comes across here? ', 'comment_created': datetime.datetime(2022, 8, 12, 11, 54, 43, tzinfo=datetime.timezone.utc), 'commenter': 'peterm790', 'type': 'User'}]","[{'commit_sha': '4d2236267ea217d47127275048fd4d0120a73e64', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7037709fbb5067618346ec634fdd7aa5ceb10760', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bde3ecfb024e8fe41fe60a02549388052f636e4a', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6c62c2bf782024124f01a3ee04635abefa5a44c4', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dc69bd1ba7f0c8a3802c848b65e6bab8e02b8392', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '72fd3d620cd1097e6f327595ac9411e1665c2a38', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}]",Peter Marsh,52179978,petermarsh790@gmail.com,User,,23,,4,7
1018906833,Update tutorial,"I have moved the existing tutorial to 'Quick Start' and used the [extended ERA5 tutorial](https://github.com/peterm790/ERA5_Kerchunk_tutorial/blob/master/ERA5_tutorial_extended.ipynb) to create a newer more thorough tutorial. 

This time created directly as a reStructuredText document to avoid pandoc. I will close https://github.com/fsspec/kerchunk/pull/193 if this builds successfully.",True,208,https://api.github.com/repos/fsspec/kerchunk/pulls/208,https://github.com/fsspec/kerchunk/pull/208,closed,589,5,6,6,1,13,0,0,[],2022-08-05 17:28:54+00:00,2022-08-17 13:03:21+00:00,1020867.0,"11 days, 19:34:27","[{'comment_id': 939167237, 'comment_body': 'You can do all this in one line, but its a bit messy:\r\n```\r\nds = xr.open_dataset(""reference://"", engine=""zarr"", backend_kwargs={\r\n    ""consolidated"": False,\r\n    ""storage_options"": {""fo"": \'01_air_pressure_at_mean_sea_level.json\', ""remote_protocol"": ""s3"",\r\n        ""remote_options"": {""anon"": True}}\r\n)\r\n```\r\n(the complexity of this line is why people would eventually want to hide such a dataset prescription inside an Intake catalogue).', 'comment_created': datetime.datetime(2022, 8, 5, 19, 33, 39, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943933977, 'comment_body': 'Now that it\'s in one line, this ""mapping"" is created by xarray/zarr. Perhaps just note that the location of the references file and details for the filesystem holding the original data all need to be given.', 'comment_created': datetime.datetime(2022, 8, 11, 20, 57, 26, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943934500, 'comment_body': '```suggestion\r\nMultiZarrtoZarr provides a number of convenience methods to combine reference files. The simplest is to concatenate along a specified dimension using the ``concat_dims`` argument, `""time0""` in this instance. Specifying the identical coordinate across the files using the ``identical_dims`` argument is not strictly necessary but will speed up computation times.\r\n```', 'comment_created': datetime.datetime(2022, 8, 11, 20, 58, 14, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943935432, 'comment_body': 'Explain that we created this dimension by supplying literal values.', 'comment_created': datetime.datetime(2022, 8, 11, 20, 59, 37, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943935793, 'comment_body': 'and now this is the result of applying the regex to each input file name', 'comment_created': datetime.datetime(2022, 8, 11, 21, 0, 10, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943936210, 'comment_body': '```suggestion\r\nSimilarly we can map each file to a new variable using the special ``var`` key in coo_map. Here we use the same ``regex`` function but instead map these as new variables.\r\n```', 'comment_created': datetime.datetime(2022, 8, 11, 21, 0, 48, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943936495, 'comment_body': ""```suggestion\r\nAnother special character in ``coo_map`` is ``attr:``. This allows the user to access values from the each dataset's global attributes.\r\n```"", 'comment_created': datetime.datetime(2022, 8, 11, 21, 1, 15, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943937618, 'comment_body': 'but both datasets had the save value of the attribute, so is this actually what we expect?', 'comment_created': datetime.datetime(2022, 8, 11, 21, 2, 45, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943938281, 'comment_body': '```suggestion\r\nThe special value ``vattr:{var}:{attr}`` allows access to variable attributes. Here renaming the variable to instead use its short name. \r\n```', 'comment_created': datetime.datetime(2022, 8, 11, 21, 3, 46, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 943938508, 'comment_body': 'This line should follow the example block below', 'comment_created': datetime.datetime(2022, 8, 11, 21, 4, 6, tzinfo=datetime.timezone.utc), 'commenter': 'martindurant', 'type': 'User'}, {'comment_id': 944388199, 'comment_body': 'updated to now read:\r\n\r\nThe ``.json`` reference files we have generated can now be used to open virtual datasets through xarray or zarr. It is necessary to specify location of the reference ``json`` files, using the ``target_options`` argument, and the source data using the ``remote_options`` and ``remote_protocol`` arguments. Here specifying that the source data is stored on ``AWS S3`` and can be accessed anonymously.', 'comment_created': datetime.datetime(2022, 8, 12, 11, 51, 18, tzinfo=datetime.timezone.utc), 'commenter': 'peterm790', 'type': 'User'}, {'comment_id': 944388996, 'comment_body': 'In this case the `json_list` contains two files of consecutive time steps and the same variable  ', 'comment_created': datetime.datetime(2022, 8, 12, 11, 52, 41, tzinfo=datetime.timezone.utc), 'commenter': 'peterm790', 'type': 'User'}, {'comment_id': 944390288, 'comment_body': 'I have added this section after out conversation yesterday to make it clear the reference files do not need to be regenerated if the source files move. Although I am not sure how well this comes across here? ', 'comment_created': datetime.datetime(2022, 8, 12, 11, 54, 43, tzinfo=datetime.timezone.utc), 'commenter': 'peterm790', 'type': 'User'}]","[{'commit_sha': '4d2236267ea217d47127275048fd4d0120a73e64', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7037709fbb5067618346ec634fdd7aa5ceb10760', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bde3ecfb024e8fe41fe60a02549388052f636e4a', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6c62c2bf782024124f01a3ee04635abefa5a44c4', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dc69bd1ba7f0c8a3802c848b65e6bab8e02b8392', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '72fd3d620cd1097e6f327595ac9411e1665c2a38', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}]",Peter Marsh,52179978,petermarsh790@gmail.com,User,,23,,4,7
1031676894,Use fs.cat in combine,Since getting many (small) files serially can be much slower.,True,213,https://api.github.com/repos/fsspec/kerchunk/pulls/213,https://github.com/fsspec/kerchunk/pull/213,closed,3,2,1,4,0,0,0,0,[],2022-08-20 02:09:34+00:00,2022-08-20 13:03:44+00:00,39250.0,10:54:10,[],"[{'commit_sha': '73ce122d86c5a015f007946c3cdd114fb48c24c2', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '471543c41d78ddacddf8cd8eb9c999209f9583f1', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '81a983b87ae066372abd648022b9edf7809d3128', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '88e18cb405766980836e531a221ee874ee3b11ca', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}]",Martin Durant,6042212,,User,,160,,0,403
1028907451,Reference Recipe Add Target Options,"This is following from https://github.com/pangeo-forge/staged-recipes/pull/154#issuecomment-1192794733 

I think this should be sufficient to get the LiveOcean [recipe](https://github.com/pangeo-forge/staged-recipes/pull/154) to run. I have not set up any tests for this as I'm not entirely sure how that could be done. ",True,399,https://api.github.com/repos/pangeo-forge/pangeo-forge-recipes/pulls/399,https://github.com/pangeo-forge/pangeo-forge-recipes/pull/399,closed,4,0,1,4,3,2,0,0,[],2022-08-17 14:19:06+00:00,2022-08-30 13:55:08+00:00,1121762.0,"12 days, 23:36:02","[{'comment_id': 951745562, 'comment_body': '```suggestion\r\n            target_options=config.target_options,\r\n```', 'comment_created': datetime.datetime(2022, 8, 22, 18, 2, 51, tzinfo=datetime.timezone.utc), 'commenter': 'rabernat', 'type': 'User'}, {'comment_id': 951745915, 'comment_body': '```suggestion\r\n    target_options: Optional[dict] = field(default_factory=dict)\r\n```', 'comment_created': datetime.datetime(2022, 8, 22, 18, 3, 14, tzinfo=datetime.timezone.utc), 'commenter': 'rabernat', 'type': 'User'}]","[{'commit_sha': 'b90f7c12d3af97ceb185c424448a47488a243747', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fbab6d4a4ba387bb8fb9f72a6d6d0855a670acfe', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': '81ad9f5495afea72c01671d041308f48c574630b', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd59e0c18cc7cd3f887191cddd076d96c1a4dadb4', 'committer_username': 'peterm790', 'committer_name': 'Peter Marsh', 'committer_email': 'petermarsh790@gmail.com', 'commit_date': datetime.datetime(2019, 6, 24, 21, 27, 4, tzinfo=datetime.timezone.utc)}]",Peter Marsh,52179978,petermarsh790@gmail.com,User,,23,,4,7

Project_ID,Name,Full_name,Language,Forks,Stars,Watchers,contributors,commits,issues,branches,PRs_count,contributor pullrequests
311700841,kerchunk,fsspec/kerchunk,Python,78,296,22,51,905,112,5,9,"[{'id': 1138025842, 'number': 256, 'closed': datetime.datetime(2022, 11, 29, 14, 30, 50, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 11, 28, 19, 41, 13, tzinfo=datetime.timezone.utc), 'time_taken': 67777.0, 'time_delta': '18:49:37', 'additions': 33, 'deletions': 1, 'state': 'closed'}, {'id': 1018906833, 'number': 208, 'closed': datetime.datetime(2022, 8, 17, 13, 3, 21, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 5, 17, 28, 54, tzinfo=datetime.timezone.utc), 'time_taken': 1020867.0, 'time_delta': '11 days, 19:34:27', 'additions': 589, 'deletions': 5, 'state': 'closed'}, {'id': 1017398394, 'number': 206, 'closed': datetime.datetime(2022, 8, 4, 13, 33, 32, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 4, 11, 19, 19, tzinfo=datetime.timezone.utc), 'time_taken': 8053.0, 'time_delta': '2:14:13', 'additions': 3, 'deletions': 4, 'state': 'closed'}, {'id': 999463603, 'number': 196, 'closed': datetime.datetime(2022, 7, 21, 20, 51, 26, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 7, 18, 14, 19, 25, tzinfo=datetime.timezone.utc), 'time_taken': 282721.0, 'time_delta': '3 days, 6:32:01', 'additions': 35, 'deletions': 0, 'state': 'closed'}, {'id': 998075196, 'number': 193, 'closed': datetime.datetime(2022, 8, 5, 18, 9, 53, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 7, 15, 19, 10, 32, tzinfo=datetime.timezone.utc), 'time_taken': 1810761.0, 'time_delta': '20 days, 22:59:21', 'additions': 5787, 'deletions': 5, 'state': 'closed'}, {'id': 969412673, 'number': 180, 'closed': datetime.datetime(2022, 6, 21, 15, 31, 18, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 6, 16, 14, 41, 57, tzinfo=datetime.timezone.utc), 'time_taken': 434961.0, 'time_delta': '5 days, 0:49:21', 'additions': 26, 'deletions': 21, 'state': 'closed'}]"
281445915,pangeo-forge-recipes,pangeo-forge/pangeo-forge-recipes,Python,54,121,16,40,1896,153,45,16,"[{'id': 1028907451, 'number': 399, 'closed': datetime.datetime(2022, 8, 30, 13, 55, 8, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 17, 14, 19, 6, tzinfo=datetime.timezone.utc), 'time_taken': 1121762.0, 'time_delta': '12 days, 23:36:02', 'additions': 4, 'deletions': 0, 'state': 'closed'}]"
