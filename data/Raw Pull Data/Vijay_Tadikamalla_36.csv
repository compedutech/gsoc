pr_id,pr_title,pr_body,is_merged,pr_number,pr_url,pr_html_url,pr_state,additions,deletions,pr_changed_files,pr_commits_count,pr_comments_count,pr_review_comments_count,pr_labels_count,pr_assignees_count,pr_labels,pr_created_at,pr_closed_at,time_taken,time_delta,pr_review_comments,pr_commits,contributor,contributor_id,contributor_email,contributor_type,contributions,contributor_public_repos,contributor_private_repos,contributor_followings,contributor_followers
475361911,Script to detect Dead URLs,"Fix https://github.com/tensorflow/datasets/issues/2352

TODO:
- [ ] Detect Dead URLs in folder datasets",True,2354,https://api.github.com/repos/tensorflow/datasets/pulls/2354,https://github.com/tensorflow/datasets/pull/2354,closed,40,0,1,3,0,3,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-08-28 11:53:23+00:00,2020-09-03 11:58:10+00:00,518687.0,"6 days, 0:04:47","[{'comment_id': 479222828, 'comment_body': 'This should go in `cleanup/` which contain our maintenance scripts.', 'comment_created': datetime.datetime(2020, 8, 28, 12, 20, 51, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 479226443, 'comment_body': 'There is a lot of URLs to check, we should use ThreadPoolExecutor from https://docs.python.org/3/library/concurrent.futures.html to parallelize the checks', 'comment_created': datetime.datetime(2020, 8, 28, 12, 24, 2, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 479226912, 'comment_body': ""Let's not save the urls. Only print it for now."", 'comment_created': datetime.datetime(2020, 8, 28, 12, 24, 29, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': 'bbc669608e3e83afb4ee8b15f1ec66b0f9340f36', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9d8ac86f1be3ddd61386fb5ec400243e89ceb8af', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5019206e2307251380f2fe6eddbcb2b8c19b7473', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
474156256,Get generated dataset file location by builder name,Related to https://github.com/tensorflow/datasets/pull/2322#discussion_r473138648 part 2,True,2346,https://api.github.com/repos/tensorflow/datasets/pulls/2346,https://github.com/tensorflow/datasets/pull/2346,closed,183,25,6,7,0,12,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:is_reviewing'}]",2020-08-26 21:13:32+00:00,2020-09-18 12:05:39+00:00,1954327.0,"22 days, 14:52:07","[{'comment_id': 477594072, 'comment_body': '`exist` function should return `True` for both directories and files', 'comment_created': datetime.datetime(2020, 8, 26, 21, 16, 6, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 479169388, 'comment_body': 'This looks good, but we should make sure this also work when the user has multiple data_dir in the search path:\r\n\r\n```py\r\nif data_dir:\r\n  data_dirs = [data_dir]\r\nelse:\r\n  data_dirs = list_data_dirs()\r\n\r\nfor data_dir in data_dirs:\r\n  ...  # Return the first data dir found\r\n```\r\n\r\nhttps://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/core/constants.py#L56\r\n', 'comment_created': datetime.datetime(2020, 8, 28, 11, 34, 25, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 479189809, 'comment_body': ""If we have a folder with:\r\n\r\n```\r\ndata_dir/dataset/config/1.0.1\r\ndata_dir/dataset/1.0.0\r\n```\r\nAnd then call `find_builder_dir('dataset/config:1.0.0')`\r\n\r\nIt will return `data_dir/dataset/1.0.0` instead of None.\r\n\r\nWe should make sure to add a test case for this."", 'comment_created': datetime.datetime(2020, 8, 28, 11, 51, 58, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 479190027, 'comment_body': '`find_builder_dir`', 'comment_created': datetime.datetime(2020, 8, 28, 11, 52, 11, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 479198564, 'comment_body': ""`listdir` is not deterministic, so running the code twice may load different version, which is a bad bug. It's best to raise error or fail in case of ambiguity.\r\n\r\nTo select default config, for now, we could check `tfds.builder_cls('name').BUILDER_CONFIGS`, wrapped in `try/catch`\r\n"", 'comment_created': datetime.datetime(2020, 8, 28, 11, 59, 44, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 479204457, 'comment_body': ""This will fail: `['9.0.0', '10.0.0']` will return `9.0.0` instead of `10.0.0`. Could you add a test for this ?\r\n\r\nYou can use `Version('1.0.0')` as `>` will work correctly between two version objects\r\n\r\nAlso look at the DatasetBuilder version code to see how this is done and whether you can reuse part of the code."", 'comment_created': datetime.datetime(2020, 8, 28, 12, 5, 2, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 479212977, 'comment_body': 'We should add more test, specially when detecting failure mode. For instance\r\n* when the ds name, config name / version do not exists (e.g. typo in the name)\r\n* When a config is passed, but the dataset do not have one (`ds/config:1.2.0` but dataset is `ds:1.0.0`)\r\n* When `ds:1.0.0` is passed for a dataset with config `ds/config:1.0.0`\r\n* When no version is passed but multiple version exists (config and non-config version)\r\n\r\n', 'comment_created': datetime.datetime(2020, 8, 28, 12, 12, 19, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 479612847, 'comment_body': 'Done for now.\r\n\r\nIn the future, we will change this right? (as we are reading the dataset code)', 'comment_created': datetime.datetime(2020, 8, 29, 5, 58, 45, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 480109189, 'comment_body': ""Yes, I don't know what should be the good long term solution. Maybe adding a `dataset/info.json` file in the top folder ? Or somehow add some local clue to set a folder as the default one. Or remove default config and raise an error ?"", 'comment_created': datetime.datetime(2020, 8, 31, 12, 52, 52, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 480116589, 'comment_body': 'Or maybe we could just warn the users to specify the config name. The default behaviour would be to select the first config in the sorted order', 'comment_created': datetime.datetime(2020, 8, 31, 13, 6, 18, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 480121062, 'comment_body': ""Select the first config in sorted order is very dangerous, for instance, if one user generate `wikipedia/en`, and another user generate both `wikipedia/en` and `wikipedia/de`, using `tfds.load('wikipedia')` will load a different dataset for each users.\r\n\r\nI think it's best to be explicit in this case:\r\n* If the code is reachable, extract the default builderConfig\r\n* Otherwise, raise an error -> Please specify the config name\r\n"", 'comment_created': datetime.datetime(2020, 8, 31, 13, 14, 12, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 480187777, 'comment_body': 'Okay, got it.\r\nMade the changes', 'comment_created': datetime.datetime(2020, 8, 31, 14, 58, 4, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': 'fd5039e90b716461a996cdf1270a49dc13f4a1d9', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cb118c15c8e954b2004f6cceddb1dc300954dee6', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2b1622ea2d08e317b2f874d0ee898143c632dd09', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b90c0e5699912e9de1c6abd7d11cd1211fa7d894', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6d01a530ab7a2bc3a0ff9273aaf600a69a53440b', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fc1d0ddc118d520f5b793ba6a03160d5168a2d30', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '25bac0c06ae867e5a890a1e156d41e4cf159a04a', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
470179777,Load dataset without reading its generation code,Related https://github.com/tensorflow/datasets/issues/2138,False,2322,https://api.github.com/repos/tensorflow/datasets/pulls/2322,https://github.com/tensorflow/datasets/pull/2322,closed,117,21,5,8,2,12,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-08-19 13:52:15+00:00,2020-09-16 22:49:52+00:00,2451457.0,"28 days, 8:57:37","[{'comment_id': 473135081, 'comment_body': 'We should not direct import members.', 'comment_created': datetime.datetime(2020, 8, 19, 15, 54, 18, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 473138648, 'comment_body': ""Let's try to avoid duplication code with `tfds.load`. Let's also try to better layer this feature in smaller independent pieces.\r\n\r\nI would rename this `builder_from_directory(name, *, data_dir)` and remove all the `as_dataset` related things.\r\n\r\n1. (this PR) Add `builder_from_directory(builder_dir: str) -> tfds.core.DatasetBuilder`\r\n\r\n```python\r\nbuilder = tfds.core.builder_from_directory('path/to/my_dataset/config/1.0.0')\r\nds = builder.as_dataset(split='train')\r\n```\r\n\r\n2. Add a `find_builder_dir(dataset_name: str, *, data_dir: str) -> builder_dir`\r\n\r\n```python\r\nfind_builder_dir('my_dataset/config:1.2.0', data_dir='~/tensorflow_datasets/') == '~/tensorflow_datasets/my_dataset/1.2.0'\r\n\r\n# If the version is not specified, return the last available version\r\nfind_builder_dir('my_dataset', data_dir='~/tensorflow_datasets/') == '~/tensorflow_datasets/my_dataset/config/1.2.0'\r\n\r\n# If the dataset, config or version is not found, return None\r\nfind_builder_dir('my_dataset/config:1.3.0', data_dir='~/tensorflow_datasets/') == None\r\n```\r\n\r\nThe main issue here will be to infer the default config when not specified: `dataset` -> `dataset/default_config:x.y.z`. Let's not have this use case for now, we could fix it later.\r\n\r\n3. Integrate this with `tfds.load`.\r\n\r\n```python\r\n# If find_builder_dir found something, return it, otherwise load from the source code\r\nds = tfds.load('mnist')\r\n```\r\n"", 'comment_created': datetime.datetime(2020, 8, 19, 15, 59, 36, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 473154229, 'comment_body': 'Rather than duplicating code, we should try to reuse the implementation from `FileAdapterBuilder`. One way would be to split `FileAdapterBuilder` into two `class FileReaderBuilder(DatasetBuilder)` and `class FileAdapterBuilder(FileReaderBuilder)`', 'comment_created': datetime.datetime(2020, 8, 19, 16, 22, 7, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 473765447, 'comment_body': 'I would move this in a separate file.\r\n\r\nAlso I think `ConfigBuilder` is confusing with `BuilderConfig`. We should try to find a better name. Maybe `ReaderBuilder`, or `ReadOnlyBuilder` to emphasise the fact the builder can only be used on already generated datasets ?\r\n', 'comment_created': datetime.datetime(2020, 8, 20, 8, 35, 28, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 473771182, 'comment_body': ""Rather than calling two different methods, it might be best to move _get_builder_kwargs inside `ConfigBuilder`:\r\n\r\n```\r\nConfigBuilder('path/to/dataset/1.2.0')\r\n```"", 'comment_created': datetime.datetime(2020, 8, 20, 8, 41, 58, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 473775955, 'comment_body': 'When the dataset already exists, `download_and_prepare()` should be a no-op rather than raising an error. This is consistent with how other DatasetBuilder works.', 'comment_created': datetime.datetime(2020, 8, 20, 8, 46, 47, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 473832702, 'comment_body': 'It feels strange that the Builder require the parent dir.\r\nIs it not possible to overwrite `def _build_data_dir(self, given_data_dir):` to return `(given_data_dir, given_data_dir)` instead ?\r\nSimilarly, instead of extracting the version and config from the path, we should try to load those info from the dataset_info.json', 'comment_created': datetime.datetime(2020, 8, 20, 9, 54, 4, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 473834842, 'comment_body': ""If it's easier, maybe at first you can try to fully overwrite `__init__` and not call super() at all. Then we can see which modification would be required to DatasetBuilder.__init__ to support both regular and config-based loading."", 'comment_created': datetime.datetime(2020, 8, 20, 9, 56, 55, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 474037487, 'comment_body': 'Should I move this to `dataset_builder.py`?', 'comment_created': datetime.datetime(2020, 8, 20, 14, 43, 28, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 474086796, 'comment_body': '@Conchylicultor, I could not find any config related in `dataset_info.py`.\r\nAm I missing out on something?', 'comment_created': datetime.datetime(2020, 8, 20, 15, 46, 59, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 474572489, 'comment_body': ""Yes, I don't think there is currently, but we should add some `config_name` in the `tensorflow_datasets/core/proto/dataset_info.proto`. This should probably be done in a separate PR."", 'comment_created': datetime.datetime(2020, 8, 21, 9, 26, 23, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 474605685, 'comment_body': 'A new file would be best, as `dataset_builder.py` is already quite big. Also, it would allow to keep test separated.', 'comment_created': datetime.datetime(2020, 8, 21, 10, 11, 24, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': '800cd5d493fe9d0742bda6cae613679599a23f00', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a0df7543492ad9d429d26092a7e622b932019dfe', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2de864e4249892ae978d74556b06a4f2d9d7451d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'caa46ae577afc72bb787f63fa18703ba97564e45', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dc70ade420659d710c85ab4cb73f30a426a8b9fa', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7b6c98d10b9227f65a78fe58e8dd44c85268c9d8', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5142c5e389ca0f639cca19ce6d6a6a4b37203f60', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2949f21325d11e4e5e185e84374269e3fddf9fda', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
471657216,Add config_name field in dataset_info,Related to https://github.com/tensorflow/datasets/pull/2322#discussion_r474572489,True,2326,https://api.github.com/repos/tensorflow/datasets/pulls/2326,https://github.com/tensorflow/datasets/pull/2326,closed,32,15,4,2,0,3,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-08-21 14:09:54+00:00,2020-08-25 08:39:04+00:00,325750.0,"3 days, 18:29:10","[{'comment_id': 474727202, 'comment_body': ""The goal of this new field is to be able to restore the `config_name` when restoring a dataset from `builder_from_dir('path/to/dataset/')`. So that the following code does not need to rely on hardcoded folder structure.\r\n\r\n```python\r\nbuilder = builder_from_dir('path/to/dataset/')\r\nbuilder.info.full_name == 'my_dataset/config_name/1.2.0'\r\n```\r\n\r\nHere it seems that this new proto field `config_name` is never saved, right ?\r\n\r\n"", 'comment_created': datetime.datetime(2020, 8, 21, 14, 17, 39, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 474730875, 'comment_body': ""This should be saved in the `dataset_info.json`, but I'm not sure this should be exposed to the public API.\r\n\r\nI would remove this property and the `INFO_STR` as we do not need this for now. Internally, we can use `info.proto.config_name`\r\n"", 'comment_created': datetime.datetime(2020, 8, 21, 14, 23, 13, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 474767893, 'comment_body': 'I tested it locally. \r\nThe generated `dataset_info.json` file had a `configName` field. (when the dataset had a BuilderConfig)', 'comment_created': datetime.datetime(2020, 8, 21, 15, 24, 9, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': 'b4b2777a428a0f9c6ab282c2522a0ce2ea88f0f5', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c85c3d72c79438a72d1fde71bc1f2a04a1191f75', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
467479417,Add generate_statistics function,Related to https://github.com/tensorflow/datasets/issues/2299,False,2301,https://api.github.com/repos/tensorflow/datasets/pulls/2301,https://github.com/tensorflow/datasets/pull/2301,open,161,108,5,3,0,13,2,0,"[{'name': 'cla: yes'}, {'name': 'author:please_respond'}]",2020-08-13 16:01:20+00:00,,0.0,,"[{'comment_id': 470190691, 'comment_body': 'Calling `generate_statistics` should always generate statistics, so I would remove `compute_stats` kwargs.\r\n\r\nLike for `builder.download_and_prepare`, `generate_statistics` should auto-detect whether statistics have already been downloaded.', 'comment_created': datetime.datetime(2020, 8, 13, 19, 18, 48, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 470191136, 'comment_body': 'To check whether statistics are already computed, we should check for the presence or not of statistics.json', 'comment_created': datetime.datetime(2020, 8, 13, 19, 19, 37, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 470201289, 'comment_body': 'If we want to better layer this feature. Currently, statistics are split between visualization, dataset_info, dataset_builder.\r\n\r\nI would organize the code as follow, we should try to move most of the code inside a new `statistics.py` objet:\r\n\r\n* `core/statistics.py` with:\r\n\r\n```py\r\nclass StatisticsInfo:\r\n\r\n  def __bool__() -> bool:  # Return True if statistics are present, false otherwise.\r\n\r\n  def compute_and_save():\r\n\r\n  def _repr_html_():  # Display the FACET display\r\n\r\n  ...\r\n```\r\n\r\n* `core/dataset_info`:\r\n\r\n```py\r\nclass DatasetInfo:\r\n\r\n  def __init__():\r\n    self._statistics = StatisticsInfo(self)\r\n```\r\n\r\n* `core/dataset_builder`:\r\n\r\n```py\r\ndef generate_statistics():\r\n  if not info.statistics:\r\n    info.statistics.compute_and_save()\r\n```\r\n\r\nTo help with the implementation, I think both the current statistics code and the new stats can be kept for now, so you can just develop the new feature in some `info._statistics` and duplicate the code there. Once complete, we can delete the old code and have `info.statistics` returns `info._statistics`.\r\n', 'comment_created': datetime.datetime(2020, 8, 13, 19, 37, 24, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 470272576, 'comment_body': 'Required for Backward compatibility with the following option\r\n```\r\nDownloadConfig(compute_stats)\r\n```', 'comment_created': datetime.datetime(2020, 8, 13, 21, 59, 31, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 470273360, 'comment_body': 'This should have code to show statistics in the documentation\r\nOr\r\nCode from the visualisation `show_statistics` function\r\n', 'comment_created': datetime.datetime(2020, 8, 13, 22, 1, 29, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 470450708, 'comment_body': 'I would rewrite as:\r\n\r\n```py\r\nif download_config.compute_stats == download.ComputeStatsMode.FORCE:\r\n  self.info.statistics.compute_and_save()\r\nelif download_config.compute_stats == download.ComputeStatsMode.AUTO:\r\n      try:\r\n        import tensorflow_data_validation\r\n      except ImportError:\r\n        logging.info(\r\n      else:\r\n        self.generate_statistics()\r\n```', 'comment_created': datetime.datetime(2020, 8, 14, 7, 13, 39, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 470452443, 'comment_body': ""`.compute_dynamic_properties()` shouldn't be called anymore and should be removed."", 'comment_created': datetime.datetime(2020, 8, 14, 7, 18, 1, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 470458314, 'comment_body': ""`show_statistics`. It should use the `tfdv.utils.display_util.get_statistics_html` called in `tfds.visualize_statistics`: https://github.com/tensorflow/data-validation/blob/v0.22.2/tensorflow_data_validation/utils/display_util.py#L270-L290\r\n\r\nSo when the user will do on colab:\r\n```\r\ninfo.splits['train'].statistics\r\n```\r\nIt will display the FACET visualization, without having to call tfds.show_examples at all."", 'comment_created': datetime.datetime(2020, 8, 14, 7, 31, 57, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 470459651, 'comment_body': 'statistics_info_path should be private I believe so I would remove this function.', 'comment_created': datetime.datetime(2020, 8, 14, 7, 35, 4, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 470460548, 'comment_body': '`StatisticsInfo. _statistics_info_path` feel redundant. How about `_path` ?', 'comment_created': datetime.datetime(2020, 8, 14, 7, 37, 7, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 470462196, 'comment_body': ""I forgot that statistics are computed per-split, and not globally. This is gonna complexify the design a little bit.\r\n\r\nSo I think `SplitInfo.statistics` we should return `StatisticsInfo`.\r\n\r\n```py\r\nclass SplitInfo:\r\n\r\n  @utils.memoized_property(self) -> StatisticsInfo:\r\n  def statistics(self) -> StatisticsInfo:\r\n    return StatisticsInfo(self._dataset_info, self.split)\r\n```\r\n\r\nThen statistics could be saved per-split.\r\n\r\n```py\r\nfor split in info.splits.values():\r\n  split_info.statistics.compute_and_save()  # Save as statistics-train.json\r\n```\r\n\r\nAnd would be used:\r\n```\r\ninfo.splits[].statistics.as_proto()\r\ninfo.splits[].statistics.compute_and_save()\r\n```\r\n\r\nThe main difficulty is to forward the `DatasetInfo` to the split.\r\n\r\nWe should first have a separate PR to refactor `SplitInfo` to `utils.as_proto_cls(proto.SplitInfo)`, but this is gonna touch many parts of the code, so this is something I'll have to do.\r\nSo we should put this on hold until I submit the code. Sorry.\r\n"", 'comment_created': datetime.datetime(2020, 8, 14, 7, 41, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 470517262, 'comment_body': 'No problem 🙂\r\nWill work on something else in the meantime.', 'comment_created': datetime.datetime(2020, 8, 14, 9, 31, 58, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 470597844, 'comment_body': ""As I started looking into this, it seems that the reason why the `SplitInfo` is complicated was because the way `DatasetInfo` was written.\r\n\r\nCurrently, we need to keep `info.splits` and `info._info_proto` in sync, so updating `info.splits` require to notify `info._info_proto`. This makes the code hard to follow.\r\nIt's my fault for implementing this like that.\r\n\r\nWe should update `DatasetInfo` to be a `dataclasses.dataclass(frozen=True)`, and rather than using the proto as ground truth, use the DatasetInfo attributes. So we can update `info.splits` without having to update the proto.\r\n\r\nSo \r\n```py\r\nclass DatasetInfo:\r\n  ...\r\n\r\n  @property\r\n  def as_proto(self):\r\n    return self._info_proto\r\n\r\n  @property\r\n  def name(self):\r\n    return self.as_proto.name\r\n```\r\nwould become:\r\n```py\r\n@dataclasses.dataclass(frozen=True)\r\nclass DatasetInfo:\r\n  ...\r\n\r\n  name: str\r\n\r\n  @property\r\n  def as_proto(self):\r\n    return dataset_info_pb2.DatasetInfo(\r\n        name=self.name,\r\n    )\r\n```\r\n\r\nThere wouldn't be `_info_proto `, and `as_proto` would only be called when saving in `write_to_directory`, instead of for every attributes\r\n\r\nSome implementations details:\r\n* With `(frozen=True)`, because info is mutable, we can use `object.__setattr__(self, 'name', value)` when we need to update dataset info, like inside `read_from_directory`.\r\n* The current __init__ would be changed as post_init to cleanup the arguments:\r\n\r\n\r\n```py\r\n@dataclasses.dataclass(frozen=True)\r\nclass _DatasetInfo:\r\n  \r\n  def __post_init__(self):\r\n    object.__setattr__(self, 'version', self.builder.version)\r\n    if self.description:\r\n      object.__setattr__(self, 'description', utils.dedent(self.description))\r\n```\r\n\r\nYou can look into this if you want but it's possible that I missed something again which would make this more complicated than expected.\r\n"", 'comment_created': datetime.datetime(2020, 8, 14, 12, 37, 55, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': '73de71a95275ebdc824bb3df097904fc7faccb7a', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b79bdb662373694d0c163e50a3a94f6497e83832', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9ae50b0dc88c565cc7941162dd7573001c024531', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
464655970,Show Supported Python versions,We should show the supported Python version in [pypi classifiers](https://pypi.org/project/tensorflow-datasets/) and [README.md](README.md),True,2284,https://api.github.com/repos/tensorflow/datasets/pulls/2284,https://github.com/tensorflow/datasets/pull/2284,closed,7,0,2,1,0,4,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-08-07 15:06:16+00:00,2020-08-24 23:01:46+00:00,1497330.0,"17 days, 7:55:30","[{'comment_id': 467184059, 'comment_body': ""Does this mean we'll have to update this every time a new version came out ? Is there a way to indicate `3.6+` instead of listing all individual versions ?"", 'comment_created': datetime.datetime(2020, 8, 7, 17, 49, 1, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 467184650, 'comment_body': ""Or otherwise, we can we just keep the `Programming Language :: Python :: 3` and remove the other ones ? Currently, this make it look like we don't support future Python version."", 'comment_created': datetime.datetime(2020, 8, 7, 17, 50, 13, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 467269294, 'comment_body': 'I could not find any `3.6+` option [here](https://pypi.org/classifiers/)\r\n\r\nSo, unfortunately, if we want users to know, tfds only supports 3.6 or higher, we will have to list all python versions.', 'comment_created': datetime.datetime(2020, 8, 7, 21, 7, 44, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 467740556, 'comment_body': ""Thanks for confirming. In this case, it seems best to only list Python 3 rather than individual versions. I'll do the change when merging internally."", 'comment_created': datetime.datetime(2020, 8, 10, 7, 56, 21, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': '492690d577e9e00cc697be0abef5d0e4a701e25f', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
463456502,Add subsplit API for Better auto-sharding,"Fix https://github.com/tensorflow/datasets/issues/2133
TODO: 
- [x] Auto-apply the subsplit API if `num_shards` < `num_pipelines` and `input_context` is provided.

Usage Example:

```python
ds = tfds.load('mnist',
               split=tfds.subsplit(split_name='train', num_subsplits=2),
               read_config=read_config)

```",True,2278,https://api.github.com/repos/tensorflow/datasets/pulls/2278,https://github.com/tensorflow/datasets/pull/2278,closed,18,0,4,1,1,2,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-08-05 15:25:02+00:00,2020-08-18 13:35:24+00:00,1116622.0,"12 days, 22:10:22","[{'comment_id': 465937644, 'comment_body': 'If you merge the subsplits, what is the point of creating them in the first place ?\r\nI think this should be renamed subsplits and should return a list of str', 'comment_created': datetime.datetime(2020, 8, 5, 18, 58, 17, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 465938521, 'comment_body': 'Test ?', 'comment_created': datetime.datetime(2020, 8, 5, 18, 59, 44, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': '4329c14ab4df87adc383b91adb45952ab0d915ba', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
455176659,Load and dump FeatureConnectors,Related to https://github.com/tensorflow/datasets/issues/2138,True,2221,https://api.github.com/repos/tensorflow/datasets/pulls/2221,https://github.com/tensorflow/datasets/pull/2221,closed,288,20,19,16,4,27,3,0,"[{'name': 'enhancement'}, {'name': 'cla: yes'}, {'name': 'author:please_resolve_conflict'}]",2020-07-22 15:02:34+00:00,2020-08-18 13:06:57+00:00,2325863.0,"26 days, 22:04:23","[{'comment_id': 458896944, 'comment_body': '`save_config` name implies that the function actually write something on disk.\r\nIntuitively, I would expect:`save_config(path: str) -> None` and it would be consistent with `save_metadata`.\r\n', 'comment_created': datetime.datetime(2020, 7, 22, 15, 51, 58, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 458900838, 'comment_body': 'It\'s a draft so you\'ll probably had plan for it. In addition of the info, we should also save the feature type.\r\n\r\nI would suggest implementing something like: https://github.com/tensorflow/datasets/pull/2167/files#diff-9fbfec231798aaac7105e45f88153dffR47\r\n\r\nUsing `__init_subclass__`, you register a mapping `str` -> `FeatureConnector`.\r\n\r\nThen in `save_config`, you can save the `type(self).__name__`.\r\n\r\n```json\r\n{\r\n    ""type"": ""FeatureDict"",\r\n    ""content"": {\r\n        ""image"": {\r\n            ""type"": ""Image"",\r\n            ""shape"": [null, null, 3],\r\n            ...,\r\n        },\r\n        ""label"": {\r\n            ""type"": ""ClassLabel"",\r\n        },\r\n    },\r\n}\r\n```\r\n\r\nThen in addition of `save_config` / `from_config`, you could have `from_json` / `to_json`:\r\n\r\n```py\r\n@classmethod\r\ndef from_json(cls) -> FeatureConnnector:\r\n  # Implementation inspired from https://github.com/tensorflow/datasets/pull/2167/files#diff-9fbfec231798aaac7105e45f88153dffR72\r\n\r\ndef to_json(self) -> Json:\r\n  ...\r\n\r\n@classmethod\r\ndef from_config(cls, path: str):\r\n  with tf.io.gfile.GFile(path) as f:\r\n    return FeatureConnector.from_json(json.loads(f.read()))\r\n\r\ndef save_config(self, path: str):\r\n  with tf.io.gfile.GFile(path, \'w\') as f:\r\n    return json..dumps(path, self.to_json())\r\n```\r\n', 'comment_created': datetime.datetime(2020, 7, 22, 15, 57, 30, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 463253326, 'comment_body': 'This code should be moved in `FeatureDict.from_json`\r\n\r\nWith my proposal above:\r\n```py\r\n# In feature_dict.py\r\ndef from_json_content(self, value):\r\n  return cls({\r\n      k: FeatureConnector.from_json(v)\r\n      for k, v in value.items()\r\n  })\r\n```', 'comment_created': datetime.datetime(2020, 7, 30, 20, 30, 32, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 463253737, 'comment_body': ""After thinking more, making this `abstractmethod` is gonna break all users which have defined their custom feature connector.\r\n\r\nInstead, I think it's best to propose a default implementation, Copying my proposal from above:\r\n\r\n```py\r\ndef from_json_content(cls, value):\r\n  if value:\r\n    raise ValueError(\r\n        'Value should be empty. This indicates the FeatureConnector'\r\n        f' {cls} should overwrite `from_json_content`'\r\n    )\r\n  return cls()\r\n\r\ndef to_json_content(cls, value):\r\n  return {}\r\n```\r\n"", 'comment_created': datetime.datetime(2020, 7, 30, 20, 31, 25, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 463255676, 'comment_body': ""Let's move this file in a separate PR. This will help keep things separated.\r\n\r\n1. Update the FeatureConnector from / to methods.\r\n2. Update the BuilderConfig and the rest of TFDS to be compatible with this new config-based system\r\n"", 'comment_created': datetime.datetime(2020, 7, 30, 20, 35, 19, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 463261093, 'comment_body': ""I think the kwarg should be `data_dir`, so the builder_cls know where to load the config, but I haven't though deeply yet about this."", 'comment_created': datetime.datetime(2020, 7, 30, 20, 46, 12, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 463283687, 'comment_body': ""Just for info, you're returning a `dict`, but the return type should be `FeatureConnector`"", 'comment_created': datetime.datetime(2020, 7, 30, 21, 32, 45, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 463286459, 'comment_body': ""The `type` field seems to be duplicated across all features. As this is how feature class are detected, I think it is important to automate this field.\r\n\r\nI think we should have two method `from_json` and `from_json_content`. Here is my proposal:\r\n\r\nThe `FeatureConnector` implement the 4 methods:\r\n\r\n```py\r\ndef from_json(cls, value) -> 'FeatureConnector':\r\n  subclass = cls._REGISTERED_FEATURES.get(value['type'])\r\n  return subclass.from_json_content(value['content'])\r\n\r\ndef to_json(self, value):\r\n  return {\r\n      'type': type(self).__name__,\r\n      'content': self.to_json_content(),\r\n  }\r\n  \r\n  \r\n\r\ndef from_json_content(cls, value):\r\n  return cls(**value)\r\n\r\ndef to_json_content(cls, value):\r\n  return {}\r\n```\r\n\r\nThen subclasses would only implement the `to_json_content` and `from_json_content` optionally.\r\n\r\n```py\r\ndef to_json_content(self):\r\n  return {\r\n        'file_format': self._file_format,\r\n        'shape': list(self._shape),\r\n        'dtype': self._dtype.name,\r\n        'sample_rate': self._sample_rate,\r\n    }\r\n```"", 'comment_created': datetime.datetime(2020, 7, 30, 21, 38, 59, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 463290989, 'comment_body': ""Would it be possible to automate this test inside `assertFeature` ?\r\n\r\nRather than checking individual fields, I think we could check that exporting/loading the feature yield the same results:\r\n\r\n```py\r\ndef assertFeature(feature, **kwargs):\r\n  # Check the test with the feature\r\n  with subtest('feature'):\r\n    _assert_feature(feature, **kwargs)\r\n\r\n  # Check the same tests with the reconstructed feature\r\n  with subtest('exported_imported'):\r\n    new_feature = FeatureConnector.from_json(feature.to_json())\r\n    _assert_feature(new_feature, **kwargs)\r\n```\r\n\r\nThis way all features would be tested automatically. And we don't need to test the intermediate representation. If the reconstructed feature works, it's good !"", 'comment_created': datetime.datetime(2020, 7, 30, 21, 49, 4, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 463293252, 'comment_body': ""Maybe the `from_json_content` in the base class could be simplified even further:\r\n\r\n```py\r\n# In features.py\r\ndef from_json_content(cls, value):\r\n  return cls(**value)\r\n```\r\n\r\nThis way, many FeatureConnector wouldn't have to implement `from_json_content` at all."", 'comment_created': datetime.datetime(2020, 7, 30, 21, 54, 24, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 464001952, 'comment_body': 'Added a similar test.\r\n\r\n```python\r\n  def _assert_feature(self, feature):\r\n    new_feature = feature.from_json_content(feature.to_json_content())\r\n    self.assertEqual(str(feature), str(new_feature))\r\n``` ', 'comment_created': datetime.datetime(2020, 8, 1, 21, 27, 54, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 464358777, 'comment_body': ""> Added a similar test.\r\n\r\nThis test is good, but feel fragile and don't really test that the feature is correctly exported/imported. Not all informations are contained in the `str`.\r\n\r\nFor instance, with ClassLabel, your implementation would not test that label names are correctly restored.\r\n"", 'comment_created': datetime.datetime(2020, 8, 3, 11, 38, 41, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 464365269, 'comment_body': 'I believe you could remove the `_shape` attribute in `__init__` and use `self.shape` directly.', 'comment_created': datetime.datetime(2020, 8, 3, 11, 53, 24, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 466950709, 'comment_body': 'This feels confusing and might create argument collision with the other args form assertFeature. It would be cleaner / more explicit to have something like:\r\n\r\n```py\r\nassertFeature(\r\n    test_imported_attrs=dict(\r\n        _file_format=None,\r\n        sample_rate=None,\r\n    ),\r\n)\r\n```\r\n', 'comment_created': datetime.datetime(2020, 8, 7, 10, 13, 49, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 466958346, 'comment_body': ""Could you add typing annotations for the value too, and to_json_content,... return types ?\r\n\r\nAs this would be used in multiple files, please add it to: https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/core/utils/type_utils.py\r\n\r\n```py\r\nJsonValue = Union[str, bool, int, float, List['JsonValue'], Dict[str, 'JsonValue']]\r\nJson = Dict[str, JsonValue]\r\n```\r\nThen\r\n```py\r\nfrom tensorflow_datasets.core.utils import type_utils\r\n\r\nJson = type_utils.Json\r\n\r\ndef def to_json(self) -> Json:\r\n```\r\nPlease not the direct import to type_utils and the fact that we are not importing member directly (contrary to `from typing import Xyz`). This is because some of our internal tools do not works well with direct member import."", 'comment_created': datetime.datetime(2020, 8, 7, 10, 31, 12, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 466962973, 'comment_body': 'Could you set `sample_rate` line 39 to make sure this is properly imported ?', 'comment_created': datetime.datetime(2020, 8, 7, 10, 42, 20, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 466965217, 'comment_body': ""Usually for unused arguments, we're writing the full argument name with `del arg  # unused` bellow: See https://google.github.io/styleguide/pyguide.html#214-decision:~:text=Unused%20argument%20warnings%20can%20be%20suppressed,it.%20%E2%80%9CUnused.%E2%80%9D%20is%20sufficient.%20For%20example%3A\r\n\r\n`_` break callers that pass arguments by name and do not enforce that the arguments are actually unused. Also this allow other user to understand what this argument was supposed to be."", 'comment_created': datetime.datetime(2020, 8, 7, 10, 47, 40, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 466973374, 'comment_body': ""Subclasses should not use the parent internal variable. Don't the following works ?\r\n\r\n```py\r\nreturn cls({\r\n    k: FeatureConnector.from_json(v)\r\n    for k, v in value.items()\r\n})\r\n```"", 'comment_created': datetime.datetime(2020, 8, 7, 11, 6, 47, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 466975342, 'comment_body': 'Same comment here', 'comment_created': datetime.datetime(2020, 8, 7, 11, 11, 37, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 466979677, 'comment_body': ""This is always empty. I would replace this by something like:\r\n\r\n```\r\nassert not kwargs, 'Json export/import should be updated'\r\n```"", 'comment_created': datetime.datetime(2020, 8, 7, 11, 22, 47, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 466980779, 'comment_body': ""I don't understand why this need to be so complicated. Why not\r\n\r\n```py\r\ndef to_json_content(self) -> Json:\r\n  return {\r\n      'length': self._length,\r\n      'feature': self.feature.to_json(),\r\n  }\r\n\r\ndef from_json_content(cls, value: Json) -> 'Sequence':\r\n  return cls(FeatureConnector.from_json(value['feature']), length=value['length'])\r\n```\r\n"", 'comment_created': datetime.datetime(2020, 8, 7, 11, 25, 38, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 467216199, 'comment_body': 'This makes it look like `Json` is defined in feature while this is defined in `type_utils`. This looks strange to me similarly to if tensorflow was imported with `tf = feature.tf`.\r\nIt would be more explicit to import type_utils here and elsewhere.', 'comment_created': datetime.datetime(2020, 8, 7, 18, 58, 9, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 467216993, 'comment_body': 'I would add the sample rate in the above test, and leave this one empty. We might add a test in the future which check that the sample rate from the wav and Feature match.', 'comment_created': datetime.datetime(2020, 8, 7, 18, 59, 49, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 467217246, 'comment_body': '(minor) `del value  # Unused`', 'comment_created': datetime.datetime(2020, 8, 7, 19, 0, 22, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 467219656, 'comment_body': ""I don't see the changes ?"", 'comment_created': datetime.datetime(2020, 8, 7, 19, 4, 50, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 467220010, 'comment_body': 'Length should be keyword-only argument', 'comment_created': datetime.datetime(2020, 8, 7, 19, 5, 36, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 467996094, 'comment_body': 'Done', 'comment_created': datetime.datetime(2020, 8, 10, 15, 38, 33, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '3c42e7463d6b2eaa4da1921ee1c303c4ab85f8ef', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8b16fcb017d2ace853f90300e704826cd4784328', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3ae9e74d13fd9e285b8fae08f7e8776fd7a6d5b2', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7f151c2934a7202111325fe3d0ab9e372938c55d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c634e2e439b8611eb110db69203d4741fc96512f', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '56c2a419bc1b6a94bed8c33d96ee281654913e92', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '109dbf79c0996db2ad29d4b2db73cc0b17357b68', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2300e3fc43d648fffcf622d256016c8a12164e2e', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '39f75253430e6cd3a3b9bb4168914bbb6c437a14', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '741daf3ee5822ab15619bfcda750cc42ac665932', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '85de45e5ec5e8d29a4c1fba355d953b3dc0d17e6', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '185e40aa9e9d95324b0ae43e1905e1488fd79420', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7c6e76914460932aabf89f2bc34ce3a3b72c228a', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '83218b7cb41cbcd3b4800313ecd8fe33b846205a', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cc4879c318f5e23391c32fc313f8fce52aa44459', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '70cae0f9cdd8b241ae7e5e272747355537453a1f', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
448580985,Add new tfds cli command,Related to #2161,False,2194,https://api.github.com/repos/tensorflow/datasets/pulls/2194,https://github.com/tensorflow/datasets/pull/2194,closed,269,3,4,6,1,9,2,0,"[{'name': 'cla: yes'}, {'name': 'author:please_resolve_conflict'}]",2020-07-14 00:56:13+00:00,2020-08-01 07:20:03+00:00,1578230.0,"18 days, 6:23:50","[{'comment_id': 455213119, 'comment_body': 'Thank you for adding this.\r\nThis is a good occasion to update the generation script with more modern Python feature. For instance, using f-string. `textwraps.dedent`, and `pathlib.Path` instead of `tf.io.gfile`', 'comment_created': datetime.datetime(2020, 7, 15, 17, 25, 25, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 455213454, 'comment_body': ""As we drop Python 2 support, this shouldn't be required anymore"", 'comment_created': datetime.datetime(2020, 7, 15, 17, 25, 59, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 455214290, 'comment_body': 'I think we should auto-fill the docstring, like\r\n\r\n```\r\n""""""DatasetBuilder for {dataset_name} dataset.""""""\r\n```\r\n', 'comment_created': datetime.datetime(2020, 7, 15, 17, 27, 22, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 455215377, 'comment_body': 'Lets make this a required arg', 'comment_created': datetime.datetime(2020, 7, 15, 17, 28, 28, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 455216843, 'comment_body': ""Let's not support multiple dataset creation for now as it doesn't seems a common use case. And it seems inconsistent with other CLI like `poetry new <project_name>`"", 'comment_created': datetime.datetime(2020, 7, 15, 17, 29, 51, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 455240720, 'comment_body': 'We should use dataclasses now', 'comment_created': datetime.datetime(2020, 7, 15, 18, 2, 53, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 455247087, 'comment_body': 'I think it would be helpful to add a `--dst_dir` flag of type `pathlib.Path` which default to Path.cwd()', 'comment_created': datetime.datetime(2020, 7, 15, 18, 13, 11, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 459738492, 'comment_body': 'Dataclasses seems to complicate the template generation as we format the string using `**data`.', 'comment_created': datetime.datetime(2020, 7, 23, 21, 28, 46, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 459760467, 'comment_body': 'But not if we use f-string, no ?\r\n\r\n```py\r\nfile_path.write_text(textwrap.dedent(f""""""\\\r\nfrom tensorflow.dataset.{data.name} import {data.cls_name}\r\n""""""))\r\n```', 'comment_created': datetime.datetime(2020, 7, 23, 22, 21, 25, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': '59026a5c9e28a7e21d1e971db7f8baad0489dc66', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c22828168863f60bcab0aaeb63a39b1b1b4530f9', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7d58bc5f05b760e6d05f2039ceb328359af0db2c', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a0b69c72b07529977d4ee1b6eae92035ae3b536a', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4afc23009c0bebc0e4e771f7c78529697fcca06d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c014e4f06d5ffdf7ce6df78fd29e56783f1baa71', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
447957731,Use f-strings to generate dataset doc,"Update catalog generation script to not use mako (stack trace harder to debug, no pylint, no syntax highlighting).",True,2186,https://api.github.com/repos/tensorflow/datasets/pulls/2186,https://github.com/tensorflow/datasets/pull/2186,closed,422,333,4,7,2,12,2,0,"[{'name': 'cla: yes'}, {'name': 'author:kokoro_test_failed'}]",2020-07-12 21:33:11+00:00,2020-07-24 21:45:56+00:00,1037565.0,"12 days, 0:12:45","[{'comment_id': 453950093, 'comment_body': 'Formatting of `""""""` looks bad. Would it be possible to wrap all of those in `tfds.core.utils.dedent` which apply `textwrap.dedent` + `strip` to normalize formatting. ', 'comment_created': datetime.datetime(2020, 7, 13, 21, 41, 37, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 453950694, 'comment_body': 'At least for the main function, it would be nice to have typing annotation if possible.', 'comment_created': datetime.datetime(2020, 7, 13, 21, 43, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 453951121, 'comment_body': 'I think this should be moved out of `template`, as template was the folder for the mako files.', 'comment_created': datetime.datetime(2020, 7, 13, 21, 43, 56, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 453953063, 'comment_body': 'Relying on Global makes the code harder to read and is usually bad practice. Instead, those could be passed as argument, using `functools.partial` bellow.', 'comment_created': datetime.datetime(2020, 7, 13, 21, 47, 4, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 453954772, 'comment_body': 'Now that doc script is Python 3, using `dataclasses` would be cleaner. And declaration should be moved outside of the function.\r\n', 'comment_created': datetime.datetime(2020, 7, 13, 21, 49, 31, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 456201432, 'comment_body': 'Is there a drawback of using tfds.core.utils.dedent everywhere, as it will strip extra whitespace in addition ?', 'comment_created': datetime.datetime(2020, 7, 17, 3, 42, 47, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 456201633, 'comment_body': ""Couldn't dedent be applied automatically without having to specify it in each functions ?"", 'comment_created': datetime.datetime(2020, 7, 17, 3, 43, 45, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 456203548, 'comment_body': 'I believe dedent is already applied in the Dataset info, no ?', 'comment_created': datetime.datetime(2020, 7, 17, 3, 52, 25, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 456204312, 'comment_body': 'As the description could be multiple lines, I think you need to use the solution from https://stackoverflow.com/a/57175149/4172685', 'comment_created': datetime.datetime(2020, 7, 17, 3, 55, 54, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 458448166, 'comment_body': 'The problem is it removes the required newlines', 'comment_created': datetime.datetime(2020, 7, 21, 23, 34, 11, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 458452852, 'comment_body': 'If you feel motivated, I think it would make sense to refactor this as subclass, to have:\r\n\r\n```py\r\nclass HomepageSection(Section):\r\n\r\n  def get_key(self, builder):\r\n    return builder.info.homepage\r\n\r\n  def display(self, builder):\r\n    return f""""""\\\r\n      *   **Homepage**: [{builder.info.homepage}]({builder.info.homepage})\r\n    """"""\r\n```\r\n\r\nAnd here:\r\n\r\n```py\r\nall_sections: List[Section] = [\r\n    ConfigDescriptionSection(),\r\n    HomepageSection(),\r\n    VersionSection(nightly_doc_util),\r\n    ...\r\n]\r\n```\r\n\r\nPreviously it wasn\'t possible as mako do not support class.\r\n\r\nI should have catched this sooner so no worries if you don\'t want to do it. I can try when merging this internally.', 'comment_created': datetime.datetime(2020, 7, 21, 23, 49, 14, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 459737115, 'comment_body': 'Done', 'comment_created': datetime.datetime(2020, 7, 23, 21, 25, 42, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '86ee24198f6aabf8b054a16ab251c3e34c2b3445', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '760059eee7c26c601e2b199180074944ebeff94d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6adb3b5d71ed17d156777fcd13fab473058cc733', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bf918fe96718fc6e8d3fc2e8f43224cb45af4428', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a0df6ea63a008f67e1ed508a6aa304fe9b5b30c7', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c8804a63fece7a67006a7420a5ada14869f4bcf4', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fbb2722b75292a35a764ed922b9a0bb35330a243', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
447864504,Add TFDS CLI,"Related to https://github.com/tensorflow/datasets/issues/2161

Example Usage

```
➜  tfds --help   
usage: tfds [-h] [-v]

Tensorflow Datasets CLI tool

optional arguments:
  -h, --help     show this help message and exit
  -v, --version  show program's version number and exit
```
",True,2185,https://api.github.com/repos/tensorflow/datasets/pulls/2185,https://github.com/tensorflow/datasets/pull/2185,closed,53,0,2,1,1,3,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-07-12 07:42:42+00:00,2020-07-26 17:02:33+00:00,1243191.0,"14 days, 9:19:51","[{'comment_id': 453956227, 'comment_body': 'See the example that I linked in #2161 to see how to pass the argument to the main function. There should be some `args_parser= _get_parser` here.', 'comment_created': datetime.datetime(2020, 7, 13, 21, 51, 46, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 453958683, 'comment_body': ""Could you split in multiple PRs ? (one to only add the base CLI, one for each of the feature). It will be easier to review.\r\n\r\nHere it's not clear why `testing/my_dataset/my_dataset.py` are added with the CLI as the 2 seems completely unrelated."", 'comment_created': datetime.datetime(2020, 7, 13, 21, 55, 41, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 454010033, 'comment_body': 'Just added them to show the result of \r\n```\r\ntfds new my_dataset.py\r\n```\r\n\r\nSo, that you could suggest any changes required in the template.', 'comment_created': datetime.datetime(2020, 7, 13, 23, 39, 52, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '8f721a82b75fd0092ee310d5ecde8c7cce644696', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
442995630,Add overwrite flag in `generate_visualization`,,True,2144,https://api.github.com/repos/tensorflow/datasets/pulls/2144,https://github.com/tensorflow/datasets/pull/2144,closed,4,2,1,1,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'author:kokoro_test_failed'}]",2020-07-01 21:18:48+00:00,2020-07-23 17:19:23+00:00,1886435.0,"21 days, 20:00:35",[],"[{'commit_sha': '4fd3ad912711355aafe9216bf5b9a29aa32e94a3', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
442427792,CleanUp use tf.nest.map_structure,Use `tf.nest.map_structure` instead of `py_utils.map_nested`,True,2142,https://api.github.com/repos/tensorflow/datasets/pulls/2142,https://github.com/tensorflow/datasets/pull/2142,closed,14,13,6,1,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'author:kokoro_test_failed'}]",2020-07-01 04:08:58+00:00,2020-07-01 21:50:52+00:00,63714.0,17:41:54,[],"[{'commit_sha': 'e47a86a5c3a855c3f9fdf5b252052f65de7ada0b', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
442378342,Support datasets in folders with checksums & fake_data,"Related to https://github.com/tensorflow/datasets/issues/2139

Required directory structure for datasets outside TFDS:
```
path/to/dataset/
    my_dataset.py
    my_dataset_test.py
    my_dataset.txt
    fake_examples/
 ```

TODO:
- [ ] Update [Define the dataset outside TFDS](https://www.tensorflow.org/datasets/add_dataset#define_the_dataset_outside_tfds) doc.
- [ ] Remove unnecessary flags in `download_and_prepare` script",True,2141,https://api.github.com/repos/tensorflow/datasets/pulls/2141,https://github.com/tensorflow/datasets/pull/2141,closed,85,4,7,5,0,13,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-07-01 00:58:26+00:00,2020-07-24 19:28:31+00:00,2053805.0,"23 days, 18:30:05","[{'comment_id': 448083858, 'comment_body': ""I don't think we should add a new flag.\r\nTFDS should automatically detect whether the checksums,... are defined, then fallback to default values otherwise.\r\n"", 'comment_created': datetime.datetime(2020, 7, 1, 2, 40, 27, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 448084726, 'comment_body': 'Adding a checksums dir may have a significant overhead if many datasets are loaded at the same time. In addition, checksums are memoized, so new directories will be ignored.\r\n\r\nInstead, we should update `download_and_prepare` or `DownloadManager` to check whether some `checksums.tsv` is present locally, and load checksums from this file if so. Otherwise, fallback to legacy checksums.\r\n', 'comment_created': datetime.datetime(2020, 7, 1, 2, 44, 16, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 451019406, 'comment_body': ""This doesn't seems to be the right place to load the checksums. We should move this when `self._url_infos` is initialised (if `checksums_path ` is set, then load from path, otherwise, load global checksums).\r\n\r\nCurrently, if the dataset is downloading 1000 images, it will check 1000 times for the path existance."", 'comment_created': datetime.datetime(2020, 7, 7, 17, 13, 18, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 451019621, 'comment_body': 'Thank you. Would it be possible to add a test (create a dummy dataset with dummy checksums and dummy fake data) to make sure this works properly ?', 'comment_created': datetime.datetime(2020, 7, 7, 17, 13, 41, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 451026258, 'comment_body': 'It seems that we want to access the file location without having to instantiate the class, so this should be a class member (e.g. set through https://docs.python.org/3/reference/datamodel.html#object.__init_subclass__, or through a memoized class property)\r\n\r\n', 'comment_created': datetime.datetime(2020, 7, 7, 17, 24, 58, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 451027040, 'comment_body': '`_get_url_infos` should be renamed `get_url_infos` if it is now part of the public API.', 'comment_created': datetime.datetime(2020, 7, 7, 17, 26, 16, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 452508882, 'comment_body': 'One line docstring and typing annotations ?', 'comment_created': datetime.datetime(2020, 7, 9, 21, 47, 21, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 452509136, 'comment_body': 'This will raise an error if this if checksums_path is None', 'comment_created': datetime.datetime(2020, 7, 9, 21, 47, 57, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 452509429, 'comment_body': '`DATASET_CLASS.code_dir()`', 'comment_created': datetime.datetime(2020, 7, 9, 21, 48, 47, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 452591158, 'comment_body': ""Rather than inheriting from an existing dataset, I think it's best to write a small dataset from scratch (similar to the one in `test_utils.py`). \r\nOtherwise, this introduce a dependency `CatsVsDogs` -> `core tests` which seems to break tests boundary.\r\nIt's not be clear here if the tests would be passing because the original `CatsVsDogs` pass, or if tests are actually using the new relative path (e.g. cats vs dog url is already registered)."", 'comment_created': datetime.datetime(2020, 7, 10, 2, 37, 10, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 455744442, 'comment_body': 'We also want to use a dummy url here (e.g. http://dummy.org/data.txt). Otherwise, the tests might still pass as url is registered in checksums.get_all_url_infos()', 'comment_created': datetime.datetime(2020, 7, 16, 12, 21, 32, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 455748165, 'comment_body': ""After thinking more about this, it would be nice to rename this `code_path` and return a pathlib.Path.\r\n\r\nIt would allow to write `builder.code_path.name` or `builder.code_path.parent / 'checksums.txt'` which seems nicer.\r\n\r\nI also think it should returns the python file, not the directory. Indeed, there are some few cases where getting the path will be helpful, like documenting the source code location, or for the refactoring script."", 'comment_created': datetime.datetime(2020, 7, 16, 12, 28, 13, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 455753857, 'comment_body': ""As for checksums, this should also be automatically detected. The user shouldn't have to manually set this."", 'comment_created': datetime.datetime(2020, 7, 16, 12, 38, 35, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': 'd65d683d976b1a83c4a85b1964290d2e168ecf1d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c667ca42430ebf0f65e3e6d001631d5476d31261', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e37328e8ccb2cad33db1957517bad98182c7b55b', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '86252a304c57abc777cb88c63dc61dea78e69fba', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e22b3e7033de329576f44b361229441f139af11c', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
435901727,Add Custom Translate Datasets template,Fix https://github.com/tensorflow/datasets/issues/2092 and also https://github.com/tensorflow/datasets/issues/1402 ,True,2094,https://api.github.com/repos/tensorflow/datasets/pulls/2094,https://github.com/tensorflow/datasets/pull/2094,closed,272,0,10,3,0,8,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:is_reviewing'}]",2020-06-17 14:46:55+00:00,2020-06-25 17:15:05+00:00,700090.0,"8 days, 2:28:10","[{'comment_id': 443755459, 'comment_body': ""Rather than nested dir, why haven't you kept your proposal which seems better `xyz.train.txt` ?"", 'comment_created': datetime.datetime(2020, 6, 22, 18, 42, 15, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443766371, 'comment_body': ""Avoid magic numbers. It's best to be explicit (`[:len('.ext')]`)"", 'comment_created': datetime.datetime(2020, 6, 22, 19, 4, 3, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443783114, 'comment_body': 'Docstring with `Returns:`', 'comment_created': datetime.datetime(2020, 6, 22, 19, 39, 7, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443784762, 'comment_body': 'As per image_folder.py, I think `_split_examples` would be a better name than `_languages_examples` as it would indicates that dict is mapping `split_name`->`examples`', 'comment_created': datetime.datetime(2020, 6, 22, 19, 42, 38, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443785735, 'comment_body': 'Why not reuse the utils from `image_folder` ? Those utils could be moved in py_utils or in some `io_utils`', 'comment_created': datetime.datetime(2020, 6, 22, 19, 44, 40, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443786835, 'comment_body': 'You only need the languages in `__init__`. No need to load all files (e.g. loading the train set if only the eval set is used).\r\n\r\nYou could only extract dict `split -> filepaths` and load the required files in `_as_datasets`', 'comment_created': datetime.datetime(2020, 6, 22, 19, 47, 5, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443824686, 'comment_body': 'We will have to read all the file content to update the split info num_examples, right?', 'comment_created': datetime.datetime(2020, 6, 22, 21, 3, 4, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 443832351, 'comment_body': ""I see. I guess it's fine to load the full data then."", 'comment_created': datetime.datetime(2020, 6, 22, 21, 18, 56, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': 'df21279b9c9969859a705e46e68c727bd2da92e9', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b700c8d604c931ce06e5705963aec152822190a7', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9f14d97230c291bebd4aa4637242db3d87e59dfe', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
434268599,Script to Benchmark dataset,"## Description
A util script for benchmarking dataset
",False,2088,https://api.github.com/repos/tensorflow/datasets/pulls/2088,https://github.com/tensorflow/datasets/pull/2088,closed,155,0,1,4,2,24,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:is_reviewing'}]",2020-06-15 03:45:54+00:00,2020-06-25 14:21:54+00:00,902160.0,"10 days, 10:36:00","[{'comment_id': 440367360, 'comment_body': ""In addition to the total time, what's more important than the is the number of images/sec as it allow to compare datasets together."", 'comment_created': datetime.datetime(2020, 6, 15, 18, 31, 17, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 440367865, 'comment_body': 'You can use `tfds.builder` with `builder.download_and_prepare()` + `builder.as_dataset()`', 'comment_created': datetime.datetime(2020, 6, 15, 18, 32, 15, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 440368207, 'comment_body': ""Rather than epoch, it's best to have `num_examples`"", 'comment_created': datetime.datetime(2020, 6, 15, 18, 32, 48, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 440373831, 'comment_body': ""We want to compare different pipeline together, with different `batch_size`, `split='train[10%:]'`, `ds = ds.map`,... so the current signature seems too restrictive."", 'comment_created': datetime.datetime(2020, 6, 15, 18, 43, 17, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 440375667, 'comment_body': ""We'll want to compare TF1 / TF2 to see if there is performance drop with eager."", 'comment_created': datetime.datetime(2020, 6, 15, 18, 46, 40, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 440376627, 'comment_body': 'The first iteration may have additional overhead so we might want to count it separately (We should display first iteration time, img/sec for the next x iteration, total time).', 'comment_created': datetime.datetime(2020, 6, 15, 18, 48, 38, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 440378182, 'comment_body': 'We want to allow user to import the script to benchmark their own pipeline.', 'comment_created': datetime.datetime(2020, 6, 15, 18, 51, 31, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 441433101, 'comment_body': 'Other methods\r\n- Pass `num_examples` as a function parameter. (For TFDS datasets, we will easily get it from `info.splits[split_name].num_examples`)\r\n- Calculate `num_examples` in the epoch loop below (Might effect benchmarking time a bit).', 'comment_created': datetime.datetime(2020, 6, 17, 10, 5, 3, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 441488349, 'comment_body': 'So should we benchmark the dataset twice?\r\n\r\nSomewhat like\r\n```\r\nbenchmark(ds, epochs)\r\ntf.compat.v1.disable_eager_execution()\r\nbenchmark(ds, epochs)\r\n```', 'comment_created': datetime.datetime(2020, 6, 17, 11, 54, 44, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 441949232, 'comment_body': 'Eager/non eager mode should be an option of the benchmark util. If the user want to test the two option, they should call the util twice.', 'comment_created': datetime.datetime(2020, 6, 18, 3, 34, 38, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 441949876, 'comment_body': ""We shouldn't auto-compute the number of images for the dataset size. Instead, we want to compute the benchmark of only 10.000 images for a variety of datasets (e.g. first 10.000 images on imagenet, cifar,...)\r\nNum images should be an option of the benchark (which can default to cardinality if not set)"", 'comment_created': datetime.datetime(2020, 6, 18, 3, 37, 25, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 441950383, 'comment_body': ""Let's not iterate twice over the full dataset as it may takes multiple hours so we don't want to double the time."", 'comment_created': datetime.datetime(2020, 6, 18, 3, 39, 37, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 441950643, 'comment_body': 'We should compute the first batch time separately.', 'comment_created': datetime.datetime(2020, 6, 18, 3, 40, 41, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 441951154, 'comment_body': 'No need to benchmark all splits. Lets only benchmark train by default.', 'comment_created': datetime.datetime(2020, 6, 18, 3, 42, 42, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 441951547, 'comment_body': 'We could try to add tqdm so the user can see the progression bar with the live statistics. I think tqdm should add a negligeable overhead (and we can add an option to disable it)', 'comment_created': datetime.datetime(2020, 6, 18, 3, 44, 34, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 441951742, 'comment_body': 'I would rename this `_make_dataset`', 'comment_created': datetime.datetime(2020, 6, 18, 3, 45, 17, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 441952842, 'comment_body': 'Or if for you this correspond to the number of trial to average the performance across multiple trials, maybe rename this `NUM_EXECUTIONS` or `NUM_REPETITIONS` or `NUM_TRIALS`.', 'comment_created': datetime.datetime(2020, 6, 18, 3, 50, 2, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443757409, 'comment_body': ""Note: split accept absolute values `'train[1200:1350]'`"", 'comment_created': datetime.datetime(2020, 6, 22, 18, 46, 27, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443757756, 'comment_body': 'The subsplit API will have bad performance drawback on the API.', 'comment_created': datetime.datetime(2020, 6, 22, 18, 47, 11, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443772558, 'comment_body': 'As explained in https://github.com/tensorflow/datasets/pull/2088#discussion_r440376627 and https://github.com/tensorflow/datasets/pull/2088#discussion_r441950643\r\nwe should compute the first iteration separately. \r\n\r\nBy first iteration/first batch, I meant first iteration of the dataset, not the first trial.\r\n', 'comment_created': datetime.datetime(2020, 6, 22, 19, 16, 49, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443775442, 'comment_body': 'When renaming `EPOCH` -> `NUM_EXECUTION`, all instance of `epoch` should be renamed. Even if I only post a single review comment. I assume that the fix is applied globally.', 'comment_created': datetime.datetime(2020, 6, 22, 19, 22, 48, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 444285216, 'comment_body': 'Sorry, I misunderstood it.\r\nFixed it now.', 'comment_created': datetime.datetime(2020, 6, 23, 14, 50, 20, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 444403251, 'comment_body': 'Sorry for the confusion. By first iteration, I meant:\r\n\r\n```python\r\niter_ds = iter(ds)\r\nnext(iter_ds)  # First warmup batch/iteration\r\nfor n in iter_ds:  # Other iterations\r\n  print(n)\r\n```\r\n', 'comment_created': datetime.datetime(2020, 6, 23, 17, 51, 9, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 444541036, 'comment_body': 'ohk..got confused by iteration / batch.\r\nFixed', 'comment_created': datetime.datetime(2020, 6, 23, 22, 21, 42, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': 'd131ed13bd4f64839033a597a75a2690ce37925f', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6f36525c5da0fe912c89790a51b9ea9e7cc3d110', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4a64b2963ab7f0d49ea399677834a2a867b753e4', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c52ef977402aba50e3ca78d42720489777bde97c', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
426246030,Support for Custom Image Datasets,Fix https://github.com/tensorflow/datasets/issues/2067,False,2068,https://api.github.com/repos/tensorflow/datasets/pulls/2068,https://github.com/tensorflow/datasets/pull/2068,closed,325,230,9,12,5,79,2,0,"[{'name': 'enhancement'}, {'name': 'cla: yes'}]",2020-06-01 22:24:10+00:00,2020-06-20 21:20:36+00:00,1637786.0,"18 days, 22:56:26","[{'comment_id': 434056855, 'comment_body': ""I don't think the ImageFolder need a name"", 'comment_created': datetime.datetime(2020, 6, 2, 17, 41, 17, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434057133, 'comment_body': ""If anything, this should reuse constants.DATA_DIR.\r\nBut here we probably don't want to have a default data_dir at all (should be mandatory)"", 'comment_created': datetime.datetime(2020, 6, 2, 17, 41, 42, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434058668, 'comment_body': '(minor) For consistency with new code, we should try to use single quote. This is the default Python `str` `repr`.', 'comment_created': datetime.datetime(2020, 6, 2, 17, 44, 23, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434065903, 'comment_body': 'Utils functions which are only used in this module should start by `_decode_img` (the utils bellow where wrongly named)', 'comment_created': datetime.datetime(2020, 6, 2, 17, 56, 30, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434066253, 'comment_body': '`_AUTOTUNE`', 'comment_created': datetime.datetime(2020, 6, 2, 17, 57, 3, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434068730, 'comment_body': ""For consistency with other datasets, I don't think we should convert image to float, but keep uint8"", 'comment_created': datetime.datetime(2020, 6, 2, 18, 0, 28, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434083780, 'comment_body': ""After thinking more about it, it might be best to have `ImageFolder` inherit from `tfds.core.DatasetBuilder` but we should list all caveat/pain point to make this works properly and found the best implementation:\r\n\r\n* It shouldn't be added to the register (maybe useing the `registered.skip_registration` ?).\r\n* The __init__ signature should be different (only contains `data_dir=`)\r\n* Data dir shouldn't be `DATA_DIR/dataset_name/config/version` but the given version.\r\n* `download_and_prepare` should raise an error if called.\r\n* `DatasetInfo`  fields & cie are dynamically computed in `__init__`\r\n* Other ?\r\n\r\nFor the implementation, it seems there are three options:\r\n\r\n1. Do not inherit at all from `DatasetBuilder`\r\n2. Inherit from `DatasetBuilder` and hack around the API.\r\n3. Split DatasetBuilder in two class `DatasetBase` and `DatasetBuilder`\r\n\r\nWithout trying, it's difficult to know which one of those solutions is the best. My feeling would be to try 2 first and see what are the hack required (overwrite `__init__`,...). If it is really too hard, eventually prototype what would need to split `DatasetBase` and `DatasetBuilder` in 2 class and have `DatasetBuilder(DatasetBase)` and `ImageFolder(DatasetBase)`.\r\n"", 'comment_created': datetime.datetime(2020, 6, 2, 18, 17, 33, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434086236, 'comment_body': 'The list of images should be shuffled deterministically. (So that two users have examples in the same order).\r\n\r\nWhen `shuffle_files=True`, there should be an additional `ds = ds.shuffle(len(image_paths))` after the `tf.data.Dataset.list_files`.\r\n', 'comment_created': datetime.datetime(2020, 6, 2, 18, 20, 20, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434088664, 'comment_body': 'Okay, sure.', 'comment_created': datetime.datetime(2020, 6, 2, 18, 23, 1, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 434133266, 'comment_body': 'Just for clarification, should I try this first \r\n* `ImageFolder` inherit from `tfds.core.GeneratorBasedBuilder`\r\n\r\nor this\r\n* Inherit from `DatasetBuilder` and hack around the API?', 'comment_created': datetime.datetime(2020, 6, 2, 19, 42, 42, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 434148067, 'comment_body': 'I meant inherit from `DatasetBuilder`, not `GeneratorBasedBuilder`. Sorry for the confusion. I updated the text', 'comment_created': datetime.datetime(2020, 6, 2, 20, 11, 30, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434207284, 'comment_body': ""> Data dir shouldn't be DATA_DIR/dataset_name/config/version but the given version.\r\n\r\nData dir should be `DATA_DIR` or `DATA_DIR/version` or something else?\r\n\r\nI think we should not copy any files and just read the images from the user-specified location."", 'comment_created': datetime.datetime(2020, 6, 2, 22, 21, 26, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 434208633, 'comment_body': 'Data dir should be the path where the images are located. There is no version.\r\nIf images are located in: `path/to/images/train/img0001.jpeg`, then data_dir should be `path/to/images/`', 'comment_created': datetime.datetime(2020, 6, 2, 22, 25, 19, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434208809, 'comment_body': ""Yes, there shouldn't be any copy. Just read the plain images."", 'comment_created': datetime.datetime(2020, 6, 2, 22, 25, 50, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434945580, 'comment_body': '(minor) Pylint requires 2 empty lines between functions in the global scope:\r\n\r\n```\r\ndef f():\r\n  return\r\n\r\n\r\ndef g():\r\n  return\r\n```', 'comment_created': datetime.datetime(2020, 6, 4, 1, 42, 35, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434946183, 'comment_body': ""I don't think there is an easy way to infer the shape. I would not set the shape."", 'comment_created': datetime.datetime(2020, 6, 4, 1, 44, 44, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434946436, 'comment_body': 'Can you move this outside of `ImageLabelFolder` ?\r\n\r\n`def _get_split_label_images(data_dir: str) -> ...`', 'comment_created': datetime.datetime(2020, 6, 4, 1, 45, 44, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434946531, 'comment_body': 'For new source code, we should try to add typing annotations (https://docs.python.org/3/library/typing.html). Which works with Pytype (https://github.com/google/pytype).\r\n\r\nOne internal limitation though is that typing is only valid for function signnature, not variables:\r\n```py\r\ndef f(x: int) -> None:  # Works\r\n  y: int  # Fail\r\n```', 'comment_created': datetime.datetime(2020, 6, 4, 1, 46, 8, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434947776, 'comment_body': 'Split names and labels should be sorted. Otherwise, running twice your datasets on different environment may return different ids for the same label.\r\n', 'comment_created': datetime.datetime(2020, 6, 4, 1, 51, 23, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434948363, 'comment_body': 'Similarly this function could be moved outside.\r\n\r\nThis makes it more explicit what are the method inputs/outputs, and guarantee that the method does not silently modify the `self`, which makes it easier to parse the source code.\r\n', 'comment_created': datetime.datetime(2020, 6, 4, 1, 53, 40, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434948539, 'comment_body': 'Remove `** kwargs` extra space', 'comment_created': datetime.datetime(2020, 6, 4, 1, 54, 28, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434949073, 'comment_body': '`self.info.splits.keys()`\r\n\r\nBut this check should be moved in `__init__`', 'comment_created': datetime.datetime(2020, 6, 4, 1, 56, 35, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434949236, 'comment_body': 'We should add unittests', 'comment_created': datetime.datetime(2020, 6, 4, 1, 57, 22, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434949495, 'comment_body': '> is this correct?\r\n\r\nYes, a single shard sounds good.', 'comment_created': datetime.datetime(2020, 6, 4, 1, 58, 13, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434949672, 'comment_body': 'What happens if you do not set this ? Is it required ?', 'comment_created': datetime.datetime(2020, 6, 4, 1, 58, 59, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434950463, 'comment_body': 'Can you remove `self._split_dict` and add `self.info. update_splits_if_different()` in `__init__` ?', 'comment_created': datetime.datetime(2020, 6, 4, 2, 2, 9, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 434951497, 'comment_body': 'We should try avoid adding too many new `self._` attributes. Some seems to duplicate information (e.g. `self._splits_dict` VS `self.info.splits` VS `self._splits`).\r\nI think they could be merged to only use `self.info.splits`.', 'comment_created': datetime.datetime(2020, 6, 4, 2, 6, 10, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 435027968, 'comment_body': '```\r\nbuilder = tfds.ImageLabelFolder(path)\r\nds = builder.as_dataset(split=""train"")\r\n```\r\nWe won\'t have this split argument in `__init__`\r\n', 'comment_created': datetime.datetime(2020, 6, 4, 6, 48, 57, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 435035792, 'comment_body': ""We get the following error\r\n```\r\nAssertionError: DatasetBuilder image_label_folder does not have a defined version. Please add a `VERSION = tfds.core.Version('x.y.z')` to the class.\r\n```"", 'comment_created': datetime.datetime(2020, 6, 4, 7, 5, 37, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 435052105, 'comment_body': 'Removed the `self._` and made them local variables.\r\nI think they cannot be merged to only use `self.info.splits` because they are class properties.', 'comment_created': datetime.datetime(2020, 6, 4, 7, 37, 31, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 435055130, 'comment_body': 'Now `_list_folder` and `_list_imgs` return sorted lists', 'comment_created': datetime.datetime(2020, 6, 4, 7, 43, 18, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 435056533, 'comment_body': 'Done', 'comment_created': datetime.datetime(2020, 6, 4, 7, 45, 56, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 435394579, 'comment_body': ""Note: You don't need to add the `Licence` header. It will be automatically added."", 'comment_created': datetime.datetime(2020, 6, 4, 16, 35, 7, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 435396743, 'comment_body': 'I would rename `ImageFolder`', 'comment_created': datetime.datetime(2020, 6, 4, 16, 38, 30, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 435646341, 'comment_body': 'Logic like this should be moved in `setUpClass(cls)` inside `ImageLabelFolderTest ` (and make sure to call `super().setUpClass()`)\r\n', 'comment_created': datetime.datetime(2020, 6, 5, 1, 46, 7, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 435646909, 'comment_body': ""Can't `self.info` be called instead ?"", 'comment_created': datetime.datetime(2020, 6, 5, 1, 48, 30, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 435647165, 'comment_body': 'I would move this back in `_info` to avoid duplication between `self.info` and `self._ds_info`', 'comment_created': datetime.datetime(2020, 6, 5, 1, 49, 32, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 435647837, 'comment_body': 'Could you add unittest for this function ? Note I think that you can use `tfds.testing.MockFs()` to avoid having to create real files/directories.', 'comment_created': datetime.datetime(2020, 6, 5, 1, 52, 16, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 435648725, 'comment_body': 'We should raise an error here.\r\nAlso could you add unittest to make sure the error is correctly raised ?\r\n\r\nIf this was changed for the test, then the test should be modified to make this a no-op', 'comment_created': datetime.datetime(2020, 6, 5, 1, 55, 35, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 435649375, 'comment_body': 'I would wrap this inside `with core.registered.skip_registration()` so that `image_folder` cannot be created from `tffds.load` (has to be explicitly defined)', 'comment_created': datetime.datetime(2020, 6, 5, 1, 57, 55, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 435649845, 'comment_body': 'I would move this test in a separate `tfds.testing.TestCase` class to separate `DatasetBuilderTestCase ` from `TestCase`', 'comment_created': datetime.datetime(2020, 6, 5, 1, 59, 45, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 435670012, 'comment_body': 'I was overriding a test in `DatasetBuilderTestCase`.\r\nSo, should I disable this test and add it to a separate class? ', 'comment_created': datetime.datetime(2020, 6, 5, 3, 24, 22, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 435815146, 'comment_body': 'I had to fix MockFs listdir function a bit.\r\nMaking a new PR for the same.', 'comment_created': datetime.datetime(2020, 6, 5, 9, 52, 28, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 435880262, 'comment_body': 'Same', 'comment_created': datetime.datetime(2020, 6, 5, 12, 13, 15, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 435880306, 'comment_body': 'I will remove this file later\r\nAdd here just for the tests to pass.', 'comment_created': datetime.datetime(2020, 6, 5, 12, 13, 20, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 436186815, 'comment_body': 'Note: `Exception` is too generic. Usually, we should raise `RuntimeError` or `NotImplementedError`', 'comment_created': datetime.datetime(2020, 6, 5, 22, 9, 51, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436187798, 'comment_body': ""We should avoid `tf.data.Dataset.from_generator` as this can't be exported as pure graph and won't works with TPU (they cannot execute Python code, only TF graph). Instead, you could use\r\n\r\n```\r\nds = tf.data.Dataset.from_tensor_slices(filepaths)\r\nds = ds.map(_read_and_decode_imgs)\r\n```"", 'comment_created': datetime.datetime(2020, 6, 5, 22, 13, 11, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436191464, 'comment_body': ""Note: you don't need to disable protected-access inside test files."", 'comment_created': datetime.datetime(2020, 6, 5, 22, 26, 26, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436199650, 'comment_body': 'Alias name should be more explicit. `func` is too generic', 'comment_created': datetime.datetime(2020, 6, 5, 22, 59, 49, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436199869, 'comment_body': 'What happen if test split contains a `label4` ?', 'comment_created': datetime.datetime(2020, 6, 5, 23, 0, 47, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436200171, 'comment_body': ""Could you also tests that \r\n\r\n```\r\nimg_folder = ImageFolder(data_dir)\r\nimg_folder.split[].num_examples\r\nimg_folder.features['label'].names\r\n```\r\nAre correct ?"", 'comment_created': datetime.datetime(2020, 6, 5, 23, 2, 19, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436200449, 'comment_body': 'It would be cleaner to use mocking here.\r\n\r\nAlso add `tearDownClass` to stop the mock.', 'comment_created': datetime.datetime(2020, 6, 5, 23, 3, 39, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436201035, 'comment_body': 'I would also add the `filename` in the feature dict', 'comment_created': datetime.datetime(2020, 6, 5, 23, 6, 45, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436201606, 'comment_body': 'See `DatasetBuilderTestCase` implementation for examples on how to use mock', 'comment_created': datetime.datetime(2020, 6, 5, 23, 9, 42, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436201777, 'comment_body': 'Thank you for fixing the bug.', 'comment_created': datetime.datetime(2020, 6, 5, 23, 10, 35, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436202158, 'comment_body': '`functools.partial(ImageFolder.__init__, root_dir= _EXAMPLE_DIR)`', 'comment_created': datetime.datetime(2020, 6, 5, 23, 12, 34, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436918109, 'comment_body': ""Why add `kwargs` if it's not used ?"", 'comment_created': datetime.datetime(2020, 6, 8, 18, 45, 46, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436918836, 'comment_body': ""This function should be executed as pure TF tf.graph, so we should not use `.numpy()` here, nor python dict indexing `.str2int`.\r\nUsing python:\r\n* Will make the pipeline much slower.\r\n* Won't be possible to execute this on TPU (which cannot execute Python code)\r\n"", 'comment_created': datetime.datetime(2020, 6, 8, 18, 47, 5, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436919331, 'comment_body': 'This should returns a `dict(image=, label=,...)` matching the `info.features` keys', 'comment_created': datetime.datetime(2020, 6, 8, 18, 48, 5, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436922758, 'comment_body': 'We should not use `py_function`. Instead use `ds = Datasets.from_tensor_slices((img_paths, label_ids))`\r\n', 'comment_created': datetime.datetime(2020, 6, 8, 18, 54, 21, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436923053, 'comment_body': 'We should make this a function and move it outside `ImageFolder`.\r\n\r\nIf you need to pass additional inputs, add them as keywords here and use `functools.partial`', 'comment_created': datetime.datetime(2020, 6, 8, 18, 54, 54, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436924050, 'comment_body': 'I think we could merge _process_ds and _map_fn', 'comment_created': datetime.datetime(2020, 6, 8, 18, 56, 47, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436924258, 'comment_body': ""Why is it required ? If output don't have the expected shape, this is likely a bug in the implementation."", 'comment_created': datetime.datetime(2020, 6, 8, 18, 57, 13, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436925321, 'comment_body': 'Rather than kwargs, I would explicitly list the arguments which are ignored, so we can see which features are not supported.', 'comment_created': datetime.datetime(2020, 6, 8, 18, 59, 19, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436925878, 'comment_body': 'The shuffling should happen before the images are loaded in memory, otherwise this will load the full dataset in memory.', 'comment_created': datetime.datetime(2020, 6, 8, 19, 0, 17, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436936951, 'comment_body': 'So, is there a way to know the integer corresponding to the label?\r\nI used this approach mainly because I used  `.str2int`', 'comment_created': datetime.datetime(2020, 6, 8, 19, 11, 54, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 436938584, 'comment_body': 'okay, will try this', 'comment_created': datetime.datetime(2020, 6, 8, 19, 13, 24, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 436940533, 'comment_body': 'I had to use this because of the current approach.\r\n`py_function` is not able to determine the output shape.', 'comment_created': datetime.datetime(2020, 6, 8, 19, 15, 7, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 436951173, 'comment_body': ""Yes, which is another reason why we shouldn't use py_function"", 'comment_created': datetime.datetime(2020, 6, 8, 19, 30, 58, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436951639, 'comment_body': 'Order is important. Labels should be a list', 'comment_created': datetime.datetime(2020, 6, 8, 19, 31, 51, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 436994863, 'comment_body': 'We get extra args like `data_dir`, `config` in tests.\r\nI added `kwargs` here because I could not find a way to handle these extra args by `functools`. ', 'comment_created': datetime.datetime(2020, 6, 8, 20, 54, 26, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 437018475, 'comment_body': 'If this is only for tests, then the test init function should be patched, not the original file.\r\nIf functools do not works, then use a function.', 'comment_created': datetime.datetime(2020, 6, 8, 21, 43, 46, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 437107768, 'comment_body': ""If the function do not use `self`, it's best to make it a simple function rather than a method. I help the user understand the function do not have side effects & cie."", 'comment_created': datetime.datetime(2020, 6, 9, 2, 48, 10, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 437108095, 'comment_body': ""We should also raise an `NotImplementedError('Xyz not supported with {}'.format(type(self).__name__))` error when decoder or read_configs are passed"", 'comment_created': datetime.datetime(2020, 6, 9, 2, 49, 43, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 437108585, 'comment_body': 'I think we could pass the full original path here, without the split/join.', 'comment_created': datetime.datetime(2020, 6, 9, 2, 51, 39, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 437109530, 'comment_body': 'You could use `_split_label_images[split].items()` to extract both labels & images at once.\r\n\r\nThen use `images, labels = zip(*inputs)` to unpack them', 'comment_created': datetime.datetime(2020, 6, 9, 2, 55, 11, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 437110366, 'comment_body': 'Why is it required ?', 'comment_created': datetime.datetime(2020, 6, 9, 2, 58, 41, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 437110762, 'comment_body': 'What about `tearDownClass` to restore the original functions ?\r\nOr I think there might be some `self.addCleanup()`', 'comment_created': datetime.datetime(2020, 6, 9, 3, 0, 9, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 437432356, 'comment_body': 'I tried this... but could not fix the tests.\r\nTests keep on failing on this line in `dataset_builder.py`\r\n\r\n```\r\nds = ds.prefetch(tf.data.experimental.AUTOTUNE)\r\n```\r\n```\r\nError: NoneType object has no prefetch method\r\n```', 'comment_created': datetime.datetime(2020, 6, 9, 13, 47, 22, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '8cfe79fc926da953e90f95b887f4dc1230283da0', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f67bdf796808f25303d8d7d0a313d90bfeec1dec', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '12f861aee16b4396d747d897f9c456eb6cdc5eb4', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3bdf61c96923e2b3c714a9a668de5bedaa61a4fe', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '54c9684f32f7714a65dd77c59eff3f90972a3b9c', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7f12334f14fa3be3d7f5a0d8aae37c57adf20b97', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4f0fb550ebba9176d8cbb3cc5c7b61abcec12bab', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cac6bbb12ad8a96713af402c9af9b928cb87951f', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1e907fc26c17381c038ae83d97dfffc84f9d8df1', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4d37ffa060c48c8a66e6fa68c3c9032888517172', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9343c227c05c25f0c4b6434295fc638446e8efa8', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0add4d49b400c3974ed0ae6dceef3f9e50ef2098', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
408774262,Add try_import context manager,Fix https://github.com/tensorflow/datasets/issues/1945,False,1947,https://api.github.com/repos/tensorflow/datasets/pulls/1947,https://github.com/tensorflow/datasets/pull/1947,open,148,5,4,17,1,48,3,0,"[{'name': 'enhancement'}, {'name': 'cla: yes'}, {'name': 'tfds:is_reviewing'}]",2020-04-24 21:31:27+00:00,,0.0,,"[{'comment_id': 414877040, 'comment_body': 'Please use `contextlib.contextmanager`', 'comment_created': datetime.datetime(2020, 4, 24, 21, 36, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 414878293, 'comment_body': 'Could you inherit from ModuleType ?', 'comment_created': datetime.datetime(2020, 4, 24, 21, 38, 39, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 414879013, 'comment_body': 'Rather than creating a new symbol, I would add a `__call__` to the existing `LazyImporter`.', 'comment_created': datetime.datetime(2020, 4, 24, 21, 40, 19, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 414879641, 'comment_body': ""Unfortunately, this won't works if there is multiple imports:\r\n\r\n```py\r\nwith lazy_imports():\r\n  import abc\r\n  import bcd\r\n\r\nbcd.some_function()\r\n```\r\n\r\nI think you'll have to add a import hook"", 'comment_created': datetime.datetime(2020, 4, 24, 21, 41, 30, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 414880081, 'comment_body': 'The assertion should only wrap the code which fails:\r\n```py\r\nwith lazy_imports():\r\n  import some_module\r\n\r\nwith self.assertRaisesWithPredicateMatch(ImportError, ""extras_require""):\r\n  some_module.some_function()\r\n```', 'comment_created': datetime.datetime(2020, 4, 24, 21, 42, 25, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 414881131, 'comment_body': 'Thank you for adding a test. I would also add a test were the import succeed.', 'comment_created': datetime.datetime(2020, 4, 24, 21, 44, 43, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 414891551, 'comment_body': ""I believe we should only accept whitelisted deps to be registered. Accepting any deps may be more difficult for the user to debug if she/he makes a typo. Instead we should add some `_ALLOWED_LAZY_DEPS = ['matplotlib', ...]`\r\n```py\r\nwith lazy_imports():\r\n  import matplotlib  # Works\r\n  import invalid_import  # Raise 'Unknown import {name}. If you believe this is correct, please add it to the whitelisted imports'\r\n```\r\n"", 'comment_created': datetime.datetime(2020, 4, 24, 22, 11, 16, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 414989881, 'comment_body': 'will do this soon', 'comment_created': datetime.datetime(2020, 4, 25, 5, 56, 13, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415225325, 'comment_body': '@Conchylicultor Most of the part of the PR is completed.\r\nHowever, I am running into infinite recursion when I try to load `FakeModule(fullname)` instead of a `imp.new_module(fullname)`.\r\nCould you please guide me through this issue.\r\n\r\nThank you', 'comment_created': datetime.datetime(2020, 4, 26, 5, 56, 44, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415226133, 'comment_body': 'I think `return super(FakeModule, self).__getattribute__(_)` can avoid infinite recursion error', 'comment_created': datetime.datetime(2020, 4, 26, 6, 1, 50, tzinfo=datetime.timezone.utc), 'commenter': 'Eshan-Agarwal', 'type': 'User'}, {'comment_id': 415229472, 'comment_body': ""That won't be required as I am already raising an `ImportError`.\r\nProgram execution stops as soon as an exception is encountered."", 'comment_created': datetime.datetime(2020, 4, 26, 6, 22, 43, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415232822, 'comment_body': ""Okay Try this  replace `self.module_name` by `_name ` where `_name=object.__getattribute__(self, 'module_name') `"", 'comment_created': datetime.datetime(2020, 4, 26, 6, 41, 23, tzinfo=datetime.timezone.utc), 'commenter': 'Eshan-Agarwal', 'type': 'User'}, {'comment_id': 415249753, 'comment_body': 'Thanks for the help.\r\nI used `_getattribute__` function instead of `_getattr__`.\r\nIssue resolved. :smile:', 'comment_created': datetime.datetime(2020, 4, 26, 8, 12, 6, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415249801, 'comment_body': 'Issue resolved', 'comment_created': datetime.datetime(2020, 4, 26, 8, 12, 23, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415321798, 'comment_body': 'Use contextlib.contextmanager with try/except.', 'comment_created': datetime.datetime(2020, 4, 26, 14, 21, 41, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415322545, 'comment_body': ""I don't think `LazyImporter` code should be mixed with the importer hook. I would move all the hook logic inside a `LazyImportHook` class"", 'comment_created': datetime.datetime(2020, 4, 26, 14, 25, 39, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415322672, 'comment_body': 'This adds the import hook globally, while we only want to add it within the lazy context manager.', 'comment_created': datetime.datetime(2020, 4, 26, 14, 26, 13, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415323524, 'comment_body': 'Why are you adding this `FAKE_PATH_TRIGGER` ? Please add comments when you write something non trivial which might be confusing for readers.', 'comment_created': datetime.datetime(2020, 4, 26, 14, 30, 35, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415325472, 'comment_body': 'use `utils.reraise(prefix=)` as we want to keep the original stacktrace.', 'comment_created': datetime.datetime(2020, 4, 26, 14, 39, 14, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415326199, 'comment_body': ""If you want to test the module was loaded properly, you should test something like `hasattr(matplotlib, 'function_name')`"", 'comment_created': datetime.datetime(2020, 4, 26, 14, 42, 38, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415328062, 'comment_body': ""Thank you. To make sure the lazy importer isn't globally added, could you also add a test to make sure importing outside the lazy importer still works ?\r\n```py\r\nimport nltk\r\nself.assertTrue(hasattr(nltk, 'function'))\r\n```\r\nAnd that unknown import still fails, even when added to `_ALLOWED_LAZY_DEPS`.\r\n```py\r\n_ALLOWED_LAZY_DEPS.append('valid_module')\r\nself.assertRaisesRegex(ImportError, 'Import error \\'valid_module\\''):\r\n  import valid_module\r\n```\r\n"", 'comment_created': datetime.datetime(2020, 4, 26, 14, 51, 42, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415541654, 'comment_body': 'Ideally, when `x` gets added, importing `x.y` should be valid too without having to whitelist all individual members. Is it possible ?', 'comment_created': datetime.datetime(2020, 4, 27, 6, 23, 24, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415549783, 'comment_body': 'Currently, this would be called `tfds.core.lazy_imports.lazy_importer()` while we want it to be called `tfds.core.lazy_imports()`. I would rename this `__call__` and change the line bellow to:\r\n\r\n```\r\nlazy_imports = LazyImporter()\r\n```', 'comment_created': datetime.datetime(2020, 4, 27, 6, 40, 45, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415550674, 'comment_body': 'Could you comment why `__path__` is set to `None` here if you overwrite it to `[]` bellow. And explain why updating `__path__` in the first place ? Especially as those features from Python are rarely used.', 'comment_created': datetime.datetime(2020, 4, 27, 6, 42, 35, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415553318, 'comment_body': ""I still don't understand why this is required ? What error do you get if this is removed ?"", 'comment_created': datetime.datetime(2020, 4, 27, 6, 48, 5, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415553720, 'comment_body': 'Could you add a comment why the hook is register as `path_hooks` rather than `sys.meta_path` ?', 'comment_created': datetime.datetime(2020, 4, 27, 6, 48, 56, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415561025, 'comment_body': ""If I understand the implementation correctly, if executing:\r\n\r\n```py\r\nwith lazy_imports():\r\n  import x\r\n```\r\nAnd that `x` internally imports `y`:\r\n```py\r\n# x/__init__.py\r\nimport y\r\n```\r\nI'm a little concerned that `y` gets lazy imported even if this is internal code that TFDS shouldn't modify.\r\nDo you think it would be possible to make the importer such as only the top-most level gets lazy imported ?"", 'comment_created': datetime.datetime(2020, 4, 27, 7, 3, 10, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415562789, 'comment_body': 'Actually, whitelisting individual members seems safer to avoid typo.', 'comment_created': datetime.datetime(2020, 4, 27, 7, 6, 33, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415564671, 'comment_body': 'Although `sys.meta_path` is a more powerful, our needs can be fulfilled by `sys.path_hooks`', 'comment_created': datetime.datetime(2020, 4, 27, 7, 10, 19, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415567554, 'comment_body': 'We can remove the constant, however, a path is passed to `__init__` of the class.\r\nThis path variable is given to `path_hook` just to raise an import error if that particular path item should not be handle by that particular loader.\r\n\r\nI think we can simplify this in our case', 'comment_created': datetime.datetime(2020, 4, 27, 7, 15, 27, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415569387, 'comment_body': ""Actually, after more though, I think your first solution of creating a new symbol was the right one, but the name should be changed to `maybe_import()` or `try_import`. I think that lazy_import isn't a great name for this feature as the imports are not lazy.\r\n\r\n```\r\nwith tfds.core.try_import():\r\n  import abc\r\n```\r\nSorry for reversing my review."", 'comment_created': datetime.datetime(2020, 4, 27, 7, 18, 32, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 415570874, 'comment_body': ""Yeah, we don't need to change the value of `__path__`. Sorry, about that.\r\n\r\n`__path__` variable was accessed by some `importlib` module functions. So I had to create it to prevent `__getattr__` from throwing any errors.\r\n"", 'comment_created': datetime.datetime(2020, 4, 27, 7, 21, 13, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415571696, 'comment_body': 'Sure, no problem. \r\nReverting it will require only copy/pasting :smile: ', 'comment_created': datetime.datetime(2020, 4, 27, 7, 22, 32, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415572189, 'comment_body': 'So, no changes needed, right?', 'comment_created': datetime.datetime(2020, 4, 27, 7, 23, 21, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415574090, 'comment_body': 'I was surprised to discover that ModuleType does not have `__path__`.\r\nIs there a better parent `FakeModule` can inherit from? ', 'comment_created': datetime.datetime(2020, 4, 27, 7, 26, 22, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415577199, 'comment_body': 'Also, `import matplotlib` does not import `matplotlib.pylot`.\r\nSo, I thought it is better to whitelist modules like `matplotlib`', 'comment_created': datetime.datetime(2020, 4, 27, 7, 31, 34, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 415583456, 'comment_body': 'I will look into this.', 'comment_created': datetime.datetime(2020, 4, 27, 7, 41, 54, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 416826770, 'comment_body': ""`PIL_Image` Isn't a valid module name"", 'comment_created': datetime.datetime(2020, 4, 28, 18, 20, 13, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 416826892, 'comment_body': 'Yes, no change needed.', 'comment_created': datetime.datetime(2020, 4, 28, 18, 20, 26, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 416828444, 'comment_body': '`os` could be removed. I think it was only used for testing.', 'comment_created': datetime.datetime(2020, 4, 28, 18, 22, 47, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 417515422, 'comment_body': ""After thinking about this, I think it may be better to patch the built-in `__import__()` directly.\r\n* The importer hook implementation seems too complicated to correctly handle edge cases.\r\n* We do not want to add the lazy imports to `sys.modules`, so it seems we're breaking some assumptions of the hook protocol\r\n\r\nYou can use `with py_utils.temporary_assignment(built_in, '__import__', new_import):` to patch temporary the built-in."", 'comment_created': datetime.datetime(2020, 4, 29, 18, 14, 13, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 417653164, 'comment_body': 'I can definitely do this.\r\nHowever, if please review the new implementation using `sys.meta_path`.\r\nIf you think this is complicated, I will switch to the suggested method', 'comment_created': datetime.datetime(2020, 4, 29, 22, 37, 30, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 417660977, 'comment_body': ""The issue with `sys.meta_path` is that it won't handle correctly the nested case (lazy-imports inside the import). Also, the lazy imports shouldn't be registered in `sys.modules` as it might break others libs.\r\n\r\nWe should add a test to make sure that nested work correctly:\r\n\r\n```py\r\nwith self.assertRaisesRegex(ImportError, 'y not found'):\r\n  with maybe_imports():\r\n    import x\r\n```\r\nWith:\r\n* ALLOWED_DEPS is x, y\r\n* x is installed, but not y\r\n* `x.py`:\r\n```py\r\n# x.py\r\nimport y\r\n# y should raise ImportError, instead of being a dummy module\r\n```\r\n"", 'comment_created': datetime.datetime(2020, 4, 29, 22, 59, 6, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 418243214, 'comment_body': 'original_import_fun should be the `__import__` function at the time the `with try_import` is called. Not as import time.\r\nOtherwise it might creates issue if other libraries also try to patch `__import__`:\r\n\r\n```\r\nwith user_custom_import():\r\n  with tfds.core.try_import():\r\n    import x\r\n  import y  # Oups, use the built-in import instead of the user_custom_import\r\n```', 'comment_created': datetime.datetime(2020, 4, 30, 19, 36, 19, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 418244127, 'comment_body': '`raise ImportError` -> `raise`. We want to forward the original error message and stacktrace', 'comment_created': datetime.datetime(2020, 4, 30, 19, 38, 11, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 418244997, 'comment_body': 'The `try/except` statement should go inside the `with` statement.', 'comment_created': datetime.datetime(2020, 4, 30, 19, 39, 52, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 418248450, 'comment_body': ""This test is good, so we should keep it. However this doesn't test nested import. Importing `matplotlib` do not try to import `matplotlib.pyplot`.\r\n\r\nWe should add a test to make sure that nested work correctly:\r\n```py\r\nwith self.assertRaisesRegex(ImportError, 'y not found'):\r\n  with tfds.core.maybe_imports():\r\n    import x\r\n```\r\nWith:\r\n* ALLOWED_DEPS is x, y\r\n* x is installed, but not y\r\n* x.py:\r\n```py\r\n# x.py\r\nimport y\r\n# y should raise ImportError, instead of being a dummy module\r\n```\r\nTo test this, you'll have to create a dummy `x.py` file.\r\n\r\nBasically this test that if `x` is installed but `import x` fails, then adding `with tfds.core.maybe_imports()` should fail too.\r\n\r\nWith the current implementation, x will be imported but executing x will fail because `x.y` is a dummy module as x hasn't been designed to work with dummy modules.\r\n"", 'comment_created': datetime.datetime(2020, 4, 30, 19, 46, 38, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 420724989, 'comment_body': 'Done.\r\nI was not sure where to create the `dummy_module_x.py`, so created it in the same directory.', 'comment_created': datetime.datetime(2020, 5, 6, 11, 40, 20, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': 'e98606f04565b69fee70736dad9150f65fe01edf', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6cedf80fa28223f9e3de6f3d1e4c44b6e3146c74', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '624915084b420699454ca385ec91eba2730f2d37', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e6b71e0b00d44feffcacf7e1f3829dad7a96ca21', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a0afea8dfe0aeaba2ba8256ba694fd5ced3a2ff3', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4cf5faac8ab1c1b99a28047d094793f67c2e492d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5872d416afa123fede36a810dcb690af06cd2a07', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bdc9df06ac4c76de3fd2303b8e5dd67184d3743f', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '18e4b27fa5464b6710c575e67fa73d5d4f80c37a', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '545b244aefd86e09fef7a731a5db65f0e792b523', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cc8c628a836d6f1eda336d674877c206632a93bb', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7281eff812d6c662f831795ae3fb7fa745680b16', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b8aa38eebc8aff9a9b665258cd4371b0df1ce9dd', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd31ab0a893c73a1a216f998a049506ac3585fa08', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0d26c9dd2ef7aac153367ace5991a5f0db905594', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b679cd16573ca0a5b4ff6a440a3cf5ce563723f1', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2437f47caedc4e984099e3fe23670051c479e126', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
400035201,Avoid registering tests datasets,Fix https://github.com/tensorflow/datasets/issues/1830,False,1832,https://api.github.com/repos/tensorflow/datasets/pulls/1832,https://github.com/tensorflow/datasets/pull/1832,closed,90,29,5,6,2,8,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-04-07 04:48:03+00:00,2020-04-07 18:34:53+00:00,49610.0,13:46:50,"[{'comment_id': 404535528, 'comment_body': 'As per pytype, do not import direct member.', 'comment_created': datetime.datetime(2020, 4, 7, 4, 58, 49, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 404882102, 'comment_body': 'Here we want to test the global import `import tensorflow_datasets as tfds`.\r\n', 'comment_created': datetime.datetime(2020, 4, 7, 15, 4, 15, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 404882433, 'comment_body': ""Note, it's good practice to wrap yield inside `try`/'finaly' block,\r\n\r\nThis make sure that `_skip_reg = True` is executed even when an exception is raised inside the context manager"", 'comment_created': datetime.datetime(2020, 4, 7, 15, 4, 43, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 404882856, 'comment_body': 'Please also add a unittest inside `registered_test.py` directly testing the feature.', 'comment_created': datetime.datetime(2020, 4, 7, 15, 5, 16, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 404885142, 'comment_body': 'It is good practice to wraps yield inside a try/finally block, to execute `_skip_reg = False` even in case an exception is raised in the context manager.', 'comment_created': datetime.datetime(2020, 4, 7, 15, 8, 17, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 404886053, 'comment_body': 'Could you also add a test in `registered_test.py` to test this feature directly (creates a new Builder class inside the context manager, without import) ?', 'comment_created': datetime.datetime(2020, 4, 7, 15, 9, 24, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 404887966, 'comment_body': 'Okay', 'comment_created': datetime.datetime(2020, 4, 7, 15, 11, 51, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 404888805, 'comment_body': 'Sure', 'comment_created': datetime.datetime(2020, 4, 7, 15, 12, 54, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '3abfd451354a8f0510955a9815f8d65deb9f296d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd42ed0ebf21bdf0f438b699666df821fa5054ad6', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6cd7c383c0b10f9a397961fa6b777f3e51214543', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '78f2e7d008b64228a2228bb1910f16d676ba8991', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '52501e3685757a5c9b42f48fc759d13c95fe094a', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2c929faa68cb465ce15ef514036fa741adfefdd0', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
400006236,Update document_datasets to indicate which datasets are tfds-nightly only,"Fix #1825 and Fix https://github.com/tensorflow/datasets/issues/584
",False,1831,https://api.github.com/repos/tensorflow/datasets/pulls/1831,https://github.com/tensorflow/datasets/pull/1831,closed,285,3,4,16,4,17,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-04-07 02:49:04+00:00,2020-04-27 22:48:28+00:00,1799964.0,"20 days, 19:59:24","[{'comment_id': 409660907, 'comment_body': ""This will bug if two configs have different versions.\r\nHow about using a nested dict/set instead ?\r\n```\r\n_to_full_name_dict([\r\n    'x/y/z',\r\n    'x/y/z2',\r\n    'x/y2/z',\r\n    'x2/y',\r\n]) == {\r\n    'x': {\r\n        'y': {'z', 'z2'},\r\n        'y2': {z},\r\n    },\r\n    'x2': {'y'}\r\n}\r\n```\r\nTo have `full_name_dict[ds_name][config_name][version]`\r\n"", 'comment_created': datetime.datetime(2020, 4, 16, 15, 46, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409662629, 'comment_body': ""Rather than regex which are complicated and error prone, why not use `.split('/')`"", 'comment_created': datetime.datetime(2020, 4, 16, 15, 48, 31, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409663006, 'comment_body': 'Could use `tfds.core.registered.is_full_name()`', 'comment_created': datetime.datetime(2020, 4, 16, 15, 49, 2, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409664092, 'comment_body': 'Rather than duplicating, you should create a `nightly_icon` constant and reuse it across the places.', 'comment_created': datetime.datetime(2020, 4, 16, 15, 50, 31, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409665262, 'comment_body': ""This should not be in the global scope as we don't want to execute this computation on import.\r\n\r\nThe `get_mako_template` is already cached thanks to `@py_utils.memoize()`"", 'comment_created': datetime.datetime(2020, 4, 16, 15, 52, 7, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409665483, 'comment_body': 'Restore', 'comment_created': datetime.datetime(2020, 4, 16, 15, 52, 26, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409677658, 'comment_body': ""Could you add a test to `document_datasets_test.py` to make sure `dataset_docs_str` works properly ?\r\nYou could patch `_get_nightly_datasets` using `mock` (so the tests don't read the actual data).\r\n\r\nWe should test:\r\n* If the dataset is entirely new (do not exists in `stable_versions.txt` -> Then the `Note: This dataset is available only in our nightly package `tfds-nightly`.` should appear on top of the page) and nightly icon on the dataset header `#1` (no need to add a nightly marker in each and every config).\r\n* If the dataset already exists in `stable_versions.txt`, then:\r\n   * Only the new `configs` and `versions` should be marked as nigthly.\r\n   * The note on top of the page should be something like: `This dataset has been updated in nightly. The new versions and config marked with <nightly-icons> are only available in the `tfds-nightly` package`\r\n\r\n```\r\nstable_full_names = {\r\n    'ds': {\r\n        'old_config_name': {'1.0.0'},\r\n        'config_name': {'1.0.0', 0.0.1},\r\n    },\r\n}\r\nregistered_full_names = {\r\n    'ds': {\r\n        'new_config_name': {'1.0.0'},  # The config should be marked as nightly\r\n        'config_name': {'1.0.0', 2.0.0},  # Only 2.0.0 should be nightly\r\n    },\r\n    'new_ds': {'1.0.0'},  # The dataset should be marked as nightly\r\n}\r\n```"", 'comment_created': datetime.datetime(2020, 4, 16, 16, 9, 57, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409685536, 'comment_body': '(minor) I think the icon should go at the end of the line, rather than the beginning. This would be more consistent with how other icons are displayed (like the experimental icon in the navigation menu https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/experimental)', 'comment_created': datetime.datetime(2020, 4, 16, 16, 21, 42, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409688199, 'comment_body': 'Could you make this a separate function and add a unittest ? For the unittest, you can reuse the `_to_full_name_dict` example from bellow.\r\n', 'comment_created': datetime.datetime(2020, 4, 16, 16, 25, 39, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409691129, 'comment_body': 'Rather than providing the dict, how about giving some `nigthly_util=NightlyUtil()` ? I think this will help to have a better formatting.\r\n\r\nCurrently, we are adding the [Nightly] everywhere, which feels redundant:\r\n\r\n```\r\n# Dataset [Nightly]\r\n## Config [Nightly]\r\n* Version [Nightly]\r\n```\r\n\r\nWhile I think it would be best to only add it to the top most level:\r\n* When the dataset is new:\r\n```\r\n# Dataset [Nightly]\r\n## Config\r\n* Version\r\n```\r\n* When the dataset already exists but config is new:\r\n```\r\n# Dataset\r\n## Config [Nightly]\r\n* Version\r\n```\r\n* When the config already existed but the version is new\r\n```\r\n# Dataset\r\n## Config\r\n* Version [Nightly]\r\n```\r\n\r\nThis `NightlyUtil()` could have:\r\n```\r\nnigthly_util.is_builder_nightly(builder)\r\nnigthly_util.is_config_nightly(builder)\r\nnigthly_util.is_version_nightly(builder, version)\r\n```\r\nEach returning a `bool` if the nightly should be added or not.\r\n\r\nThose functions could be unittested to.', 'comment_created': datetime.datetime(2020, 4, 16, 16, 30, 2, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409760767, 'comment_body': 'Okay, will change it.\r\nI used regex just because config can be optional.', 'comment_created': datetime.datetime(2020, 4, 16, 18, 23, 1, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 409764907, 'comment_body': 'I meant that currently, if `stable_versions.txt` is:\r\n```\r\nx/y1/v1\r\nx/y2/v0\r\n```\r\nAnd `registered` is:\r\n```\r\nx/y1/v1\r\nx/y2/v1  # < New nightly version\r\nx/y2/v0\r\n```\r\nDisplay should be:\r\n```\r\n# x\r\n## y1\r\n* v1\r\n## y2\r\n* v1 [Nightly]\r\n* v0\r\n```\r\nWhile currently it is:\r\n```\r\n# x\r\n## y1\r\n* v1 [Nightly]  # << Wrong\r\n## y2\r\n* v1 [Nightly]\r\n* v0\r\n```\r\nHave a look at my other messages for more informations\r\n\r\nA config should be in tf-nightly if it does not appear in `stable_versions.txt`', 'comment_created': datetime.datetime(2020, 4, 16, 18, 30, 14, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409786687, 'comment_body': 'If I am not wrong the current doc template does not have any field like\r\n```\r\n## Config_name\r\n* Versions\r\n   * v1\r\n   * v2\r\n```\r\nSo, should I add a field containing the list of versions in the config description?\r\n', 'comment_created': datetime.datetime(2020, 4, 16, 19, 8, 48, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 409791124, 'comment_body': 'This is already done automatically. If all configs have the same value, the sections are merged in the main section (to avoid redundancy). But if different `BuilderConfig` have different version, they will be displayed individually.', 'comment_created': datetime.datetime(2020, 4, 16, 19, 17, 1, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409918325, 'comment_body': 'You could use `bool(x)` instead.', 'comment_created': datetime.datetime(2020, 4, 17, 0, 8, 3, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409930171, 'comment_body': '`return builder in self._nightly_ds`', 'comment_created': datetime.datetime(2020, 4, 17, 0, 49, 8, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409931039, 'comment_body': '`builder.builder_config.name` will fail for datasets which do not have config. Instead, you could pass the `builder` directly.', 'comment_created': datetime.datetime(2020, 4, 17, 0, 52, 52, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': 'cfec967be4602dff636adb951b582d1db114f578', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9ff3cb7b30b091bc817602f3aa5cdd49c5a5347e', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a40a377fd9d008060f361c5cbed2d1ec31ede5ca', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0a27f04162c41abcb247c87fb2ec46004890767e', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4df5ae492ac2d631a43153bb4dcb159753ed7ffd', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f63995da2695a4b983b162df26b518232aad0ff6', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '18987ab7263168afcea6e60a4708382676217933', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '87bcee738d12970c0c9cc6acae4a982474ee173a', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f43c914c1ffddf7d226c26578f1ee96da29240ff', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '997cc97022163b7b32329ba5eb47c90604fa5db1', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '86756ec315669b1720234099ecebc545088ebfb9', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c0c1545e55b90f2fa40ba016a4e4629195d6ac47', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5be1879444009097e044be06a072a640af33b55e', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a48034bf9df8dcd0752bbf3bee29f3e4a6c6b294', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '75070c4e043c89cb12049718747dc26f386d5d2d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '335f19a8e4b740dfc714c061a461fdc73971f9bb', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
399977008,Script for updating all stable versions of tfds datasets,Fix a part of https://github.com/tensorflow/datasets/issues/1825,False,1826,https://api.github.com/repos/tensorflow/datasets/pulls/1826,https://github.com/tensorflow/datasets/pull/1826,closed,1584,0,2,3,3,3,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-04-07 00:46:24+00:00,2020-04-08 14:45:01+00:00,136717.0,"1 day, 13:58:37","[{'comment_id': 404474545, 'comment_body': 'Rather than dumping the list, could you print one result per line ?\r\nI think it would make the file more human readable.', 'comment_created': datetime.datetime(2020, 4, 7, 0, 54, 28, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 404474590, 'comment_body': ""Remove '\\'"", 'comment_created': datetime.datetime(2020, 4, 7, 0, 54, 38, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 404479186, 'comment_body': 'Removed `\\`', 'comment_created': datetime.datetime(2020, 4, 7, 1, 11, 26, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '824c2914342116168d64dbe3a2fb9bf75118deb9', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e97abd43f8b211c52ac5febc1379e259f4cb4f6b', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd668019dc8da95e8d48784993e743bb564830ce9', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
393275694,Add new Image Classification section,Fix #1709,False,1717,https://api.github.com/repos/tensorflow/datasets/pulls/1717,https://github.com/tensorflow/datasets/pull/1717,closed,189,170,161,7,2,0,2,0,"[{'name': 'cla: yes'}, {'name': 'author:please_resolve_conflict'}]",2020-03-24 21:55:25+00:00,2020-03-27 17:16:25+00:00,242460.0,"2 days, 19:21:00",[],"[{'commit_sha': 'bdc4f222cf18b051b6e1d5050b89ca8627634ddf', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dc88663f39a4d9786c89035cf6a5e110f787aa6b', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ca6b717f2c769c865c714731f9896f228ed23967', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '301b9e311ab90b99b30ab90bd6d3adb24f64e8b4', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9576cae8d695a761aa86a5faf3476a6d148376f7', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6098b9350f3918b4ce42d90f4a98d1f124a5b04e', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '57b6bcbb07078491595af487143b19bfe959710b', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
384523377,Test that new dataset has correctly set the checksums,Fix #1397,True,1586,https://api.github.com/repos/tensorflow/datasets/pulls/1586,https://github.com/tensorflow/datasets/pull/1586,closed,27,1,1,6,1,24,3,0,"[{'name': 'cla: yes'}, {'name': 'contributions welcome'}, {'name': 'author:kokoro_test_failed'}]",2020-03-05 22:03:19+00:00,2020-03-24 20:27:13+00:00,1635834.0,"18 days, 22:23:54","[{'comment_id': 388594125, 'comment_body': 'Please suggest a good method for finding the path to the checksum file', 'comment_created': datetime.datetime(2020, 3, 5, 22, 4, 9, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 388709281, 'comment_body': ""Why don't you have a look at the checksums.py file to see how this is done ?"", 'comment_created': datetime.datetime(2020, 3, 6, 4, 35, 37, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 389041854, 'comment_body': ""The checksum path should be automatically look-up, by checking all the registered checksum paths. This shouldn't be a property."", 'comment_created': datetime.datetime(2020, 3, 6, 17, 31, 55, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 389042095, 'comment_body': 'You should use `checksums._CHECKSUM_DIRS` instead.', 'comment_created': datetime.datetime(2020, 3, 6, 17, 32, 25, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 389044085, 'comment_body': 'This might not be a string but a `tfds.download.Resource(`. Could you tried to run on `image/sun_test.py` to check this is working properly ?', 'comment_created': datetime.datetime(2020, 3, 6, 17, 36, 24, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 389046201, 'comment_body': ""All class will share the same urls, which might create conflicts between multiple instances. This shouldn't be a global attribute.\r\nI would move it `setUpClass` instead.\r\nAlso UPPER_CASE mean constant in Python, I would rename to `_download_urls`.\r\n"", 'comment_created': datetime.datetime(2020, 3, 6, 17, 39, 55, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 389047082, 'comment_body': 'You should also check the file exists in the first place', 'comment_created': datetime.datetime(2020, 3, 6, 17, 41, 24, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 389047636, 'comment_body': 'The user is likely to be confused about this error. Could you raise a more explicit error message ?', 'comment_created': datetime.datetime(2020, 3, 6, 17, 42, 30, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 389048387, 'comment_body': 'Why `rb` ? and not just `r` ?', 'comment_created': datetime.datetime(2020, 3, 6, 17, 44, 7, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 392621256, 'comment_body': '@Conchylicultor I am unable to get the file path to the correct checksum file.\r\nThe above code does not work for test cases like `translate/wmt_test.py`\r\nand\r\n`self.builder.name`, `self.info.name` does not work for test cases like `sun_test.py`.\r\n', 'comment_created': datetime.datetime(2020, 3, 14, 21, 36, 39, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 392621282, 'comment_body': 'will remove it...', 'comment_created': datetime.datetime(2020, 3, 14, 21, 37, 7, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 392621675, 'comment_body': 'All URLs are not encountered. So `_download_urls` is only a subset of all URLs in the url_checksum file.\r\nPlease correct me, if I am wrong.', 'comment_created': datetime.datetime(2020, 3, 14, 21, 43, 9, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 393973717, 'comment_body': 'You could use `tf.nest.map_structure` instead to recursively add the urls.\r\n\r\nAnd you also need to modify `_get_dl_download_result `', 'comment_created': datetime.datetime(2020, 3, 17, 21, 12, 31, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 393974116, 'comment_body': 'Yes, this is correct (ex:for builder config)', 'comment_created': datetime.datetime(2020, 3, 17, 21, 13, 23, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 393977460, 'comment_body': ""Why don't you use `checksums._get_path(dataset_name)` ? It will already raise the error if it don't find the file"", 'comment_created': datetime.datetime(2020, 3, 17, 21, 20, 16, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 393979039, 'comment_body': 'You should reuse existing code whenever possible, here by loading the checksums with `checksums._get_sizes_checksums(path)`', 'comment_created': datetime.datetime(2020, 3, 17, 21, 23, 47, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 394695337, 'comment_body': 'I used `checksums._get_path(self.dataset_name)` but for test files like sun_test.py, I get the following error.\r\n```\r\nFile ""/home/vijay/GitHub-Repos/Tensorflow/datasets/tensorflow_datasets/testing/dataset_builder_testing.py"", line 289, in _test_checksums\r\nfilepath = os.path.join(download.checksums._get_path(self.dataset_name))\r\nAttributeError: \'Sun397TfdsTest\' object has no attribute \'dataset_name\'\r\n\r\n```', 'comment_created': datetime.datetime(2020, 3, 18, 23, 24, 43, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 394696720, 'comment_body': 'or if we use `checksums._get_path(self.builder.name)`. I got the following error\r\n```\r\nAssertionError: No checksums file could be found for dataset testable_sun397. Please create one in one of:\r\n```\r\nSo, is there any proper way to handle all these errors and get the path to proper checksum file.', 'comment_created': datetime.datetime(2020, 3, 18, 23, 28, 57, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 394756223, 'comment_body': 'What is the content of `checksums._CHECKSUM_DIRS` ?\r\nNote that when encountering a bug like this, you should try to look at the code which raise the error and try to debug from there.', 'comment_created': datetime.datetime(2020, 3, 19, 2, 27, 35, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 394756578, 'comment_body': ""Also it is expected that this fail if you haven't created a `testable_sun397.txt` file first. This is what this PR is all about, right ?\r\nWhy don't you try to test on an existing dataset test first, like mnist_test ?"", 'comment_created': datetime.datetime(2020, 3, 19, 2, 29, 5, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 396616963, 'comment_body': ""I'm sure there is a `self.assertSubSet`, or a better function than `assertTrue `."", 'comment_created': datetime.datetime(2020, 3, 23, 17, 13, 50, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 396617857, 'comment_body': '```\r\nfilepath = os.path.join(checksums._get_path(self.builder.name))\r\nif not tf.io.gfile.exists(filepath):\r\n  raise AssertionError(""url checksums file not found at %s"" % filepath)\r\n\r\nsizes_checksums = checksums._get_sizes_checksums(filepath)\r\nurls = sizes_checksums.keys()\r\n```', 'comment_created': datetime.datetime(2020, 3, 23, 17, 15, 9, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 396618561, 'comment_body': 'Note you should only check the file existence if the `dl_manager.download` has been called. If there is no check to do if `self._download_urls` is empty.', 'comment_created': datetime.datetime(2020, 3, 23, 17, 16, 8, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 396724149, 'comment_body': 'I could only find [assertSetEqual](https://docs.python.org/3/library/unittest.html#unittest.TestCase.assertSetEqual).\r\nA similar subset function [assertDictContainsSubset](https://docs.python.org/2/library/unittest.html#unittest.TestCase.assertDictContainsSubset) is deprecated', 'comment_created': datetime.datetime(2020, 3, 23, 20, 4, 17, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '8eef04780ea34dbf2d00a9e7d91cca449b01f8e0', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b3849327fb4ac7f160bc803a9e7aeb508f490861', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd747ef945d89aa55bf30b602607e3efd46e23775', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '48b132d948ec1560a9254bf886d8058a1b7723dd', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ed03b82fe98b3d2f249f56d4903a107904acfe66', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '038f5246819f449e6f4197b1ce3af379fa5e0c71', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
462412147,Append path hash to extracted files,"Fix https://github.com/tensorflow/datasets/issues/2267
",False,2269,https://api.github.com/repos/tensorflow/datasets/pulls/2269,https://github.com/tensorflow/datasets/pull/2269,open,7,3,2,1,0,0,3,0,"[{'name': 'bug'}, {'name': 'cla: yes'}, {'name': 'author:please_resolve_conflict'}]",2020-08-03 21:43:42+00:00,,0.0,,[],"[{'commit_sha': 'ac9e998e09700dae2a42138c7d98851263456b5e', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
448547212,tf.io.gfile fails for gcs paths ,"Fix https://github.com/tensorflow/datasets/issues/2190
CC: @Conchylicultor ",True,2193,https://api.github.com/repos/tensorflow/datasets/pulls/2193,https://github.com/tensorflow/datasets/pull/2193,closed,8,6,1,1,1,2,3,0,"[{'name': 'cla: yes'}, {'name': 'kokoro:run'}, {'name': 'tfds:ready_to_merge'}]",2020-07-13 23:02:47+00:00,2020-07-15 01:06:53+00:00,93846.0,"1 day, 2:04:06","[{'comment_id': 454659122, 'comment_body': ""I think you could replace this by `_is_gcs_disabled = os.name == 'nt'`"", 'comment_created': datetime.datetime(2020, 7, 14, 21, 34, 28, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 454742410, 'comment_body': 'That Global variable seemed unnecessary', 'comment_created': datetime.datetime(2020, 7, 15, 1, 50, 27, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '5f145d7342c6b2c9a53494574e8116574a87cdb6', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
436803399,Image Encoding,"Fix https://github.com/tensorflow/datasets/issues/2042

TODO:
* [ ] Fix Image and Image classification tests
* [ ] Add a method to verify encoding format",False,2099,https://api.github.com/repos/tensorflow/datasets/pulls/2099,https://github.com/tensorflow/datasets/pull/2099,closed,30,14,2,2,2,7,3,0,"[{'name': 'bug'}, {'name': 'cla: yes'}, {'name': 'author:please_resolve_conflict'}]",2020-06-18 23:03:26+00:00,2020-10-08 15:54:34+00:00,9651068.0,"111 days, 16:51:08","[{'comment_id': 443763692, 'comment_body': 'Remove ? Eventually replace by a TODO', 'comment_created': datetime.datetime(2020, 6, 22, 18, 58, 39, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443764097, 'comment_body': 'You should also add a test to make sure that providing a `np.array` with `encoding_format=None` raise an error.', 'comment_created': datetime.datetime(2020, 6, 22, 18, 59, 30, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443764710, 'comment_body': ""We want to test paths and file_obj with `encoding_format=None`, but np.array with `encoding_format='pnng'`"", 'comment_created': datetime.datetime(2020, 6, 22, 19, 0, 42, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443765039, 'comment_body': ""To avoid extra indentation: \r\n```\r\nacceptable_dtypes = ACCEPTABLE_DTYPES.get(self._encoding_format, None)\r\nif acceptable_dtypes and dtype not in acceptable_dtypes:\r\n      raise ValueError('Acceptable `dtype` for %s: %s (was %s)' % (\r\n          self._encoding_format, acceptable_dtypes, dtype))\r\n```"", 'comment_created': datetime.datetime(2020, 6, 22, 19, 1, 22, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443780715, 'comment_body': '- Should I remove the `_verify_encoding` function?\r\n- We have to check if the encoding format specified by the user matches the encoding format of the image, right? \r\n\r\n', 'comment_created': datetime.datetime(2020, 6, 22, 19, 33, 54, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 443790675, 'comment_body': '> Should I remove the _verify_encoding function?\r\n> We have to check if the encoding format specified by the user matches the encoding format of the image, right?\r\n\r\nIdeally yes (as long as we can do this efficiently without decoding the full image), but I think this should be done in another PR. For now a TODO is fine.\r\n\r\nIf encoding is defined, then yield np.array should be encoded in the requested format\r\nIf encoding is undefined, then yield np.array should raise error (as unknown encoding format). Image paths and file object could still be passed though.\r\n', 'comment_created': datetime.datetime(2020, 6, 22, 19, 55, 33, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 443794627, 'comment_body': 'Ok, done.', 'comment_created': datetime.datetime(2020, 6, 22, 20, 3, 45, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '89c2df5c7c38775fecdc8e6eaa3a76af817d7fff', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '31397a8135dfc932cf9778ea6d72a32778ad5fe6', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
428356023,Fix _list_directory function,"**Expected**
The function returns a list containing the names of the entries in the directory given by path.

**Bug**
The function returns a list of all the files in the directory tree given by path.
",True,2074,https://api.github.com/repos/tensorflow/datasets/pulls/2074,https://github.com/tensorflow/datasets/pull/2074,closed,7,5,2,1,0,0,3,0,"[{'name': 'bug'}, {'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-06-05 10:11:32+00:00,2020-06-06 01:47:46+00:00,56174.0,15:36:14,[],"[{'commit_sha': 'f3d7f073656f6ef00b0cdcef7e83c5a0009f5dcc', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
405745967,Fix tfds/core bugs on windows,"Related https://github.com/tensorflow/datasets/issues/1911#issuecomment-616224484

TODO: 
* [ ] Fix `download_manger_test.py`
* [x] Fix `dataset_info_test.py`
* [X] Fix `tfrecords_reader_test.py`
* [X] Fix `features_test.py`
* [X] Fix `extractor_test.py`
* [ ] Fix `gcs_utils_test.py`
* [X] Fix `py_utils_test.py`
",True,1916,https://api.github.com/repos/tensorflow/datasets/pulls/1916,https://github.com/tensorflow/datasets/pull/1916,closed,19,8,6,4,1,1,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:needs_final_approval'}]",2020-04-19 22:19:52+00:00,2020-04-21 18:08:43+00:00,157731.0,"1 day, 19:48:51","[{'comment_id': 411365289, 'comment_body': ""This was required to fix \r\n```\r\nFAILED tensorflow_datasets/core/download/extractor_test.py::ExtractorTest::test_wrong_method\r\n\r\nc:\\programdata\\miniconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\test_util.py:2839: AssertionError: Exception of type <class 'UnboundLocalError'>: local variable 'dst_path' referenced before assignment\r\n```"", 'comment_created': datetime.datetime(2020, 4, 20, 13, 12, 41, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '2b9b79d8ca8af5ecfb445c7300c4e02351f10f14', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bef289962408875e42e8030560dbe8899925e2c3', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '16890de3f618d15f7f1f99301b9e83c0755fae90', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f34e76796cc3ce1ef07afe88ec88bfe4906586d6', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
405736503,Renaming files on windows fails sometimes,"
This bug is not related to `tf.io.gfile.rename` as `os.rename` also fails to rename files.

This bug probably occurs because the `rename` function fails to acquire a `lock`.

Related to https://github.com/tensorflow/datasets/issues/1911#issuecomment-616219946

<details><summary><b> See pytest results </b></summary>
<p>

```
PS C:\Users\VIJAY\Desktop\GitHub_Repos\datasets> python .\tensorflow_datasets\image_classification\horses_or_humans_test.py
Running tests under Python 3.7.6: C:\ProgramData\Miniconda3\python.exe
[ RUN      ] HorsesOrHumansTest.test_baseclass
[       OK ] HorsesOrHumansTest.test_baseclass
[ RUN      ] HorsesOrHumansTest.test_download_and_prepare_as_dataset
Total configs: 0
I0420 02:50:07.489775  8064 dataset_builder.py:333] Generating dataset horses_or_humans (C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0)
I0420 02:50:07.489775  8064 dataset_builder.py:333] Generating dataset horses_or_humans (C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0)
Downloading and preparing dataset horses_or_humans/3.0.0 (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0...
I0420 02:50:07.567562  8064 dataset_builder.py:924] Generating split train
I0420 02:50:07.567562  8064 dataset_builder.py:924] Generating split train
Shuffling and writing examples to C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-train.tfrecord
  0%|                                                                                                                                           | 0/2 [00:00<?, ? examples/sI 
0420 02:50:07.810911  8064 tfrecords_writer.py:227] Done writing C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-train.tfrecord. Shard lengths: [2]
I0420 02:50:07.810911  8064 tfrecords_writer.py:227] Done writing C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-train.tfrecord. Shard lengths: [2]
I0420 02:50:07.812914  8064 dataset_builder.py:924] Generating split test
I0420 02:50:07.812914  8064 dataset_builder.py:924] Generating split test
Shuffling and writing examples to C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-test.tfrecord
  0%|                                                                                                                                           | 0/2 [00:00<?, ? examples/sI 
0420 02:50:07.974472  8064 tfrecords_writer.py:227] Done writing C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-test.tfrecord. Shard lengths: [2]
I0420 02:50:07.974472  8064 tfrecords_writer.py:227] Done writing C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-test.tfrecord. Shard lengths: [2]
I0420 02:50:07.992432  8064 dataset_builder.py:382] Computing statistics.
I0420 02:50:07.992432  8064 dataset_builder.py:382] Computing statistics.
Computing statistics...:   0%|                                                                                                                     | 0/2 [00:00<?, ? split/s]I0420 02:50:08.004396  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0.incompleteWO0GI5
I0420 02:50:08.004396  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0.incompleteWO0GI5
2020-04-20 02:50:08.007653: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
Computing statistics...:  50%|██████████████████████████████████████████████████████▌                                                      | 1/2 [00:00<00:00,  1.15 split/s]I0420 02:50:08.873071  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0.incompleteWO0GI5
I0420 02:50:08.873071  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0.incompleteWO0GI5
Computing statistics...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.80 split/s]
Dataset horses_or_humans downloaded and prepared to C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0. Subsequent calls will 
reuse this data.
I0420 02:50:09.473466  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:09.473466  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:09.599129  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:09.599129  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:09.798595  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:09.798595  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:09.914287  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:09.914287  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:10.113752  8064 dataset_info.py:361] Load dataset info from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:10.113752  8064 dataset_info.py:361] Load dataset info from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:10.119736  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:10.119736  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:10.203512  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:10.203512  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:10.379043  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:10.379043  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:10.464814  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
I0420 02:50:10.464814  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmp81u89vgv\horses_or_humans\3.0.0
WARNING:tensorflow:From C:\ProgramData\Miniconda3\lib\contextlib.py:82: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
W0420 02:50:10.659292  8064 deprecation.py:323] From C:\ProgramData\Miniconda3\lib\contextlib.py:82: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
W0420 02:50:10.659292  8064 deprecation.py:323] From C:\ProgramData\Miniconda3\lib\contextlib.py:82: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
Total configs: 0
I0420 02:50:10.676252  8064 dataset_builder.py:333] Generating dataset horses_or_humans (C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0)
I0420 02:50:10.676252  8064 dataset_builder.py:333] Generating dataset horses_or_humans (C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0)
Downloading and preparing dataset horses_or_humans/3.0.0 (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0...
I0420 02:50:10.690212  8064 dataset_builder.py:924] Generating split train
I0420 02:50:10.690212  8064 dataset_builder.py:924] Generating split train
Shuffling and writing examples to C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-train.tfrecord
  0%|                                                                                                                                           | 0/2 [00:00<?, ? examples/sI 
0420 02:50:10.811913  8064 tfrecords_writer.py:227] Done writing C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-train.tfrecord. Shard lengths: [2]
I0420 02:50:10.811913  8064 tfrecords_writer.py:227] Done writing C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-train.tfrecord. Shard lengths: [2]
I0420 02:50:10.813880  8064 dataset_builder.py:924] Generating split test
I0420 02:50:10.813880  8064 dataset_builder.py:924] Generating split test
Shuffling and writing examples to C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-test.tfrecord
  0%|                                                                                                                                           | 0/2 [00:00<?, ? examples/sI 
0420 02:50:10.976445  8064 tfrecords_writer.py:227] Done writing C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-test.tfrecord. Shard lengths: [2]
I0420 02:50:10.976445  8064 tfrecords_writer.py:227] Done writing C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0.incompleteWO0GI5\horses_or_humans-test.tfrecord. Shard lengths: [2]
I0420 02:50:10.978439  8064 dataset_builder.py:382] Computing statistics.
I0420 02:50:10.978439  8064 dataset_builder.py:382] Computing statistics.
Computing statistics...:   0%|                                                                                                                     | 0/2 [00:00<?, ? split/s]I0420 02:50:10.990415  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0.incompleteWO0GI5
I0420 02:50:10.990415  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split test, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0.incompleteWO0GI5
Computing statistics...:  50%|██████████████████████████████████████████████████████▌                                                      | 1/2 [00:00<00:00,  3.65 split/s]I0420 02:50:11.263676  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0.incompleteWO0GI5
I0420 02:50:11.263676  8064 dataset_builder.py:478] Constructing tf.data.Dataset for split train, from C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0.incompleteWO0GI5
Computing statistics...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  3.74 split/s]
[  FAILED  ] HorsesOrHumansTest.test_download_and_prepare_as_dataset
[ RUN      ] HorsesOrHumansTest.test_info
[       OK ] HorsesOrHumansTest.test_info
[ RUN      ] HorsesOrHumansTest.test_registered
[       OK ] HorsesOrHumansTest.test_registered
[ RUN      ] HorsesOrHumansTest.test_session
[  SKIPPED ] HorsesOrHumansTest.test_session
======================================================================
ERROR: test_download_and_prepare_as_dataset (__main__.HorsesOrHumansTest)
test_download_and_prepare_as_dataset (__main__.HorsesOrHumansTest)
Run the decorated test method.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Users\VIJAY\Desktop\GitHub_Repos\datasets\tensorflow_datasets\testing\test_utils.py"", line 205, in decorated
    f(self, *args, **kwargs)
  File ""C:\Users\VIJAY\Desktop\GitHub_Repos\datasets\tensorflow_datasets\testing\dataset_builder_testing.py"", line 298, in test_download_and_prepare_as_dataset
    self._download_and_prepare_as_dataset(self.builder)
  File ""C:\Users\VIJAY\Desktop\GitHub_Repos\datasets\tensorflow_datasets\testing\dataset_builder_testing.py"", line 359, in _download_and_prepare_as_dataset
    builder.download_and_prepare(download_config=download_config)
  File ""C:\Users\VIJAY\Desktop\GitHub_Repos\datasets\tensorflow_datasets\core\api_utils.py"", line 69, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""C:\Users\VIJAY\Desktop\GitHub_Repos\datasets\tensorflow_datasets\core\dataset_builder.py"", line 386, in download_and_prepare
    self.info.write_to_directory(self._data_dir)
  File ""C:\ProgramData\Miniconda3\lib\contextlib.py"", line 119, in __exit__
    next(self.gen)
  File ""C:\Users\VIJAY\Desktop\GitHub_Repos\datasets\tensorflow_datasets\core\utils\py_utils.py"", line 309, in incomplete_dir
    tf.io.gfile.rename(tmp_dir, dirname)
  File ""C:\ProgramData\Miniconda3\lib\site-packages\tensorflow_core\python\lib\io\file_io.py"", line 521, in rename_v2
    compat.as_bytes(src), compat.as_bytes(dst), overwrite)
tensorflow.python.framework.errors_impl.UnknownError: Failed to rename: C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0.incompleteWO0GI5 to: C:\Users\VIJAY\AppData\Local\Temp\horses_or_humans_testv9ls7ywi\tmpkogee3tp\horses_or_humans\3.0.0 : Access is denied.
; Input/output error

----------------------------------------------------------------------
Ran 5 tests in 4.557s

FAILED (errors=1, skipped=1)
PS C:\Users\VIJAY\Desktop\GitHub_Repos\datasets> 
```

</p>
</details>
",False,1915,https://api.github.com/repos/tensorflow/datasets/pulls/1915,https://github.com/tensorflow/datasets/pull/1915,closed,7,1,1,1,1,2,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:is_reviewing'}]",2020-04-19 21:21:45+00:00,2020-10-28 09:15:12+00:00,16545207.0,"191 days, 11:53:27","[{'comment_id': 417544390, 'comment_body': 'This fix feels too hacky and might creates an infinite loop.\r\nI think the `tf.io.gfile.rename` might fails if `dirname` already exists, which may be the sign of another bug.', 'comment_created': datetime.datetime(2020, 4, 29, 19, 3, 38, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 417610224, 'comment_body': 'Yes, this is indeed very hacky.\r\nAs, I mentioned previously, this might because the rename function fails to acquire a lock while renaming the directory.\r\nTo confirm the same, I added a `sleep` statement before the `rename`  function and it ran without any problem.\r\nSo, could you suggest a better solution to this problem?', 'comment_created': datetime.datetime(2020, 4, 29, 21, 3, 36, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '810937246abea15c41920c173bc1bd26eea86f1d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
405731751,Fix remaining tfds datasets bugs on windows,"Fixed `tfds\image`, `tfds\obj_dec`, `tfds\structured`, `tfds\text`, `tfds\translate`. 
Fix https://github.com/tensorflow/datasets/issues/1901

See comments of #1911 for old results.",True,1914,https://api.github.com/repos/tensorflow/datasets/pulls/1914,https://github.com/tensorflow/datasets/pull/1914,closed,40,17,8,1,5,7,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-04-19 20:48:20+00:00,2020-04-30 03:34:37+00:00,888377.0,"10 days, 6:46:17","[{'comment_id': 410999477, 'comment_body': 'please remove these', 'comment_created': datetime.datetime(2020, 4, 19, 21, 51, 21, tzinfo=datetime.timezone.utc), 'commenter': 'Eshan-Agarwal', 'type': 'User'}, {'comment_id': 411000204, 'comment_body': 'Actually, I wanted to show it, as I am not sure `tf.io.glob` error in TensorFlow for windows is resolved or not.\r\nI will remove it later on.\r\nThanks for the review.', 'comment_created': datetime.datetime(2020, 4, 19, 21, 55, 17, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 411000816, 'comment_body': 'I am not sure `tf.io.glob` error in TensorFlow for windows is resolved or not.\r\n\r\nI will remove this file in future commits ', 'comment_created': datetime.datetime(2020, 4, 19, 21, 58, 24, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 411044361, 'comment_body': 'Could you provide more context on this one ? `uint32 should be system independent`', 'comment_created': datetime.datetime(2020, 4, 20, 1, 50, 26, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 411070378, 'comment_body': 'I got the following stack trace \r\n\r\n\r\n<details><summary><b> See pytest results </b></summary>\r\n<p>\r\n\r\n```\r\nERROR: test_download_and_prepare_as_dataset (__main__.DukeUltrasoundTest)\r\ntest_download_and_prepare_as_dataset (__main__.DukeUltrasoundTest)\r\nRun the decorated test method.\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File ""C:\\Users\\VIJAY\\Desktop\\GitHub_Repos\\datasets\\tensorflow_datasets\\testing\\test_utils.py"", line 198, in decorated\r\n    f(self, *args, **kwargs)\r\n  File ""C:\\Users\\VIJAY\\Desktop\\GitHub_Repos\\datasets\\tensorflow_datasets\\testing\\dataset_builder_testing.py"", line 298, in test_download_and_prepare_as_dataset\r\n    self._download_and_prepare_as_dataset(self.builder)\r\n  File ""C:\\Users\\VIJAY\\Desktop\\GitHub_Repos\\datasets\\tensorflow_datasets\\testing\\dataset_builder_testing.py"", line 359, in _download_and_prepare_as_dataset\r\n    builder.download_and_prepare(download_config=download_config)\r\n  File ""C:\\Users\\VIJAY\\Desktop\\GitHub_Repos\\datasets\\tensorflow_datasets\\core\\api_utils.py"", line 69, in disallow_positional_args_dec\r\n    return fn(*args, **kwargs)\r\n  File ""C:\\Users\\VIJAY\\Desktop\\GitHub_Repos\\datasets\\tensorflow_datasets\\core\\dataset_builder.py"", line 363, in download_and_prepare\r\n    download_config=download_config)\r\n  File ""C:\\Users\\VIJAY\\Desktop\\GitHub_Repos\\datasets\\tensorflow_datasets\\core\\dataset_builder.py"", line 996, in _download_and_prepare\r\n    max_examples_per_split=download_config.max_examples_per_split,\r\n  File ""C:\\Users\\VIJAY\\Desktop\\GitHub_Repos\\datasets\\tensorflow_datasets\\core\\dataset_builder.py"", line 928, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File ""C:\\Users\\VIJAY\\Desktop\\GitHub_Repos\\datasets\\tensorflow_datasets\\core\\dataset_builder.py"", line 1012, in _prepare_split\r\n    example = self.info.features.encode_example(record)\r\n  File ""C:\\Users\\VIJAY\\Desktop\\GitHub_Repos\\datasets\\tensorflow_datasets\\core\\features\\features_dict.py"", line 170, in encode_example\r\n    in utils.zip_dict(self._feature_dict, example_dict)\r\n  File ""C:\\Users\\VIJAY\\Desktop\\GitHub_Repos\\datasets\\tensorflow_datasets\\core\\features\\features_dict.py"", line 169, in <dictcomp>\r\n    for k, (feature, example_value)\r\n  File ""C:\\Users\\VIJAY\\Desktop\\GitHub_Repos\\datasets\\tensorflow_datasets\\core\\features\\feature.py"", line 541, in encode_example\r\n    example_data = np.array(example_data, dtype=np_dtype)\r\nOverflowError: Python int too large to convert to C long\r\n\r\n----------------------------------------------------------------------\r\nRan 5 tests in 1.021s\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n\r\nI am using Windows 10 64-bit Operating System. python3.7.7\r\n\r\nThe  dataset prepared successfully when replaced `tf.uint32` with `tf.uint64`\r\n\r\n', 'comment_created': datetime.datetime(2020, 4, 20, 3, 33, 28, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 417549490, 'comment_body': 'This doesn\'t seems to be the right place to fix this.\r\n\r\nThe error is raised in feature.py"", line 541, in encode_example, so might be an issue with np_dtype, or similar. What is the int value ?', 'comment_created': datetime.datetime(2020, 4, 29, 19, 12, 40, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 417549696, 'comment_body': 'I think this has been solved in tf-nightly', 'comment_created': datetime.datetime(2020, 4, 29, 19, 13, 1, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': 'f51636af20d51bff2e46d1a79fa58456008d38c7', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
405729686,Fix tfds/image_classification bugs on windows,Related to https://github.com/tensorflow/datasets/issues/1911#issuecomment-616220757,True,1913,https://api.github.com/repos/tensorflow/datasets/pulls/1913,https://github.com/tensorflow/datasets/pull/1913,closed,11,5,10,2,1,3,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-04-19 20:34:06+00:00,2020-04-30 17:57:16+00:00,940990.0,"10 days, 21:23:10","[{'comment_id': 417710860, 'comment_body': ""I'm confused how can the test pass if `path_prefix` is created with `os.path.join` (so contains the '\\\\'), but `fname.replace` contains the '/' ?"", 'comment_created': datetime.datetime(2020, 4, 30, 1, 51, 23, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 417713114, 'comment_body': 'From what I understand,\r\n`path_prefix` is a string constant like `test_256`, `val_256` and from the code, it does not look like it is created by `os.path.join`.\r\n\r\n', 'comment_created': datetime.datetime(2020, 4, 30, 2, 0, 55, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 418197451, 'comment_body': 'Yes, indeed, I misread the code. Sorry.', 'comment_created': datetime.datetime(2020, 4, 30, 18, 11, 10, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': 'ac5f3f6d1fd00be8f432c265434131ce47861546', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b91e1b79e458f0b7b636ce1650783611908a2856', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
405727192,Don't mock some os functions on windows,"Fix #817

Related https://github.com/tensorflow/datasets/issues/1911#issuecomment-616217448",True,1912,https://api.github.com/repos/tensorflow/datasets/pulls/1912,https://github.com/tensorflow/datasets/pull/1912,closed,10,4,1,1,6,0,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-04-19 20:16:51+00:00,2020-04-30 03:18:05+00:00,889274.0,"10 days, 7:01:14",[],"[{'commit_sha': '6ddc06cf9d6d945ad6842a03f1866ab661f97e0d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
404751882,Fix regex error for windows,Fix #1888,True,1889,https://api.github.com/repos/tensorflow/datasets/pulls/1889,https://github.com/tensorflow/datasets/pull/1889,closed,1,1,1,1,1,1,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-04-16 21:29:07+00:00,2020-04-16 23:20:00+00:00,6653.0,1:50:53,"[{'comment_id': 409872428, 'comment_body': ""Brace means capturing group, which I don't think we want there. Will fix internally"", 'comment_created': datetime.datetime(2020, 4, 16, 21, 57, 49, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': 'f5fa721dc31c45a70d0b857c2dda15db090ea269', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
403959949,Fix kaggle downloader silent compression to '.zip' files,Fix https://github.com/tensorflow/datasets/issues/844 and Fix #1824 ,True,1877,https://api.github.com/repos/tensorflow/datasets/pulls/1877,https://github.com/tensorflow/datasets/pull/1877,closed,12,2,1,3,0,2,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-04-15 19:58:34+00:00,2020-04-17 19:47:34+00:00,172140.0,"1 day, 23:49:00","[{'comment_id': 409879135, 'comment_body': 'zipfile.is_zipfile', 'comment_created': datetime.datetime(2020, 4, 16, 22, 13, 21, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 409882759, 'comment_body': 'Also we should add a `# TODO(tfds): use --unzip once supported by kaggle (https://github.com/Kaggle/kaggle-api/issues/9)`', 'comment_created': datetime.datetime(2020, 4, 16, 22, 22, 25, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': 'fb7797622eff24d91a9bbbafcff9ab4ce076fddc', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'db887bdd5236d8d34f0dd3625c7f1026e511b640', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8c5fbd1454c6c821d40cd5fc25ff60cb006a1d00', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
402044319,`list_full_names` should not yield datasets with `None` version value,Fix #1853,True,1857,https://api.github.com/repos/tensorflow/datasets/pulls/1857,https://github.com/tensorflow/datasets/pull/1857,closed,21,20,2,1,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-04-10 19:50:42+00:00,2020-04-11 00:19:35+00:00,16133.0,4:28:53,[],"[{'commit_sha': '544e3b197e42d32699adc3c6acedd05905c4a5d2', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
395077661,`iter_archive()` should not return directory path name,"Problem: The function `iter_archive()` returned path to all directories in a `zip` file.

",True,1750,https://api.github.com/repos/tensorflow/datasets/pulls/1750,https://github.com/tensorflow/datasets/pull/1750,closed,26,3,6,3,7,2,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-03-28 10:40:16+00:00,2020-04-03 03:48:04+00:00,493668.0,"5 days, 17:07:48","[{'comment_id': 399691349, 'comment_body': 'We still want to support Python 2 internally, could you revert those changes ?', 'comment_created': datetime.datetime(2020, 3, 28, 18, 3, 21, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 399694596, 'comment_body': 'Done', 'comment_created': datetime.datetime(2020, 3, 28, 18, 35, 57, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': '5787c5a0bfb85f2f487ba427f8894f383775a8a6', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '43416e038993b29652a5e088e8d3bf384983f2f8', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f3d0b313b503d1ceac713a848ea135e71ffca743', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
394360643,Download specified version through `download_and_prepare` script,"Fix #1707
When we use the following command
```
python -m tensorflow_datasets.scripts.download_and_prepare --datasets=scientific_papers:1.1.0
```
the `download_and_prepare` script should make the following `builder`
```
tfds.builder(""scientific_papers:1.1.0"",
              data_dir=FLAGS.data_dir,
              config=config,
              **version_kwarg))
```
but previously it made
```
tfds.builder(""scientific_papers"",
              data_dir=FLAGS.data_dir,
              config=config,
              **version_kwarg))
```
So `scientific_papers:1.1.1` was downloaded instead of `scientific_papers:1.1.0`.",False,1734,https://api.github.com/repos/tensorflow/datasets/pulls/1734,https://github.com/tensorflow/datasets/pull/1734,closed,7,3,1,1,1,0,3,0,"[{'name': 'bug'}, {'name': 'cla: yes'}, {'name': 'community:please_review'}]",2020-03-26 18:58:16+00:00,2020-11-12 16:38:53+00:00,19950037.0,"230 days, 21:40:37",[],"[{'commit_sha': '219b691c250a52cbaad506f4c5dba1b54e319c3d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
393245109,Download dataset with given version,Fix https://github.com/tensorflow/datasets/issues/1707,True,1712,https://api.github.com/repos/tensorflow/datasets/pulls/1712,https://github.com/tensorflow/datasets/pull/1712,closed,9,7,2,2,10,6,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:is_reviewing'}]",2020-03-24 20:41:39+00:00,2020-03-26 18:16:40+00:00,164101.0,"1 day, 21:35:01","[{'comment_id': 397469120, 'comment_body': ""We should not duplicate logic. This code is already present in `tfds.builder` as `tfds.builder('ds:version')` works.\r\nCould you find another way ?"", 'comment_created': datetime.datetime(2020, 3, 24, 21, 22, 22, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 397490308, 'comment_body': 'For instance, you could check that passing `tfds.builder(name, data_dir=FLAGS.data_dir, **version_kwargs)` works.\r\nwhen `version_kwargs = {}` when `FLAGS.version is None`.', 'comment_created': datetime.datetime(2020, 3, 24, 22, 5, 46, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 398242888, 'comment_body': 'Please use the human readable names, rather than symbols `C0204` (e.g. `import-not-on-top`, `variable-redefined`)', 'comment_created': datetime.datetime(2020, 3, 26, 0, 2, 53, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 398243645, 'comment_body': ""I may be wrong but I don't think you should not modify `register.py`. Unless generating `tfds.load('scientific_paper:1.1.0')` generate `1.1.1`."", 'comment_created': datetime.datetime(2020, 3, 26, 0, 5, 33, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 398361856, 'comment_body': ""`tfds.builder('scientific_paper:1.1.0', version = None)` generates `1.1.1`\r\nSo, I updated the code to exclude `None` values."", 'comment_created': datetime.datetime(2020, 3, 26, 7, 25, 57, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 398630891, 'comment_body': 'Just read the documentation.\r\n```\r\n**builder_init_kwargs: `dict` of keyword arguments passed to the\r\n   `DatasetBuilder`. These will override keyword arguments passed in `name`,\r\n    if any.\r\n```\r\nSo, I fixed this error in the script. ', 'comment_created': datetime.datetime(2020, 3, 26, 14, 47, 24, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': 'e241c4b273480cea0a07fc09dbdc5ce7e5c63015', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5d17fb817bd432a9be4a27c4be38a4321a9dc57d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
407033572,Update tf_flower dataset to iterate over archive,"Fix https://github.com/tensorflow/datasets/issues/658
dataset_info.json: [Link](https://gist.github.com/vijayphoenix/ff68a7662290a84e0d1bf27d7d62225e)

@Conchylicultor Please review.

Thank you.
",True,1929,https://api.github.com/repos/tensorflow/datasets/pulls/1929,https://github.com/tensorflow/datasets/pull/1929,closed,10,19,9,1,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'kokoro:run'}]",2020-04-22 04:12:45+00:00,2020-04-29 19:50:27+00:00,661062.0,"7 days, 15:37:42",[],"[{'commit_sha': '9cf938143a8cbfc491a46902bd21bc73001cff2b', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
393093317,Read CelebA pictures from archive directly.,"Fix https://github.com/tensorflow/datasets/issues/118
Link: [dataset_info.json](https://gist.github.com/vijayphoenix/edeb8df8829b08b3f42d8279daad24af)",True,1706,https://api.github.com/repos/tensorflow/datasets/pulls/1706,https://github.com/tensorflow/datasets/pull/1706,closed,15,11,9,2,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-03-24 16:12:54+00:00,2020-04-03 17:13:21+00:00,867627.0,"10 days, 1:00:27",[],"[{'commit_sha': '4e5d06a3624602757acefe214f6d9a31be18dbf5', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dc497a4461ca04d88b55cb86286c62d8da905af4', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
384238823,Add visual question-answer feature in clevr dataset,"Fix #827

`data_info.json`: [link](https://gist.github.com/vijayphoenix/ededc2f2ea060b5bc7d918944d2b216f)
This gist only contains info for validation set because of some computational limits.",True,1583,https://api.github.com/repos/tensorflow/datasets/pulls/1583,https://github.com/tensorflow/datasets/pull/1583,closed,122,4,4,3,4,1,2,0,"[{'name': 'cla: yes'}, {'name': 'community:is_reviewing'}]",2020-03-05 12:08:31+00:00,2020-04-07 01:41:20+00:00,2813569.0,"32 days, 13:32:49","[{'comment_id': 389904798, 'comment_body': 'Just use `""""` as other keys to look good. Thanks ! ', 'comment_created': datetime.datetime(2020, 3, 9, 19, 13, 58, tzinfo=datetime.timezone.utc), 'commenter': 'ChanchalKumarMaji', 'type': 'User'}]","[{'commit_sha': 'a6a58fca4fce70d9797046116785c3781d935d79', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '71dd89f34125c8ee6362d1379f70dc12b076b48c', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9c8058d5492a944fda54d85e56ccda32bca59be8', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
368228869,Add affNIST dataset to tfds.,"# Add Dataset

* Dataset Name: affNIST
* Issue Reference: #1424 
* `dataset_info.json`: [Gist](https://gist.github.com/vijayphoenix/2fe04278e16349f291b4fcda87d0b6ad)

## Description
Added `affNIST` dataset.
The affNIST dataset is based on the well-known MNIST dataset. It is made by taking images from MNIST and applying various reasonable affine transformations to them.
  
## Checklist
* [x] Address all TODO's
* [x] Add alphabetized import to subdirectory's `__init__.py`
* [x] Run `download_and_prepare` successfully
* [x] Add checksums file
* [x] Properly cite in `BibTeX` format
* [x] Add passing test(s)
* [x] Add test data
* [x] Add data generation script (if applicable)
* [x] Lint code
",False,1427,https://api.github.com/repos/tensorflow/datasets/pulls/1427,https://github.com/tensorflow/datasets/pull/1427,open,137,0,9,11,7,7,3,0,"[{'name': 'cla: yes'}, {'name': 'dataset request'}, {'name': 'tfds:needs_final_approval'}]",2020-01-28 21:05:21+00:00,,0.0,,"[{'comment_id': 376707020, 'comment_body': '`S3 : False` version is not required in the new datasets. The new dataset version should be `0.0.1`. ', 'comment_created': datetime.datetime(2020, 2, 8, 12, 21, 57, tzinfo=datetime.timezone.utc), 'commenter': 'ChanchalKumarMaji', 'type': 'User'}, {'comment_id': 376707072, 'comment_body': 'indentations not correct. Here `4` spaces required. Check some other datasets. ', 'comment_created': datetime.datetime(2020, 2, 8, 12, 23, 3, tzinfo=datetime.timezone.utc), 'commenter': 'ChanchalKumarMaji', 'type': 'User'}, {'comment_id': 376707238, 'comment_body': '`4` spaces required. ', 'comment_created': datetime.datetime(2020, 2, 8, 12, 26, 7, tzinfo=datetime.timezone.utc), 'commenter': 'ChanchalKumarMaji', 'type': 'User'}, {'comment_id': 376707289, 'comment_body': '`4` spaces required. ', 'comment_created': datetime.datetime(2020, 2, 8, 12, 27, 31, tzinfo=datetime.timezone.utc), 'commenter': 'ChanchalKumarMaji', 'type': 'User'}, {'comment_id': 383841902, 'comment_body': 'remove commented out code.', 'comment_created': datetime.datetime(2020, 2, 25, 12, 13, 58, tzinfo=datetime.timezone.utc), 'commenter': 'cyfra', 'type': 'User'}, {'comment_id': 383842441, 'comment_body': 'We have to read file contents using tf.io.gfile (to allow reading from non-local storage systems):\r\n\r\n    with tf.io.gfile.GFile(filepath, ""rb"") as f:\r\n      data = tfds.core.lazy_imports.scipy.io.loadmat(f)', 'comment_created': datetime.datetime(2020, 2, 25, 12, 15, 20, tzinfo=datetime.timezone.utc), 'commenter': 'cyfra', 'type': 'User'}, {'comment_id': 383843495, 'comment_body': ""I remember hearing that tf.io.gfile.listdir doesn't guarantee ordering.\r\n\r\nAs you use the enumerate in 109 -- consider sorting the list of filenames before loading them (therefore making sure that ordering stays the same)"", 'comment_created': datetime.datetime(2020, 2, 25, 12, 17, 43, tzinfo=datetime.timezone.utc), 'commenter': 'cyfra', 'type': 'User'}]","[{'commit_sha': '54685e32b98ee9123da25e7a4081c55606eb5173', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8f9b85f892d4aec6f6ba0412312e0e6292d2a117', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f576e986266d8a79013d79cbf5d34547c0eb4ec8', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '653ea79cf1cce5ee86f3d74c46f42a8f41be3ebf', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9b68469c5bfe0122988a2a874d6d7df059edc1f2', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2d0a9fc8c7bc056577bd40aee4587cba7aed68a2', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '073c436adb972a7e01c9e7fbc5e863f6a93207b2', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '661c46767149a68aa31003896dde0b8264122dfb', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9b996f1f7f077ae957402fcc440b72ac02d6a9ea', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '46eeb5e1d0135f2aa8c1d4dadaa65c94bb2a5f62', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'addfa97dc3f1ca2d046593450672c798e3d1f748', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
455211058,Update uc_merced Docstring,Fix https://github.com/tensorflow/datasets/issues/2212,True,2223,https://api.github.com/repos/tensorflow/datasets/pulls/2223,https://github.com/tensorflow/datasets/pull/2223,closed,1,1,1,1,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'community:please_review'}]",2020-07-22 16:01:03+00:00,2020-07-23 01:32:03+00:00,34260.0,9:31:00,[],"[{'commit_sha': '7e520b99d543bf006910dc1a31f8a6902b459f7c', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
442348358,Update the source code link on dataset Catalog,"Update the source code link
For example:
```
tfds.image_classification.imagenet.Imagenet2012 -> tfds.image_classification.Imagenet2012
```",True,2140,https://api.github.com/repos/tensorflow/datasets/pulls/2140,https://github.com/tensorflow/datasets/pull/2140,closed,6,1,1,1,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'community:please_review'}]",2020-06-30 23:19:28+00:00,2020-07-01 00:58:38+00:00,5950.0,1:39:10,[],"[{'commit_sha': '6e5561c922dfefbf91d6693c8186130cf04738a5', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
438247410,Improve Kaggle API documentation and error msgs,"- Improve Function docstring 
- Better error msg",False,2112,https://api.github.com/repos/tensorflow/datasets/pulls/2112,https://github.com/tensorflow/datasets/pull/2112,closed,23,3,2,2,3,0,3,0,"[{'name': 'cla: yes'}, {'name': 'documentation'}, {'name': 'tfds:ready_to_merge'}]",2020-06-23 00:40:25+00:00,2020-06-24 00:07:46+00:00,84441.0,23:27:21,[],"[{'commit_sha': '799cd8d8036ba437ecf576916b21ec5ac3fcccaf', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '614cf918b777b5ccc96c3be0c874922f70ac2020', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
392585000,Improve define the dataset outside TFDS doc,Fix https://github.com/tensorflow/datasets/issues/1223,False,1702,https://api.github.com/repos/tensorflow/datasets/pulls/1702,https://github.com/tensorflow/datasets/pull/1702,closed,20,1,1,1,1,0,3,0,"[{'name': 'cla: yes'}, {'name': 'documentation'}, {'name': 'tfds:ready_to_merge'}]",2020-03-23 19:16:35+00:00,2020-10-14 10:25:58+00:00,17680163.0,"204 days, 15:09:23",[],"[{'commit_sha': 'a681781d9667a165c07a080703a5a60877fedef3', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
385762215,More beginner friendly documentation,"Fix #1544 and Fix #2160
and addresses other common doubts like https://github.com/tensorflow/datasets/issues/1521, https://github.com/tensorflow/datasets/issues/1560, etc.

Please review @ChanchalKumarMaji, @Conchylicultor.
Thank you",True,1611,https://api.github.com/repos/tensorflow/datasets/pulls/1611,https://github.com/tensorflow/datasets/pull/1611,closed,30,5,1,3,2,0,3,0,"[{'name': 'cla: yes'}, {'name': 'documentation'}, {'name': 'community:please_review'}]",2020-03-09 18:55:43+00:00,2020-07-31 01:39:28+00:00,12379425.0,"143 days, 6:43:45",[],"[{'commit_sha': 'a29343d96425bc9eb3dc1fc51cf722929c339b9d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7fb7192a54b3259d447350c73f0731412b661240', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd9dcb7facf3b92bafe9d1d107ca92277b81bdf88', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
463021897,Update Caltech Birds dataset download urls,"Fix #2272 
The old download URLs redirect to drive links

View [dataset_info](https://gist.github.com/vijayphoenix/5cfca1eae1056b4f500627b9dd822902) gist",True,2274,https://api.github.com/repos/tensorflow/datasets/pulls/2274,https://github.com/tensorflow/datasets/pull/2274,closed,12,12,3,1,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'community:please_review'}]",2020-08-04 21:37:44+00:00,2020-08-12 18:52:33+00:00,681289.0,"7 days, 21:14:49",[],"[{'commit_sha': '762b195c0274037286ea9084c317eb94bf5c047f', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
455970620,Update plant_village download url,,True,2229,https://api.github.com/repos/tensorflow/datasets/pulls/2229,https://github.com/tensorflow/datasets/pull/2229,closed,3,3,2,1,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-07-23 21:52:22+00:00,2020-07-24 18:04:50+00:00,72748.0,20:12:28,[],"[{'commit_sha': '57afca48ffc3a8d7086a289e4ef1bfb27301120b', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
447805353,Update Citrus leaves Dataset,"Fix #2155 
`dataset_info.json` Gist: https://gist.github.com/vijayphoenix/a77aba203847074cb8dc97dc3620acad
",True,2183,https://api.github.com/repos/tensorflow/datasets/pulls/2183,https://github.com/tensorflow/datasets/pull/2183,closed,3,3,2,1,0,0,3,0,"[{'name': 'cla: yes'}, {'name': 'dataset request'}, {'name': 'community:please_review'}]",2020-07-11 19:07:58+00:00,2020-07-23 16:51:57+00:00,1028639.0,"11 days, 21:43:59",[],"[{'commit_sha': '10e3e703c1fcb38d67bf12dba4fdb0bd4e44a3c1', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
438946206,Minor changes to GitHub workflow,"- We can assign an issue without sending an invite.
- Ignore folders created by pytype CLI tool and vscode editor ",True,2117,https://api.github.com/repos/tensorflow/datasets/pulls/2117,https://github.com/tensorflow/datasets/pull/2117,closed,7,5,2,1,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'author:kokoro_test_failed'}]",2020-06-24 04:00:55+00:00,2020-06-25 01:41:24+00:00,78029.0,21:40:29,[],"[{'commit_sha': '6c15d83a56c6f2ae691d3dc6eb47e3dc0bbe7b70', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
415086375,Update xnli and mutli_nli checksums,"Fix #2004
dataset_info: https://gist.github.com/vijayphoenix/c65f04cf68ee996c41536fb94da51b4e

FYI: The datasets are already present on tfds gcs.",True,2006,https://api.github.com/repos/tensorflow/datasets/pulls/2006,https://github.com/tensorflow/datasets/pull/2006,closed,4,4,4,1,1,0,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-05-08 07:14:42+00:00,2020-05-09 00:50:43+00:00,63361.0,17:36:01,[],"[{'commit_sha': '887b9274d7d9a94a2b5ec06bbec8d70a150bd5f7', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
412126754,Update checksums of oxford_flowers102 and dtd datasets,"Fix #1977 

oxford_flowers102 : [dataset_info](https://gist.github.com/vijayphoenix/e8da593cce3498739439dbbf01c785e3)
dtd: [dataset_info](https://gist.github.com/vijayphoenix/3d5719db2ef8fb2e544ea088a7c4d32f)",True,1980,https://api.github.com/repos/tensorflow/datasets/pulls/1980,https://github.com/tensorflow/datasets/pull/1980,closed,2,2,2,2,6,0,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-05-01 13:18:09+00:00,2020-05-01 18:12:38+00:00,17669.0,4:54:29,[],"[{'commit_sha': 'b3aa8cc15cfe86bff48ce93a867e56c3854ebee5', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2b8f8f5fd7b0f38397edaa62fda30f38a36c329d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
408969779,Improve GitHub workflow,"* Update `pull_request_template.md`
* Add new `documentation` issue template
* Add `.vscode` folder to `.gitignore`. [Just for my convenience :sweat_smile: ]",True,1954,https://api.github.com/repos/tensorflow/datasets/pulls/1954,https://github.com/tensorflow/datasets/pull/1954,closed,38,2,3,4,0,4,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:is_reviewing'}]",2020-04-25 21:02:46+00:00,2020-04-30 00:38:11+00:00,358525.0,"4 days, 3:35:25","[{'comment_id': 417522108, 'comment_body': 'What are you expecting this to be used for ?\r\nI think documentations issues could go into other category (bug report or feature request).\r\nShould we improve the other templates documentation instead ?', 'comment_created': datetime.datetime(2020, 4, 29, 18, 25, 31, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 417524594, 'comment_body': 'Rather than naming some symbols which may be obscure for new contributors, I would instead only add a single item with a link to the doc.\r\n```\r\n* [ ] If using additional dependencies (e.g. `nltk`), use [lazy_imports](https://www.tensorflow.org/datasets/add_dataset#extra_dependencies)\r\n```', 'comment_created': datetime.datetime(2020, 4, 29, 18, 29, 51, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 417616130, 'comment_body': 'It could be used for the following documentation problems:\r\n* Vague; not clear; Incomplete; Misleading; Outdated API documentation\r\n* Any spelling mistake\r\n* Any update in dataset description, `home_url`, `citation`, etc.\r\n\r\nIn my opinion, the above problems could be labeled as `documentation` issue. This will also help us to filter the real bugs.\r\nOr as you suggested we could add a new `other` issue template.', 'comment_created': datetime.datetime(2020, 4, 29, 21, 14, 46, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 417691586, 'comment_body': ""Ok sounds good. However I'll reduce the text as the current feels a little too verbose."", 'comment_created': datetime.datetime(2020, 4, 30, 0, 37, 2, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': '43bac63fd14ff964ed1941778325476ee5194504', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0eebc60b550b45ddecbf9b56f7c4428fa9671ce3', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '37af18a848dbcb7f22301dd324ed6867359edd8c', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c7a6e3783290f83d0a0d3a0c03d4b29fcd208bf0', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
401318805,Minor typos,"* Fix #1849 and Fix https://github.com/tensorflow/datasets/issues/1928
and some other problems",True,1850,https://api.github.com/repos/tensorflow/datasets/pulls/1850,https://github.com/tensorflow/datasets/pull/1850,closed,14,14,8,7,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:please_review'}]",2020-04-09 09:39:45+00:00,2020-07-23 17:41:26+00:00,9100901.0,"105 days, 8:01:41",[],"[{'commit_sha': 'ac438594f18810afaa42e40da1a7677682fb9138', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5c5d5e1e8edb61fee63a0fc4675557220304c25e', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5e2b64dcb46e4af5f49659085eb161079253bc15', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '448148e3e11e0a388f51ca322e94e8518c209097', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5de1830121744f4693b538bd1a7ee416f10b6690', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ce669a777be549c04fb1918ab697be00e5f66ba3', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5bc8928ab7c80a17406a550d917ea7611d53f1b4', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
397919466,Remove unneccesary files in caltech_birds2011 dataset,Fix a part of #1792,True,1793,https://api.github.com/repos/tensorflow/datasets/pulls/1793,https://github.com/tensorflow/datasets/pull/1793,closed,0,4649493,20,2,0,0,1,0,[{'name': 'cla: yes'}],2020-04-03 03:45:47+00:00,2020-04-03 18:35:39+00:00,53392.0,14:49:52,[],"[{'commit_sha': '54365b4f3a48c6f15c0d74c550027664bd3e398b', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'eda324ef7d1e9584df81a4776283a21cfe3b605b', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
396437064,Fix pylint errors in Image Classification datasets,Fix some of https://github.com/tensorflow/datasets/issues/1644,False,1774,https://api.github.com/repos/tensorflow/datasets/pulls/1774,https://github.com/tensorflow/datasets/pull/1774,open,321,77,67,2,0,0,3,0,"[{'name': 'cla: yes'}, {'name': 'tfds:please_review'}, {'name': 'py3-migration'}]",2020-03-31 16:07:17+00:00,,0.0,,[],"[{'commit_sha': '15f9f633b92a49e39ade457bc07f0719500c3ee5', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '685093f41fb6c52668953753e976e5d0fdb056ed', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
396435265,Fix some minor typos,Fix some minor typos,True,1773,https://api.github.com/repos/tensorflow/datasets/pulls/1773,https://github.com/tensorflow/datasets/pull/1773,closed,6,6,4,1,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-03-31 16:04:12+00:00,2020-04-02 00:25:12+00:00,116460.0,"1 day, 8:21:00",[],"[{'commit_sha': 'f0ac1302c7cdb6016d64b18c4c338173fd7cd09d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
394473715,Update Wikipedia checksums and dataset,Fix https://github.com/tensorflow/datasets/issues/1125 and some part of https://github.com/tensorflow/datasets/issues/1716,True,1737,https://api.github.com/repos/tensorflow/datasets/pulls/1737,https://github.com/tensorflow/datasets/pull/1737,closed,821,757,5,3,1,0,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-03-26 23:30:21+00:00,2020-04-02 22:36:33+00:00,601572.0,"6 days, 23:06:12",[],"[{'commit_sha': '82a36b963651e898c1eba5ed06637539c892b4ff', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '83dd997721297702a54b5cce1122cb0b418c37bd', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'eb3f832648a2d4aa9144af3b0a240608726ea3a1', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
394322889,Update README,"* Bumped up to TF 1.15+
* Fixed MNIST URL in the citation
* Add API ref badge
* Fixed some minor typos, and links",True,1733,https://api.github.com/repos/tensorflow/datasets/pulls/1733,https://github.com/tensorflow/datasets/pull/1733,closed,19,19,2,1,0,0,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:needs_final_approval'}]",2020-03-26 17:47:33+00:00,2020-04-30 02:55:35+00:00,2970482.0,"34 days, 9:08:02",[],"[{'commit_sha': '7a1a646c0c905b1dd756b6c7f55d04f61f636a7e', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
394138372,Fix pylint for tfds script and proto,Fix https://github.com/tensorflow/datasets/issues/1644,False,1731,https://api.github.com/repos/tensorflow/datasets/pulls/1731,https://github.com/tensorflow/datasets/pull/1731,open,13,12,6,2,0,5,3,0,"[{'name': 'cla: yes'}, {'name': 'tfds:please_review'}, {'name': 'py3-migration'}]",2020-03-26 12:25:03+00:00,,0.0,,"[{'comment_id': 399640664, 'comment_body': "" It is in camel case. Please change this don't disable it."", 'comment_created': datetime.datetime(2020, 3, 28, 9, 21, 40, tzinfo=datetime.timezone.utc), 'commenter': 'Eshan-Agarwal', 'type': 'User'}, {'comment_id': 399640681, 'comment_body': 'Same as above', 'comment_created': datetime.datetime(2020, 3, 28, 9, 21, 51, tzinfo=datetime.timezone.utc), 'commenter': 'Eshan-Agarwal', 'type': 'User'}, {'comment_id': 399640691, 'comment_body': 'same', 'comment_created': datetime.datetime(2020, 3, 28, 9, 21, 58, tzinfo=datetime.timezone.utc), 'commenter': 'Eshan-Agarwal', 'type': 'User'}, {'comment_id': 399648202, 'comment_body': 'Thanks for the review but I feel that we should not change the naming format here because it is a common practice to name the class setup methods in such way', 'comment_created': datetime.datetime(2020, 3, 28, 10, 50, 6, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}, {'comment_id': 399648485, 'comment_body': 'We can change this.\r\nThank you for pointing this out.', 'comment_created': datetime.datetime(2020, 3, 28, 10, 53, 29, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': 'a4711171a1d6332a248f86cc36ce0619e000e80a', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b93afb6e3f8fc4e3f50f5c7bb8913720d1dc6e83', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
389789979,Fix pylint errors for tensorflow_dataset/testing,"* Fix https://github.com/tensorflow/datasets/issues/1644
* Some changes were made in tensorflow_dataset/core and other locations to prevent the code from breaking down.

CC: @ChanchalKumarMaji please review. 
Thanks!",False,1658,https://api.github.com/repos/tensorflow/datasets/pulls/1658,https://github.com/tensorflow/datasets/pull/1658,open,47,43,13,1,0,0,3,0,"[{'name': 'cla: yes'}, {'name': 'community:please_review'}, {'name': 'py3-migration'}]",2020-03-17 11:52:27+00:00,,0.0,,[],"[{'commit_sha': '7813ecb1525f8f3e29d46cabc75743599a6b8bf2', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
385938472,Cleanup legacy code,Fix https://github.com/tensorflow/datasets/issues/1519,True,1619,https://api.github.com/repos/tensorflow/datasets/pulls/1619,https://github.com/tensorflow/datasets/pull/1619,closed,4,12,7,1,3,0,2,0,"[{'name': 'cla: yes'}, {'name': 'author:kokoro_test_failed'}]",2020-03-10 05:01:33+00:00,2020-03-24 00:35:29+00:00,1193636.0,"13 days, 19:33:56",[],"[{'commit_sha': '2fb428a35b3add6b0987f8c41c6377c491dc6c7a', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
382447820,Remove legacy Python 2 code and add pytype Python3 support,Addressed https://github.com/tensorflow/datasets/issues/1552,True,1553,https://api.github.com/repos/tensorflow/datasets/pulls/1553,https://github.com/tensorflow/datasets/pull/1553,closed,476,1288,468,4,12,1,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-03-02 15:27:32+00:00,2020-03-10 02:16:55+00:00,643763.0,"7 days, 10:49:23","[{'comment_id': 386682482, 'comment_body': 'Sorry about this, but could you remove the extra line here, as the example I wrote in https://github.com/tensorflow/datasets/pull/1553#issuecomment-593559304 ?\r\n\r\nLooks good otherwise.', 'comment_created': datetime.datetime(2020, 3, 2, 22, 12, 25, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}]","[{'commit_sha': '2734b2063a3db322cad77d1e6157becc4d8166c6', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b0d39933806ad4e2aa1d679f27f27bcdb2ffab2d', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6bf91cbba4a3aeb96fd4025a2e090c05cb62ac30', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2b204ab26d4259de1d648de97fd970062cf93cbb', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
380602413,Update datasets to clean up legacy code,"Fix https://github.com/tensorflow/datasets/issues/1519

",True,1522,https://api.github.com/repos/tensorflow/datasets/pulls/1522,https://github.com/tensorflow/datasets/pull/1522,closed,8,487,83,12,5,2,2,0,"[{'name': 'cla: yes'}, {'name': 'tfds:ready_to_merge'}]",2020-02-27 02:21:57+00:00,2020-02-28 16:29:55+00:00,137278.0,"1 day, 14:07:58","[{'comment_id': 384898730, 'comment_body': ""Note that you could also remove `_NUM_SHARDS = 123` at the top of the file, which isn't used anymore."", 'comment_created': datetime.datetime(2020, 2, 27, 3, 44, tzinfo=datetime.timezone.utc), 'commenter': 'Conchylicultor', 'type': 'User'}, {'comment_id': 384903700, 'comment_body': 'Yeah, I noticed that.', 'comment_created': datetime.datetime(2020, 2, 27, 4, 8, 22, tzinfo=datetime.timezone.utc), 'commenter': 'vijayphoenix', 'type': 'User'}]","[{'commit_sha': 'e32a2782d1f77a7fa75ef46b2cba45af90335f93', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b80d77e6a2d0b81c059487ca0823ca6b06426061', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7be969c2dfc2141a1d554f3118502a2839f3ce20', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6fa37b4d57689aa930a4d3860eeb19829a1b0f79', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '11b1b823af3766f9b1b8ddb7610e20f0601c0fd1', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'aeac7a51330fda1a408add542d615787d29ff3f5', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '34e8b99f88fa0f58293247a4c4bed7de23443aa0', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fe78c97d1d6823884e8c6a0c5c58ce576ba82183', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '52cbe2f6bd5b2bc63a8dfe9c7a7f0800f4b31423', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f63cbe05b475adb5213dd6d55d60b715bb925c91', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '78a8d2276595568918c2ceca06a1433712d6da21', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': '272316267ffef595f5000db4996e544a00940010', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
379755054,Update kitti dataset checksum files,"* Dataset Name: kitti dataset
* Issue Reference: https://github.com/tensorflow/datasets/issues/1221
* `dataset_info.json` Gist: [link](https://gist.github.com/vijayphoenix/cb778fc85a6f1fee3c1cf703ddd353c9)
  
  
## Checklist
* [X] Run `download_and_prepare` successfully
* [X] Add checksums file

CC: @Conchylicultor @cyfra, please review",True,1507,https://api.github.com/repos/tensorflow/datasets/pulls/1507,https://github.com/tensorflow/datasets/pull/1507,closed,2,2,2,2,7,0,2,0,"[{'name': 'cla: yes'}, {'name': 'community:please_review'}]",2020-02-25 19:20:12+00:00,2020-02-27 02:24:24+00:00,111852.0,"1 day, 7:04:12",[],"[{'commit_sha': '31b7dd18bb57e355f850dadeb72b89c9ebb16a4b', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c0a554c5c065cd5d588689239afc78f753c8b45a', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131
364556787,Fix some broken links,Fixed some broken links in [add_dataset.md](https://github.com/tensorflow/datasets/blob/master/docs/add_dataset.md),True,1400,https://api.github.com/repos/tensorflow/datasets/pulls/1400,https://github.com/tensorflow/datasets/pull/1400,closed,3,3,1,1,4,0,1,0,[{'name': 'cla: yes'}],2020-01-19 17:17:47+00:00,2020-01-21 18:58:59+00:00,178872.0,"2 days, 1:41:12",[],"[{'commit_sha': 'e9d951b98c4ffd7f5434496ddef380ca0b4ecbaf', 'committer_username': 'vijayphoenix', 'committer_name': 'Vijay Tadikamalla', 'committer_email': 'vijay.tadik@gmail.com', 'commit_date': datetime.datetime(2018, 7, 31, 21, 11, 12, tzinfo=datetime.timezone.utc)}]",Vijay Tadikamalla,41972768,vijay.tadik@gmail.com,User,,9,,36,131

Project_ID,Name,Full_name,Language,Forks,Stars,Watchers,contributors,commits,issues,branches,PRs_count,contributor pullrequests
148221325,datasets,tensorflow/datasets,Python,1527,4260,109,311,6426,689,171,271,"[{'id': 546956393, 'number': 2898, 'closed': datetime.datetime(2021, 1, 5, 14, 28, 3, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 12, 30, 14, 0, 9, tzinfo=datetime.timezone.utc), 'time_taken': 520074.0, 'time_delta': '6 days, 0:27:54', 'additions': 2, 'deletions': 0, 'state': 'closed'}, {'id': 546430281, 'number': 2896, 'closed': datetime.datetime(2021, 1, 6, 13, 50, 44, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 12, 29, 10, 41, 19, tzinfo=datetime.timezone.utc), 'time_taken': 702565.0, 'time_delta': '8 days, 3:09:25', 'additions': 8, 'deletions': 8, 'state': 'closed'}, {'id': 497282710, 'number': 2532, 'closed': datetime.datetime(2020, 10, 29, 11, 51, 21, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 10, 3, 14, 12, 49, tzinfo=datetime.timezone.utc), 'time_taken': 2237912.0, 'time_delta': '25 days, 21:38:32', 'additions': 10, 'deletions': 2, 'state': 'closed'}, {'id': 497186400, 'number': 2531, 'closed': datetime.datetime(2020, 10, 14, 10, 7, 14, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 10, 3, 0, 31, 7, tzinfo=datetime.timezone.utc), 'time_taken': 984967.0, 'time_delta': '11 days, 9:36:07', 'additions': 475, 'deletions': 0, 'state': 'closed'}, {'id': 497087422, 'number': 2529, 'closed': datetime.datetime(2020, 10, 8, 16, 8, 54, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 10, 2, 19, 10, 44, tzinfo=datetime.timezone.utc), 'time_taken': 507490.0, 'time_delta': '5 days, 20:58:10', 'additions': 36, 'deletions': 0, 'state': 'closed'}, {'id': 493107709, 'number': 2494, 'closed': datetime.datetime(2020, 10, 3, 8, 23, 54, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 9, 25, 14, 16, 36, tzinfo=datetime.timezone.utc), 'time_taken': 670038.0, 'time_delta': '7 days, 18:07:18', 'additions': 24, 'deletions': 4, 'state': 'closed'}, {'id': 491090803, 'number': 2477, 'closed': datetime.datetime(2020, 9, 24, 7, 34, 56, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 9, 22, 17, 31, 11, tzinfo=datetime.timezone.utc), 'time_taken': 137025.0, 'time_delta': '1 day, 14:03:45', 'additions': 14, 'deletions': 8, 'state': 'closed'}, {'id': 490212539, 'number': 2459, 'closed': datetime.datetime(2020, 9, 21, 22, 9, 54, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 9, 21, 11, 2, 28, tzinfo=datetime.timezone.utc), 'time_taken': 40046.0, 'time_delta': '11:07:26', 'additions': 22, 'deletions': 0, 'state': 'closed'}, {'id': 490211274, 'number': 2458, 'closed': datetime.datetime(2020, 9, 21, 23, 1, 23, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 9, 21, 11, 0, 32, tzinfo=datetime.timezone.utc), 'time_taken': 43251.0, 'time_delta': '12:00:51', 'additions': 1, 'deletions': 6, 'state': 'closed'}, {'id': 489810203, 'number': 2455, 'closed': datetime.datetime(2020, 9, 21, 22, 35, 56, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 9, 19, 18, 53, 48, tzinfo=datetime.timezone.utc), 'time_taken': 186128.0, 'time_delta': '2 days, 3:42:08', 'additions': 33, 'deletions': 9, 'state': 'closed'}, {'id': 488272845, 'number': 2444, 'closed': datetime.datetime(2020, 9, 18, 11, 39, 44, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 9, 16, 22, 13, 7, tzinfo=datetime.timezone.utc), 'time_taken': 134797.0, 'time_delta': '1 day, 13:26:37', 'additions': 3, 'deletions': 3, 'state': 'closed'}, {'id': 486204589, 'number': 2427, 'closed': datetime.datetime(2020, 9, 23, 8, 5, 5, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 9, 13, 16, 18, 28, tzinfo=datetime.timezone.utc), 'time_taken': 834397.0, 'time_delta': '9 days, 15:46:37', 'additions': 78, 'deletions': 5, 'state': 'closed'}, {'id': 477882978, 'number': 2388, 'closed': datetime.datetime(2020, 9, 11, 12, 44, 38, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 9, 2, 15, 2, 20, tzinfo=datetime.timezone.utc), 'time_taken': 769338.0, 'time_delta': '8 days, 21:42:18', 'additions': 6, 'deletions': 1, 'state': 'closed'}, {'id': 477856151, 'number': 2387, 'closed': datetime.datetime(2020, 9, 3, 20, 32, 18, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 9, 2, 14, 21, 56, tzinfo=datetime.timezone.utc), 'time_taken': 108622.0, 'time_delta': '1 day, 6:10:22', 'additions': 2, 'deletions': 6, 'state': 'closed'}, {'id': 475361911, 'number': 2354, 'closed': datetime.datetime(2020, 9, 3, 11, 58, 10, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 8, 28, 11, 53, 23, tzinfo=datetime.timezone.utc), 'time_taken': 518687.0, 'time_delta': '6 days, 0:04:47', 'additions': 40, 'deletions': 0, 'state': 'closed'}, {'id': 474156256, 'number': 2346, 'closed': datetime.datetime(2020, 9, 18, 12, 5, 39, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 8, 26, 21, 13, 32, tzinfo=datetime.timezone.utc), 'time_taken': 1954327.0, 'time_delta': '22 days, 14:52:07', 'additions': 183, 'deletions': 25, 'state': 'closed'}, {'id': 471657216, 'number': 2326, 'closed': datetime.datetime(2020, 8, 25, 8, 39, 4, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 8, 21, 14, 9, 54, tzinfo=datetime.timezone.utc), 'time_taken': 325750.0, 'time_delta': '3 days, 18:29:10', 'additions': 32, 'deletions': 15, 'state': 'closed'}, {'id': 470179777, 'number': 2322, 'closed': datetime.datetime(2020, 9, 16, 22, 49, 52, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 8, 19, 13, 52, 15, tzinfo=datetime.timezone.utc), 'time_taken': 2451457.0, 'time_delta': '28 days, 8:57:37', 'additions': 117, 'deletions': 21, 'state': 'closed'}, {'id': 467479417, 'number': 2301, 'closed': None, 'created': datetime.datetime(2020, 8, 13, 16, 1, 20, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 161, 'deletions': 108, 'state': 'open'}, {'id': 464655970, 'number': 2284, 'closed': datetime.datetime(2020, 8, 24, 23, 1, 46, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 8, 7, 15, 6, 16, tzinfo=datetime.timezone.utc), 'time_taken': 1497330.0, 'time_delta': '17 days, 7:55:30', 'additions': 7, 'deletions': 0, 'state': 'closed'}, {'id': 463456502, 'number': 2278, 'closed': datetime.datetime(2020, 8, 18, 13, 35, 24, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 8, 5, 15, 25, 2, tzinfo=datetime.timezone.utc), 'time_taken': 1116622.0, 'time_delta': '12 days, 22:10:22', 'additions': 18, 'deletions': 0, 'state': 'closed'}, {'id': 463021897, 'number': 2274, 'closed': datetime.datetime(2020, 8, 12, 18, 52, 33, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 8, 4, 21, 37, 44, tzinfo=datetime.timezone.utc), 'time_taken': 681289.0, 'time_delta': '7 days, 21:14:49', 'additions': 12, 'deletions': 12, 'state': 'closed'}, {'id': 462412147, 'number': 2269, 'closed': None, 'created': datetime.datetime(2020, 8, 3, 21, 43, 42, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 7, 'deletions': 3, 'state': 'open'}, {'id': 455970620, 'number': 2229, 'closed': datetime.datetime(2020, 7, 24, 18, 4, 50, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 7, 23, 21, 52, 22, tzinfo=datetime.timezone.utc), 'time_taken': 72748.0, 'time_delta': '20:12:28', 'additions': 3, 'deletions': 3, 'state': 'closed'}, {'id': 455211058, 'number': 2223, 'closed': datetime.datetime(2020, 7, 23, 1, 32, 3, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 7, 22, 16, 1, 3, tzinfo=datetime.timezone.utc), 'time_taken': 34260.0, 'time_delta': '9:31:00', 'additions': 1, 'deletions': 1, 'state': 'closed'}, {'id': 455176659, 'number': 2221, 'closed': datetime.datetime(2020, 8, 18, 13, 6, 57, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 7, 22, 15, 2, 34, tzinfo=datetime.timezone.utc), 'time_taken': 2325863.0, 'time_delta': '26 days, 22:04:23', 'additions': 288, 'deletions': 20, 'state': 'closed'}, {'id': 448580985, 'number': 2194, 'closed': datetime.datetime(2020, 8, 1, 7, 20, 3, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 7, 14, 0, 56, 13, tzinfo=datetime.timezone.utc), 'time_taken': 1578230.0, 'time_delta': '18 days, 6:23:50', 'additions': 269, 'deletions': 3, 'state': 'closed'}, {'id': 448547212, 'number': 2193, 'closed': datetime.datetime(2020, 7, 15, 1, 6, 53, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 7, 13, 23, 2, 47, tzinfo=datetime.timezone.utc), 'time_taken': 93846.0, 'time_delta': '1 day, 2:04:06', 'additions': 8, 'deletions': 6, 'state': 'closed'}, {'id': 447957731, 'number': 2186, 'closed': datetime.datetime(2020, 7, 24, 21, 45, 56, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 7, 12, 21, 33, 11, tzinfo=datetime.timezone.utc), 'time_taken': 1037565.0, 'time_delta': '12 days, 0:12:45', 'additions': 422, 'deletions': 333, 'state': 'closed'}, {'id': 447864504, 'number': 2185, 'closed': datetime.datetime(2020, 7, 26, 17, 2, 33, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 7, 12, 7, 42, 42, tzinfo=datetime.timezone.utc), 'time_taken': 1243191.0, 'time_delta': '14 days, 9:19:51', 'additions': 53, 'deletions': 0, 'state': 'closed'}, {'id': 447805353, 'number': 2183, 'closed': datetime.datetime(2020, 7, 23, 16, 51, 57, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 7, 11, 19, 7, 58, tzinfo=datetime.timezone.utc), 'time_taken': 1028639.0, 'time_delta': '11 days, 21:43:59', 'additions': 3, 'deletions': 3, 'state': 'closed'}, {'id': 442995630, 'number': 2144, 'closed': datetime.datetime(2020, 7, 23, 17, 19, 23, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 7, 1, 21, 18, 48, tzinfo=datetime.timezone.utc), 'time_taken': 1886435.0, 'time_delta': '21 days, 20:00:35', 'additions': 4, 'deletions': 2, 'state': 'closed'}, {'id': 442427792, 'number': 2142, 'closed': datetime.datetime(2020, 7, 1, 21, 50, 52, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 7, 1, 4, 8, 58, tzinfo=datetime.timezone.utc), 'time_taken': 63714.0, 'time_delta': '17:41:54', 'additions': 14, 'deletions': 13, 'state': 'closed'}, {'id': 442378342, 'number': 2141, 'closed': datetime.datetime(2020, 7, 24, 19, 28, 31, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 7, 1, 0, 58, 26, tzinfo=datetime.timezone.utc), 'time_taken': 2053805.0, 'time_delta': '23 days, 18:30:05', 'additions': 85, 'deletions': 4, 'state': 'closed'}, {'id': 442348358, 'number': 2140, 'closed': datetime.datetime(2020, 7, 1, 0, 58, 38, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 6, 30, 23, 19, 28, tzinfo=datetime.timezone.utc), 'time_taken': 5950.0, 'time_delta': '1:39:10', 'additions': 6, 'deletions': 1, 'state': 'closed'}, {'id': 438946206, 'number': 2117, 'closed': datetime.datetime(2020, 6, 25, 1, 41, 24, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 6, 24, 4, 0, 55, tzinfo=datetime.timezone.utc), 'time_taken': 78029.0, 'time_delta': '21:40:29', 'additions': 7, 'deletions': 5, 'state': 'closed'}, {'id': 438247410, 'number': 2112, 'closed': datetime.datetime(2020, 6, 24, 0, 7, 46, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 6, 23, 0, 40, 25, tzinfo=datetime.timezone.utc), 'time_taken': 84441.0, 'time_delta': '23:27:21', 'additions': 23, 'deletions': 3, 'state': 'closed'}, {'id': 436803399, 'number': 2099, 'closed': datetime.datetime(2020, 10, 8, 15, 54, 34, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 6, 18, 23, 3, 26, tzinfo=datetime.timezone.utc), 'time_taken': 9651068.0, 'time_delta': '111 days, 16:51:08', 'additions': 30, 'deletions': 14, 'state': 'closed'}, {'id': 436774170, 'number': 2098, 'closed': datetime.datetime(2020, 6, 25, 14, 22, 52, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 6, 18, 21, 33, 27, tzinfo=datetime.timezone.utc), 'time_taken': 578965.0, 'time_delta': '6 days, 16:49:25', 'additions': 2, 'deletions': 0, 'state': 'closed'}, {'id': 435901727, 'number': 2094, 'closed': datetime.datetime(2020, 6, 25, 17, 15, 5, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 6, 17, 14, 46, 55, tzinfo=datetime.timezone.utc), 'time_taken': 700090.0, 'time_delta': '8 days, 2:28:10', 'additions': 272, 'deletions': 0, 'state': 'closed'}, {'id': 434268599, 'number': 2088, 'closed': datetime.datetime(2020, 6, 25, 14, 21, 54, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 6, 15, 3, 45, 54, tzinfo=datetime.timezone.utc), 'time_taken': 902160.0, 'time_delta': '10 days, 10:36:00', 'additions': 155, 'deletions': 0, 'state': 'closed'}, {'id': 428356023, 'number': 2074, 'closed': datetime.datetime(2020, 6, 6, 1, 47, 46, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 6, 5, 10, 11, 32, tzinfo=datetime.timezone.utc), 'time_taken': 56174.0, 'time_delta': '15:36:14', 'additions': 7, 'deletions': 5, 'state': 'closed'}, {'id': 426246030, 'number': 2068, 'closed': datetime.datetime(2020, 6, 20, 21, 20, 36, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 6, 1, 22, 24, 10, tzinfo=datetime.timezone.utc), 'time_taken': 1637786.0, 'time_delta': '18 days, 22:56:26', 'additions': 325, 'deletions': 230, 'state': 'closed'}, {'id': 417304318, 'number': 2022, 'closed': datetime.datetime(2020, 5, 13, 18, 33, 23, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 5, 13, 11, 38, 31, tzinfo=datetime.timezone.utc), 'time_taken': 24892.0, 'time_delta': '6:54:52', 'additions': 1, 'deletions': 1, 'state': 'closed'}, {'id': 415086375, 'number': 2006, 'closed': datetime.datetime(2020, 5, 9, 0, 50, 43, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 5, 8, 7, 14, 42, tzinfo=datetime.timezone.utc), 'time_taken': 63361.0, 'time_delta': '17:36:01', 'additions': 4, 'deletions': 4, 'state': 'closed'}, {'id': 412126754, 'number': 1980, 'closed': datetime.datetime(2020, 5, 1, 18, 12, 38, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 5, 1, 13, 18, 9, tzinfo=datetime.timezone.utc), 'time_taken': 17669.0, 'time_delta': '4:54:29', 'additions': 2, 'deletions': 2, 'state': 'closed'}, {'id': 408969779, 'number': 1954, 'closed': datetime.datetime(2020, 4, 30, 0, 38, 11, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 25, 21, 2, 46, tzinfo=datetime.timezone.utc), 'time_taken': 358525.0, 'time_delta': '4 days, 3:35:25', 'additions': 38, 'deletions': 2, 'state': 'closed'}, {'id': 408774262, 'number': 1947, 'closed': None, 'created': datetime.datetime(2020, 4, 24, 21, 31, 27, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 148, 'deletions': 5, 'state': 'open'}, {'id': 407033572, 'number': 1929, 'closed': datetime.datetime(2020, 4, 29, 19, 50, 27, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 22, 4, 12, 45, tzinfo=datetime.timezone.utc), 'time_taken': 661062.0, 'time_delta': '7 days, 15:37:42', 'additions': 10, 'deletions': 19, 'state': 'closed'}, {'id': 405745967, 'number': 1916, 'closed': datetime.datetime(2020, 4, 21, 18, 8, 43, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 19, 22, 19, 52, tzinfo=datetime.timezone.utc), 'time_taken': 157731.0, 'time_delta': '1 day, 19:48:51', 'additions': 19, 'deletions': 8, 'state': 'closed'}, {'id': 405736503, 'number': 1915, 'closed': datetime.datetime(2020, 10, 28, 9, 15, 12, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 19, 21, 21, 45, tzinfo=datetime.timezone.utc), 'time_taken': 16545207.0, 'time_delta': '191 days, 11:53:27', 'additions': 7, 'deletions': 1, 'state': 'closed'}, {'id': 405731751, 'number': 1914, 'closed': datetime.datetime(2020, 4, 30, 3, 34, 37, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 19, 20, 48, 20, tzinfo=datetime.timezone.utc), 'time_taken': 888377.0, 'time_delta': '10 days, 6:46:17', 'additions': 40, 'deletions': 17, 'state': 'closed'}, {'id': 405729686, 'number': 1913, 'closed': datetime.datetime(2020, 4, 30, 17, 57, 16, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 19, 20, 34, 6, tzinfo=datetime.timezone.utc), 'time_taken': 940990.0, 'time_delta': '10 days, 21:23:10', 'additions': 11, 'deletions': 5, 'state': 'closed'}, {'id': 405727192, 'number': 1912, 'closed': datetime.datetime(2020, 4, 30, 3, 18, 5, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 19, 20, 16, 51, tzinfo=datetime.timezone.utc), 'time_taken': 889274.0, 'time_delta': '10 days, 7:01:14', 'additions': 10, 'deletions': 4, 'state': 'closed'}, {'id': 404751882, 'number': 1889, 'closed': datetime.datetime(2020, 4, 16, 23, 20, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 16, 21, 29, 7, tzinfo=datetime.timezone.utc), 'time_taken': 6653.0, 'time_delta': '1:50:53', 'additions': 1, 'deletions': 1, 'state': 'closed'}, {'id': 403959949, 'number': 1877, 'closed': datetime.datetime(2020, 4, 17, 19, 47, 34, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 15, 19, 58, 34, tzinfo=datetime.timezone.utc), 'time_taken': 172140.0, 'time_delta': '1 day, 23:49:00', 'additions': 12, 'deletions': 2, 'state': 'closed'}, {'id': 402044319, 'number': 1857, 'closed': datetime.datetime(2020, 4, 11, 0, 19, 35, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 10, 19, 50, 42, tzinfo=datetime.timezone.utc), 'time_taken': 16133.0, 'time_delta': '4:28:53', 'additions': 21, 'deletions': 20, 'state': 'closed'}, {'id': 402006550, 'number': 1855, 'closed': datetime.datetime(2020, 4, 10, 19, 24, 35, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 10, 17, 58, 31, tzinfo=datetime.timezone.utc), 'time_taken': 5164.0, 'time_delta': '1:26:04', 'additions': 21, 'deletions': 19, 'state': 'closed'}, {'id': 401318805, 'number': 1850, 'closed': datetime.datetime(2020, 7, 23, 17, 41, 26, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 9, 9, 39, 45, tzinfo=datetime.timezone.utc), 'time_taken': 9100901.0, 'time_delta': '105 days, 8:01:41', 'additions': 14, 'deletions': 14, 'state': 'closed'}, {'id': 400386800, 'number': 1837, 'closed': datetime.datetime(2020, 4, 7, 17, 55, 52, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 7, 16, 52, 18, tzinfo=datetime.timezone.utc), 'time_taken': 3814.0, 'time_delta': '1:03:34', 'additions': 3, 'deletions': 7, 'state': 'closed'}, {'id': 400035201, 'number': 1832, 'closed': datetime.datetime(2020, 4, 7, 18, 34, 53, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 7, 4, 48, 3, tzinfo=datetime.timezone.utc), 'time_taken': 49610.0, 'time_delta': '13:46:50', 'additions': 90, 'deletions': 29, 'state': 'closed'}, {'id': 400006236, 'number': 1831, 'closed': datetime.datetime(2020, 4, 27, 22, 48, 28, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 7, 2, 49, 4, tzinfo=datetime.timezone.utc), 'time_taken': 1799964.0, 'time_delta': '20 days, 19:59:24', 'additions': 285, 'deletions': 3, 'state': 'closed'}, {'id': 399977008, 'number': 1826, 'closed': datetime.datetime(2020, 4, 8, 14, 45, 1, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 7, 0, 46, 24, tzinfo=datetime.timezone.utc), 'time_taken': 136717.0, 'time_delta': '1 day, 13:58:37', 'additions': 1584, 'deletions': 0, 'state': 'closed'}, {'id': 397927716, 'number': 1795, 'closed': datetime.datetime(2020, 6, 18, 23, 18, 54, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 3, 4, 22, 49, tzinfo=datetime.timezone.utc), 'time_taken': 6634565.0, 'time_delta': '76 days, 18:56:05', 'additions': 2, 'deletions': 4, 'state': 'closed'}, {'id': 397919466, 'number': 1793, 'closed': datetime.datetime(2020, 4, 3, 18, 35, 39, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 4, 3, 3, 45, 47, tzinfo=datetime.timezone.utc), 'time_taken': 53392.0, 'time_delta': '14:49:52', 'additions': 0, 'deletions': 4649493, 'state': 'closed'}, {'id': 396437064, 'number': 1774, 'closed': None, 'created': datetime.datetime(2020, 3, 31, 16, 7, 17, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 321, 'deletions': 77, 'state': 'open'}, {'id': 396435265, 'number': 1773, 'closed': datetime.datetime(2020, 4, 2, 0, 25, 12, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 31, 16, 4, 12, tzinfo=datetime.timezone.utc), 'time_taken': 116460.0, 'time_delta': '1 day, 8:21:00', 'additions': 6, 'deletions': 6, 'state': 'closed'}, {'id': 395084266, 'number': 1751, 'closed': datetime.datetime(2020, 3, 30, 20, 13, 31, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 28, 11, 45, 21, tzinfo=datetime.timezone.utc), 'time_taken': 203290.0, 'time_delta': '2 days, 8:28:10', 'additions': 1, 'deletions': 1, 'state': 'closed'}, {'id': 395077661, 'number': 1750, 'closed': datetime.datetime(2020, 4, 3, 3, 48, 4, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 28, 10, 40, 16, tzinfo=datetime.timezone.utc), 'time_taken': 493668.0, 'time_delta': '5 days, 17:07:48', 'additions': 26, 'deletions': 3, 'state': 'closed'}, {'id': 395015557, 'number': 1747, 'closed': datetime.datetime(2020, 6, 3, 23, 47, 12, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 28, 0, 4, 10, tzinfo=datetime.timezone.utc), 'time_taken': 5874182.0, 'time_delta': '67 days, 23:43:02', 'additions': 16, 'deletions': 15, 'state': 'closed'}, {'id': 394950112, 'number': 1745, 'closed': datetime.datetime(2020, 3, 28, 2, 18, 55, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 27, 20, 13, 28, tzinfo=datetime.timezone.utc), 'time_taken': 21927.0, 'time_delta': '6:05:27', 'additions': 9, 'deletions': 8, 'state': 'closed'}, {'id': 394473715, 'number': 1737, 'closed': datetime.datetime(2020, 4, 2, 22, 36, 33, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 26, 23, 30, 21, tzinfo=datetime.timezone.utc), 'time_taken': 601572.0, 'time_delta': '6 days, 23:06:12', 'additions': 821, 'deletions': 757, 'state': 'closed'}, {'id': 394360643, 'number': 1734, 'closed': datetime.datetime(2020, 11, 12, 16, 38, 53, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 26, 18, 58, 16, tzinfo=datetime.timezone.utc), 'time_taken': 19950037.0, 'time_delta': '230 days, 21:40:37', 'additions': 7, 'deletions': 3, 'state': 'closed'}, {'id': 394322889, 'number': 1733, 'closed': datetime.datetime(2020, 4, 30, 2, 55, 35, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 26, 17, 47, 33, tzinfo=datetime.timezone.utc), 'time_taken': 2970482.0, 'time_delta': '34 days, 9:08:02', 'additions': 19, 'deletions': 19, 'state': 'closed'}, {'id': 394138372, 'number': 1731, 'closed': None, 'created': datetime.datetime(2020, 3, 26, 12, 25, 3, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 13, 'deletions': 12, 'state': 'open'}, {'id': 393296350, 'number': 1718, 'closed': datetime.datetime(2020, 3, 28, 6, 1, 7, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 24, 22, 57, 32, tzinfo=datetime.timezone.utc), 'time_taken': 284615.0, 'time_delta': '3 days, 7:03:35', 'additions': 32, 'deletions': 49, 'state': 'closed'}, {'id': 393275694, 'number': 1717, 'closed': datetime.datetime(2020, 3, 27, 17, 16, 25, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 24, 21, 55, 25, tzinfo=datetime.timezone.utc), 'time_taken': 242460.0, 'time_delta': '2 days, 19:21:00', 'additions': 189, 'deletions': 170, 'state': 'closed'}, {'id': 393245109, 'number': 1712, 'closed': datetime.datetime(2020, 3, 26, 18, 16, 40, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 24, 20, 41, 39, tzinfo=datetime.timezone.utc), 'time_taken': 164101.0, 'time_delta': '1 day, 21:35:01', 'additions': 9, 'deletions': 7, 'state': 'closed'}, {'id': 393093317, 'number': 1706, 'closed': datetime.datetime(2020, 4, 3, 17, 13, 21, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 24, 16, 12, 54, tzinfo=datetime.timezone.utc), 'time_taken': 867627.0, 'time_delta': '10 days, 1:00:27', 'additions': 15, 'deletions': 11, 'state': 'closed'}, {'id': 392585000, 'number': 1702, 'closed': datetime.datetime(2020, 10, 14, 10, 25, 58, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 23, 19, 16, 35, tzinfo=datetime.timezone.utc), 'time_taken': 17680163.0, 'time_delta': '204 days, 15:09:23', 'additions': 20, 'deletions': 1, 'state': 'closed'}, {'id': 389789979, 'number': 1658, 'closed': None, 'created': datetime.datetime(2020, 3, 17, 11, 52, 27, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 47, 'deletions': 43, 'state': 'open'}, {'id': 385938472, 'number': 1619, 'closed': datetime.datetime(2020, 3, 24, 0, 35, 29, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 10, 5, 1, 33, tzinfo=datetime.timezone.utc), 'time_taken': 1193636.0, 'time_delta': '13 days, 19:33:56', 'additions': 4, 'deletions': 12, 'state': 'closed'}, {'id': 385762215, 'number': 1611, 'closed': datetime.datetime(2020, 7, 31, 1, 39, 28, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 9, 18, 55, 43, tzinfo=datetime.timezone.utc), 'time_taken': 12379425.0, 'time_delta': '143 days, 6:43:45', 'additions': 30, 'deletions': 5, 'state': 'closed'}, {'id': 384523377, 'number': 1586, 'closed': datetime.datetime(2020, 3, 24, 20, 27, 13, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 5, 22, 3, 19, tzinfo=datetime.timezone.utc), 'time_taken': 1635834.0, 'time_delta': '18 days, 22:23:54', 'additions': 27, 'deletions': 1, 'state': 'closed'}, {'id': 384238823, 'number': 1583, 'closed': datetime.datetime(2020, 4, 7, 1, 41, 20, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 5, 12, 8, 31, tzinfo=datetime.timezone.utc), 'time_taken': 2813569.0, 'time_delta': '32 days, 13:32:49', 'additions': 122, 'deletions': 4, 'state': 'closed'}, {'id': 384099867, 'number': 1580, 'closed': datetime.datetime(2020, 8, 28, 15, 54, 12, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 5, 6, 55, 8, tzinfo=datetime.timezone.utc), 'time_taken': 15238744.0, 'time_delta': '176 days, 8:59:04', 'additions': 36, 'deletions': 15, 'state': 'closed'}, {'id': 383417326, 'number': 1567, 'closed': datetime.datetime(2020, 3, 4, 9, 57, 8, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 4, 8, 54, 38, tzinfo=datetime.timezone.utc), 'time_taken': 3750.0, 'time_delta': '1:02:30', 'additions': 1, 'deletions': 1, 'state': 'closed'}, {'id': 382447820, 'number': 1553, 'closed': datetime.datetime(2020, 3, 10, 2, 16, 55, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 3, 2, 15, 27, 32, tzinfo=datetime.timezone.utc), 'time_taken': 643763.0, 'time_delta': '7 days, 10:49:23', 'additions': 476, 'deletions': 1288, 'state': 'closed'}, {'id': 380602413, 'number': 1522, 'closed': datetime.datetime(2020, 2, 28, 16, 29, 55, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 2, 27, 2, 21, 57, tzinfo=datetime.timezone.utc), 'time_taken': 137278.0, 'time_delta': '1 day, 14:07:58', 'additions': 8, 'deletions': 487, 'state': 'closed'}, {'id': 379755054, 'number': 1507, 'closed': datetime.datetime(2020, 2, 27, 2, 24, 24, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 2, 25, 19, 20, 12, tzinfo=datetime.timezone.utc), 'time_taken': 111852.0, 'time_delta': '1 day, 7:04:12', 'additions': 2, 'deletions': 2, 'state': 'closed'}, {'id': 368228869, 'number': 1427, 'closed': None, 'created': datetime.datetime(2020, 1, 28, 21, 5, 21, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 137, 'deletions': 0, 'state': 'open'}, {'id': 364556787, 'number': 1400, 'closed': datetime.datetime(2020, 1, 21, 18, 58, 59, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2020, 1, 19, 17, 17, 47, tzinfo=datetime.timezone.utc), 'time_taken': 178872.0, 'time_delta': '2 days, 1:41:12', 'additions': 3, 'deletions': 3, 'state': 'closed'}]"
