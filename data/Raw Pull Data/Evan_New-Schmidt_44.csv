pr_id,pr_title,pr_body,is_merged,pr_number,pr_url,pr_html_url,pr_state,additions,deletions,pr_changed_files,pr_commits_count,pr_comments_count,pr_review_comments_count,pr_labels_count,pr_assignees_count,pr_labels,pr_created_at,pr_closed_at,time_taken,time_delta,pr_review_comments,pr_commits,contributor,contributor_id,contributor_email,contributor_type,contributions,contributor_public_repos,contributor_private_repos,contributor_followings,contributor_followers
1405729219,Generator directory format,"I decided to break up the next steps into smaller PRs compared to the last one.

This PR updates the program to create to the folder structure that the map generator expects, e.g.:

```
.
├── de.wikipedia.org
│  └── wiki
│     ├── Coal_River_Springs_Territorial_Park
│     │  ├── de.html
│     │  └── ru.html
│     ├── Ni'iinlii_Njik_(Fishing_Branch)_Territorial_Park
│     │  ├── de.html
│     │  └── en.html
│    ...
├── en.wikipedia.org
│  └── wiki
│     ├── Arctic_National_Wildlife_Refuge
│     │  ├── de.html
│     │  ├── en.html
│     │  ├── es.html
│     │  ├── fr.html
│     │  └── ru.html
│     │
│     │ **NOTE: Article titles with a `/` are not escaped, so ""Baltimore/Washington_International_Airport"" becomes two subfolders as below.**
│     │
│     ├── Baltimore
│     │  └── Washington_International_Airport
│     │     ├── de.html
│     │     ├── en.html
│     │     ├── es.html
│     │     ├── fr.html
│     │     └── ru.html
│    ...
└── wikidata
   ├── Q59320
   │  ├── de.html
   │  ├── en.html
   │  ├── es.html
   │  ├── fr.html
   │  └── ru.html
   ├── Q120306
   │  ├── de.html
   │  ├── en.html
   │  ├── es.html
   │  ├── fr.html
   │  └── ru.html
  ...
```

While the old description scraper would write duplicates for the same article's title and qid, this implementation writes symlinks in the wikipedia tree that point to the wikidata files.

I know I can change what the generator looks for, but I figured it would be easier to have this working and then change them _together_ instead of debugging both at the same time while neither works.

The goal is that with this PR, the parser will be a **drop-in replacement for the current scraper**, even if the speed and html size is not what we'd like.

Remaining work for this PR:
- [x] handle articles without QIDs (yes, they exist! :shrug:)
- [x] only write symlinks for requested redirects
- [x] handle updating existing files ~~(e.g. timestamps)~~ _timestamps moved to #9_
- [x] do a test run with the generator and multiple languages
- [x] add documentation for running with multiple languages",True,6,https://api.github.com/repos/organicmaps/wikiparser/pulls/6,https://github.com/organicmaps/wikiparser/pull/6,closed,233,95,4,1,5,36,0,0,[],2023-06-23 22:00:14+00:00,2023-07-10 14:34:21+00:00,1442047.0,"16 days, 16:34:07","[{'comment_id': 1240608890, 'comment_body': 'How are they processed in the generator?', 'comment_created': datetime.datetime(2023, 6, 24, 5, 45, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1240608905, 'comment_body': 'For example?', 'comment_created': datetime.datetime(2023, 6, 24, 5, 45, 9, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1240610969, 'comment_body': 'Lang is used two times here in the path, but only one file is always stored in the directory, right?', 'comment_created': datetime.datetime(2023, 6, 24, 5, 47, 9, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1240612609, 'comment_body': 'Can / be percent-escaped in such cases? How the generator handles it now?', 'comment_created': datetime.datetime(2023, 6, 24, 5, 48, 53, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1240612679, 'comment_body': 'Is more than one slash in the title possible?', 'comment_created': datetime.datetime(2023, 6, 24, 5, 49, 27, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1240903020, 'comment_body': ""The generator only works with complete wikipedia urls (and wikidata QIDs). The OSM tags like `en:Article_Title` are converted to urls somewhere early in the OSM ingestion process. \r\nIt [dumps the urls to a file](https://github.com/organicmaps/organicmaps/blob/acc7c0547db4285dd8841ae7f98811268e38d908/generator/wiki_url_dumper.cpp#L63) for the descriptions scraper, then when it adds them to the mwm files [it strips the protocol, appends the url to the base directory, and looks for language html files in the folder at that location.](https://github.com/organicmaps/organicmaps/blob/34bbdf6a2f077b3d629b3f17e8e05bd18a4e4110/generator/descriptions_section_builder.cpp#L142).\r\nIt doesn't do any special processing for articles with a slash in the title, they are just another subdirectory down. I'll update the diagram to show that."", 'comment_created': datetime.datetime(2023, 6, 24, 17, 6, 15, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1240904263, 'comment_body': 'The ""incorrect links, directories"" refers to updating a directory tree from a previous run, instead of starting from scratch. Right now the behavior is to skip any file that exists.', 'comment_created': datetime.datetime(2023, 6, 24, 17, 15, 1, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1240906059, 'comment_body': ""The behavior that the generator/scraper expects is to write all available translations in each directory.\r\nSo for the article for [Berlin](https://en.wikipedia.org/wiki/Berlin), if there are OSM tags for `wikipedia:en=Berlin`, `wikipedia:de=Berlin`, `wikipedia:fr=Berlin` and `wikidata=Q64`, and the generator keeps them all, then there will be four folders with duplicates of all language copies:\r\n\r\n```\r\nen.wikipedia.org/wiki/Berlin/{en.html, de.html, fr.html, ...}\r\nde.wikipedia.org/wiki/Berlin/{en.html, de.html, fr.html, ...}\r\nfr.wikipedia.org/wiki/Berlin/{en.html, de.html, fr.html, ...}\r\nwikidata/Q64/{en.html, de.html, fr.html, ...}\r\n```\r\n\r\nNow, I don't understand exactly how the generator picks which tags to use yet, but just from looking at the Canada Yukon region map there are duplicated copies of wikipedia items there.\r\n\r\nFor this program, we only see one language at a time, so we write that copy to the master wikidata directory. When later we get the same article in a different language, we write it to the same wikidata directory.\r\n\r\nOnce all the languages have been processed, it would look like:\r\n```\r\nen.wikipedia.org/wiki/Berlin/ -> wikidata/Q64/\r\nde.wikipedia.org/wiki/Berlin/ -> wikidata/Q64/\r\nfr.wikipedia.org/wiki/Berlin/ -> wikidata/Q64/\r\nwikidata/Q64/{en.html, de.html, fr.html, ...}\r\n```"", 'comment_created': datetime.datetime(2023, 6, 24, 17, 28, 10, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1240928446, 'comment_body': ""I guess it could be, I haven't looked for that. Wikipedia works with either.\r\n\r\nSee below for more details, but the generator should decode those before dumping the urls.\r\n\r\nIt looks like a handful of encoded titles still slip through, but none with `%2F`=`/`.\r\nI made an issue with some notes about this in #7.\r\n\r\nFrom my read of when it [first adds a wikipedia tag](https://github.com/organicmaps/organicmaps/blob/34bbdf6a2f077b3d629b3f17e8e05bd18a4e4110/generator/osm2meta.cpp#L241) and later [writes it as a url](https://github.com/organicmaps/organicmaps/blob/34bbdf6a2f077b3d629b3f17e8e05bd18a4e4110/indexer/feature_meta.cpp#L19):\r\n1. If the tag looks like a url instead of the expected `lang:Article Title` format, take what's after `.wikipedia.org/wiki/`, url decode it, replace underscores with spaces, then concat that with the lang at the beginning of the url and store it.\r\n2. Otherwise attempt to check if it's a url, replace underscores with spaces, and store it.\r\n3. To transform it back into a url, replace spaces with underscores in the title, escape any `%`s, and add it to the end of `https://lang.wikipedia.org/wiki/`.\r\n\r\nGlancing at the [url decoding](https://github.com/organicmaps/organicmaps/blob/34bbdf6a2f077b3d629b3f17e8e05bd18a4e4110/coding/url.cpp#L118), I don't think there's anything wrong with it - it should handle arbitrary characters, although neither the encoding or decoding look unicode-aware."", 'comment_created': datetime.datetime(2023, 6, 24, 19, 47, 21, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1240930226, 'comment_body': ""Yes, there are a handful, for example <https://en.wikipedia.org/wiki/KXTV/KOVR/KCRA_Tower>.\r\n\r\n<details><summary>There are 39 present in the generator urls</summary>\r\n<pre>\r\n$ grep -E '^https://\\w+\\.wikipedia\\.org/wiki/.+/.+/' /tmp/wikipedia_urls.txt | sort | uniq\r\nhttps://de.wikipedia.org/wiki/Darum/Gretesch/Lüstringen\r\nhttps://de.wikipedia.org/wiki/Kienhorst/Köllnseen/Eichheide\r\nhttps://de.wikipedia.org/wiki/Liste_der_Baudenkmäler_in_Erlangen/A#Altstädter_Friedhof_2/3,_Altstädter_Friedhof\r\nhttps://de.wikipedia.org/wiki/Liste_der_Baudenkmäler_in_Neuss_(1/001-1/099)\r\nhttps://de.wikipedia.org/wiki/Liste_der_Baudenkmäler_in_Neuss_(1/001–1/099)\r\nhttps://de.wikipedia.org/wiki/Liste_der_Baudenkmäler_in_Neuss_(1/001–1/099)#Evang._Christuskirche\r\nhttps://de.wikipedia.org/wiki/Liste_der_Baudenkmäler_in_Neuss_(1/100–1/199)\r\nhttps://de.wikipedia.org/wiki/Liste_der_Baudenkmäler_in_Neuss_(1/200–1/299)\r\nhttps://de.wikipedia.org/wiki/Liste_der_Baudenkmäler_in_Neuss_(1/300–1/399)\r\nhttps://de.wikipedia.org/wiki/Liste_der_Baudenkmäler_in_Neuss_(1/400–1/499)\r\nhttps://de.wikipedia.org/wiki/Liste_der_Baudenkmäler_in_Neuss_(1/500–1/580)\r\nhttps://de.wikipedia.org/wiki/Liste_der_Baudenkmäler_in_Neuss_(1/500–1/580)#Schulgeb.C3.A4ude\r\nhttps://de.wikipedia.org/wiki/Rhumeaue/Ellerniederung/Gillersheimer_Bachtal\r\nhttps://de.wikipedia.org/wiki/Speck_/_Wehl_/_Helpenstein\r\nhttps://de.wikipedia.org/wiki/Veldrom/Feldrom/Kempen\r\nhttps://de.wikipedia.org/wiki/VHS_Witten/Wetter/Herdecke\r\nhttps://de.wikipedia.org/wiki/Wikipedia:WikiProjekt_Österreich/JE/Bach\r\nhttps://de.wikipedia.org/wiki/Wikipedia:WikiProjekt_Österreich/JE/Judenberg\r\nhttps://de.wikipedia.org/wiki/Wikipedia:WikiProjekt_Österreich/JE/Kramerberg\r\nhttps://de.wikipedia.org/wiki/Wikipedia:WikiProjekt_Österreich/JE/Loasleiten\r\nhttps://de.wikipedia.org/wiki/Wikipedia:WikiProjekt_Österreich/JE/Pelzereck\r\nhttps://de.wikipedia.org/wiki/Wikipedia:WikiProjekt_Österreich/JE/Theresienberg\r\nhttps://de.wikipedia.org/wiki/Wohnanlage_Arzbacher_Straße/Thalkirchner_Straße/Wackersberger_Straße/Würzstraße\r\nhttps://en.wikipedia.org/wiki/Abura/Asebu/Kwamankese_District\r\nhttps://en.wikipedia.org/wiki/Ajumako/Enyan/Essiam_District\r\nhttps://en.wikipedia.org/wiki/Bibiani/Anhwiaso/Bekwai_Municipal_District\r\nhttps://en.wikipedia.org/wiki/Clapp/Langley/Crawford_Complex\r\nhttps://en.wikipedia.org/wiki/KXTV/KOVR/KCRA_Tower\r\nhttps://en.wikipedia.org/wiki/SAIT/AUArts/Jubilee_station\r\nhttps://en.wikipedia.org/wiki/Santa_Cruz/Graciosa_Bay/Luova_Airport\r\nhttps://fr.wikipedia.org/wiki/Landunvez#/media/Fichier:10_Samson_C.jpg\r\nhttps://gl.wikipedia.org/wiki/Moaña#/media/Ficheiro:Plano_de_Moaña.png\r\nhttps://it.wikipedia.org/wiki/Tswagare/Lothoje/Lokalana\r\nhttps://lb.wikipedia.org/wiki/Lëscht_vun_den_nationale_Monumenter_an_der_Gemeng_Betzder#/media/Fichier:Roodt-sur-Syre,_14_rue_d'Olingen.jpg\r\nhttps://pt.wikipedia.org/wiki/Wikipédia:Wikipédia_na_Universidade/Cursos/Rurtugal/Gontães\r\nhttps://ru.wikipedia.org/wiki/Алажиде#/maplink/0\r\nhttps://uk.wikipedia.org/wiki/Вікіпедія:Вікі_любить_пам'ятки/Волинська_область/Старовижівський_район\r\nhttps://uk.wikipedia.org/wiki/Вікіпедія:Вікі_любить_пам'ятки/Київська_область/Броварський_район\r\nhttps://uk.wikipedia.org/wiki/Вікіпедія:Вікі_любить_пам'ятки/Полтавська_область/Семенівський_район\r\n</pre>\r\n</details> "", 'comment_created': datetime.datetime(2023, 6, 24, 19, 53, 50, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1240941428, 'comment_body': ""Good. Please don't forget that if something can be simplified or improved by changing the current generator approach, then it makes sense to do it."", 'comment_created': datetime.datetime(2023, 6, 24, 20, 45, 44, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1242232248, 'comment_body': ""I'm working on a list of changes that would be helpful"", 'comment_created': datetime.datetime(2023, 6, 26, 13, 52, 47, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1253462408, 'comment_body': 'List? Delimited by what? Any example? Is specifying a directory with dumps better?', 'comment_created': datetime.datetime(2023, 7, 5, 18, 11, 13, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1253462788, 'comment_body': 'Why it should be at PATH? Can it be run from any directory?', 'comment_created': datetime.datetime(2023, 7, 5, 18, 11, 36, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1253465466, 'comment_body': 'Is extracting ids directly from the osm pbf planet dump better than relying on the intermediate generator files? What are pros and cons?', 'comment_created': datetime.datetime(2023, 7, 5, 18, 13, 39, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1253465882, 'comment_body': 'nit: Start sentences with a capital letter and end them with a dot.', 'comment_created': datetime.datetime(2023, 7, 5, 18, 14, 4, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1253469151, 'comment_body': 'Would a hint about om-wikiparser command line options be helpful?', 'comment_created': datetime.datetime(2023, 7, 5, 18, 17, 35, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1253964288, 'comment_body': 'Print file name too?', 'comment_created': datetime.datetime(2023, 7, 6, 5, 49, 23, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1253964447, 'comment_body': 'In which cases dates can be different?', 'comment_created': datetime.datetime(2023, 7, 6, 5, 49, 38, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1253964888, 'comment_body': 'How will it work now?', 'comment_created': datetime.datetime(2023, 7, 6, 5, 50, 22, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1254497269, 'comment_body': 'I meant a shell list/array(?), separated by spaces.\r\n\r\nOne example is a glob, so using a directory and then referencing `$WIKIPEDIA_DUMP_DIRECTORY/*.json.tar.gz` might be clearer?', 'comment_created': datetime.datetime(2023, 7, 6, 14, 7, 21, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1254542661, 'comment_body': ""It doesn't need to be, the example script read more clearly to me if it's in the context of the `intermediate_data` directory. It could also be run as `../../../wikiparser/target/release/om-wikiparser`, with `cargo run --release` from the wikiparser directory, or anything else."", 'comment_created': datetime.datetime(2023, 7, 6, 14, 41, 6, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1254557677, 'comment_body': ""Pros:\r\n- Independent of the generator process. Can be run as soon as planet file is updated.\r\n\r\nCons:\r\n- Need to keep osm query in sync with generator's own multi-step filtering and transformation process.\r\n- Need to match generator's multi-step processing of urls exactly.\r\n\r\nWhen I did this earlier, it was with the `osm-filter` tool, I only tested it on the yukon region, and it output _more_ entries than the generator did.\r\n\r\nI can create an issue for this, but the rough steps to get that working are:\r\n- Convert `osmfilter` query to `osmium` command so it can work on `pbf` files directly.\r\n- Dig into generator map processing to try to improve querying.\r\n- Compare processing of a complete planet with generator output.\r\n- Write conversion of `osmuim` output for `wikiparser` to use."", 'comment_created': datetime.datetime(2023, 7, 6, 14, 52, 10, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1254558606, 'comment_body': 'Sorry, old habits die hard.', 'comment_created': datetime.datetime(2023, 7, 6, 14, 52, 54, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1254560707, 'comment_body': ""That's an old TODO, I'll remove it. It returns any parse errors it encounters with the title and redirects."", 'comment_created': datetime.datetime(2023, 7, 6, 14, 54, 30, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1254563194, 'comment_body': 'The debug line above does that.', 'comment_created': datetime.datetime(2023, 7, 6, 14, 56, 27, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1254568386, 'comment_body': ""That's referring to #9, but I should remove that line now that it is designed to overwrite the directories from a previous run."", 'comment_created': datetime.datetime(2023, 7, 6, 15, 0, 20, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1254572609, 'comment_body': ""It's better to mention list item separators explicitly and provide some example for clarity."", 'comment_created': datetime.datetime(2023, 7, 6, 15, 3, 40, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1254573753, 'comment_body': '...then why suggesting to install the tool at PATH?', 'comment_created': datetime.datetime(2023, 7, 6, 15, 4, 39, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1254587643, 'comment_body': '1. Does it make sense to create an issue to document the existing generator\'s output format, and propose some improvements if necessary? What kind of complex transformations are done in the generator now, and why?\r\n2. What\'s wrong in outputting more URLs? I assume that the generator may filter now OSM POIs/types that we are not supporting yet. In the worst case, some more articles will be extracted from the planet, right? Do you remember how big is the percent of ""unnecessary"" articles?\r\n3. osmfilter can work with o5m, osmconvert can process pbf. There is also https://docs.rs/osmpbf/latest/osmpbf/ for direct pbf processing if it makes the approach simpler. How good is the osmium tool compared to other options?\r\n\r\nIt would be great to have a well-defined and independent API between the generator and wikiparser, to avoid complications when supporting it in the longer term. WDYT?', 'comment_created': datetime.datetime(2023, 7, 6, 15, 15, 48, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1254594764, 'comment_body': 'So that you can always reference it as `om-wikiparser` wherever you are, without worrying about where it is relative to you, or copying it into your working directory.\r\n\r\nI meant this as an explanation of how to use it, not a step-by-step for what to run on the build server filesystem.\r\n\r\nMaybe writing a shell script to use on the maps server instead would be helpful?\r\n\r\nWould you prefer:\r\n```shell\r\n# Transform intermediate files from generator.\r\ncut -f 2 id_to_wikidata.csv > wikidata_ids.txt\r\ntail -n +2 wiki_urls.txt | cut -f 3 > wikipedia_urls.txt\r\n# Begin extraction.\r\nfor dump in $WIKIPEDIA_ENTERPRISE_DUMPS\r\ndo\r\n  tar xzf $dump | $WIKIPARSER_DIR/target/release/om-wikiparser \\\r\n    --wikidata-ids wikidata_ids.txt \\\r\n    --wikipedia-urls wikipedia_urls.txt \\\r\n    descriptions/\r\ndone\r\n```\r\nor\r\n```shell\r\n# Transform intermediate files from generator.\r\nmaps_build=~/maps_build/$BUILD_DATE/intermediate_data\r\ncut -f 2 $maps_build/id_to_wikidata.csv > $maps_build/wikidata_ids.txt\r\ntail -n +2 $maps_build/wiki_urls.txt | cut -f 3 > $maps_build/wikipedia_urls.txt\r\n# Begin extraction.\r\nfor dump in $WIKIPEDIA_ENTERPRISE_DUMPS\r\ndo\r\n  tar xzf $dump | ./target/release/om-wikiparser \\\r\n    --wikidata-ids $maps_build/wikidata_ids.txt \\\r\n    --wikipedia-urls $maps_build/wikipedia_urls.txt \\\r\n    $maps_build/descriptions/\r\ndone\r\n```', 'comment_created': datetime.datetime(2023, 7, 6, 15, 21, 47, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1254644330, 'comment_body': ""1. Can it be wrapped in a helper script that can be easily customized and run on the generator, maybe directly from the wikiparser repo? :)\r\n2. `cargo run -r` may be even better instead of a path to binary :) But it's also ok to hard-code the path or use `$WIKIPARSER_BINARY` var.\r\n\r\nThink about me testing your code soon on a production server. Less surprises = less stress ;-)"", 'comment_created': datetime.datetime(2023, 7, 6, 16, 4, 6, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1254645796, 'comment_body': 'Btw, it may make sense to also print/measure time taken to execute some commands after the first run on the whole planet, to have some reference starting values.', 'comment_created': datetime.datetime(2023, 7, 6, 16, 5, 30, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1254668123, 'comment_body': '> It would be great to have a well-defined and independent API between the generator and wikiparser, to avoid complications when supporting it in the longer term.\r\n\r\nAbsolutely agree!\r\n\r\n> Does it make sense to create an issue to document the existing generator\'s output format, and propose some improvements if necessary? What kind of complex transformations are done in the generator now, and why?\r\n\r\nI think so, do you mean the wikipedia/wikidata files or the mwm format in general?\r\n\r\nBy transformations, when I looked at it last, it looked like it was doing some sort of merging of ways/nodes/shapes to get a single parent object.\r\nWhen I compared the OSM IDs that it output with the Wikidata ids it didn\'t match up with what I got from `osmfilter`, even if the urls were the same. Not a problem for the wikiparser, as long as the QIDs/articles are all caught, but it was harder to tell if they were doing the same thing.\r\n\r\nAs we talked about before, there are also multiple layers of filtering nodes by amenity or other tags, and I only looked at the final layer when I was trying this `osmfilter` approach (based on [`ftypes_matcher.cpp`](https://github.com/organicmaps/organicmaps/blob/982c6aa92d7196a5690dcdc1564e427de7611806/indexer/ftypes_matcher.cpp#L473)).\r\n\r\n> What\'s wrong in outputting more URLs? I assume that the generator may filter now OSM POIs/types that we are not supporting yet. In the worst case, some more articles will be extracted from the planet, right?\r\n\r\nAs you say, the worst case isn\'t a problem for the end user, but I want to do more comparisons with the whole planet file to be confident that this is really a superset of them.\r\n\r\n> Do you remember how big is the percent of ""unnecessary"" articles?\r\n\r\nThat was around 25%, but in the Yukon territory so not very many nodes and I would guess not comparable to the planet.\r\n\r\n> How good is the osmium tool compared to other options?\r\n\r\nI haven\'t looked into `omium` much, but my understanding is it is at least as powerful as `osmfilter`/`osmconvert`. I know we talked about using pbfs directly at some point so that\'s why I mentioned it.', 'comment_created': datetime.datetime(2023, 7, 6, 16, 25, 1, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1254675170, 'comment_body': 'I will update the README to be more of an explanation, and make another issue/PR for a script that handles the build directory, timing, backtraces, saving logs, etc.', 'comment_created': datetime.datetime(2023, 7, 6, 16, 32, 6, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1254681027, 'comment_body': '> I think so, do you mean the wikipedia/wikidata files or the mwm format in general?\r\n\r\nI meant those files that are required for wikiparser to work. It actually may make sense to keep it in README or some other doc, not in an issue.', 'comment_created': datetime.datetime(2023, 7, 6, 16, 38, 34, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}]","[{'commit_sha': '382d351740fd9bc0cefbe4b122d9122190ae5b72', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}]",Evan Lloyd New-Schmidt,10699964,,User,,78,,108,33
1470713130,Improved Simplification,"Additional work to bring HTML simplification in line with the Wikipedia Extracts API.

Closes #4.

Remaining work:
- [x] Extracts element denylist
- [x] Media elements
- [x] Mediawiki attributes
- [x] Span attributes
- [x] Mediawiki `id`s
- [x] Comments
- [x] Whitespace
- [x] Update section removal
- [x] Remove empty sections at end of processing
- [x] Add benchmarks for specific articles
- [x] Add html pretty printer from servo/html5ever#359 for comparison
- [x] Add snapshot tests
- [x] Get size/speed comparison for planet dump
- [x] Clean up/refactor simplification code
- [x] Test random articles",True,26,https://api.github.com/repos/organicmaps/wikiparser/pulls/26,https://github.com/organicmaps/wikiparser/pull/26,closed,1022,61,13,14,9,7,0,0,[],2023-08-10 17:24:29+00:00,2023-08-15 22:37:43+00:00,450794.0,"5 days, 5:13:14","[{'comment_id': 1295142642, 'comment_body': 'Is it hard to test?', 'comment_created': datetime.datetime(2023, 8, 15, 21, 38, 34, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1295144434, 'comment_body': ""What's the behavior with these elements? Are they not stripped? Does it make sense to mention it?"", 'comment_created': datetime.datetime(2023, 8, 15, 21, 41, 5, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1295145789, 'comment_body': 'Until we start using ids, they can also be stripped out to reduce the size. The code that strips them may mention an issue or TODO to remove it when (if) we use ids in TOCs.', 'comment_created': datetime.datetime(2023, 8, 15, 21, 43, 9, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1295146154, 'comment_body': 'Do we have \\n in the final output? Please remove them, if yes, to reduce the size in production.', 'comment_created': datetime.datetime(2023, 8, 15, 21, 43, 43, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1295148051, 'comment_body': ""I just checked that for 5835ae4, [it doesn't](https://docs.rs/scraper/latest/src/scraper/element_ref/mod.rs.html#90) and I've removed the comment."", 'comment_created': datetime.datetime(2023, 8, 15, 21, 46, 29, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1295152948, 'comment_body': 'No, for the snapshot tests they are pretty-printed to make the diffs easier to read.\r\nIn the main wikiparser the newlines between elements are not printed.', 'comment_created': datetime.datetime(2023, 8, 15, 21, 54, 3, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1295154255, 'comment_body': ""That is for the pretty printer, it is designed to work on any HTML.\r\n\r\nThe listed elements are printed on a single line instead of expanded, I'll add a comment."", 'comment_created': datetime.datetime(2023, 8, 15, 21, 55, 52, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}]","[{'commit_sha': 'c2d4ded75a902b0cd737f50259a481412f905cf4', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4b917144a156076b32717c9ed31336e5cef85c19', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b765c07b83d35c69439c82c1d0ee747c6e712783', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f9fac73a3d46f4297eec6d69a2cdb6e9fd60ebd8', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8396c12690263b969c4f90d9bf547cac8e98a184', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '28790990a994bddf433e087dec03e7156dbed6ac', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6e73cc16754d2af407e5335a65df144365b70801', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4c3643576c6f7a19e842c5e492f3ac405efcff15', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '099072a45906e62ea28d390464fe6c9106b5a84a', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'affe164d86ec2235a615ac9fadc71f482e84fa58', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a1430362837576b0774529d667cd1df1f3a30b0f', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4be39acdd30b8427667da491d4e9aecfa0b20535', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8ae2597c5bdebdaaf34506c20331fe57da16f0c4', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '06f3e632765c22fe8a275899f6d9a274a4009bbe', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}]",Evan Lloyd New-Schmidt,10699964,,User,,78,,108,33
1461522598,Add osm tag file parsing,"Parse wikipedia and wikidata tags from a tsv file of OSM tags,
compatible with the `--csv` output of `osmconvert`.

Closes #19.

Notes from parsing all planet wikipedia/wikidata tags:
- 6 UTF-8 TSV parsing errors
- 585 tag parsing errors
    - 511 titles
        - no lang
        - `;`/`း` instead of `:`
        - qid instead of title
    - 74 qids
        - wikipedia title instead of qid
        - `Q123;Q124`
        - `(Q123)`
        - `Warfstermolen (Q1866088)`
        - yandex urls
        - amenity
        - coordinates

There are 50 wikipedia entries with url escaping, some are urls instead of titles and not handled correctly.

Remaining work:
- [x] add url checks in title parsing, match generator url handling (and handle mobile website urls)
- [x] ~~serialize parse errors to disk for changes~~ Add structured parse errors (see #25 for rest), log summary
- [x] Use `osmpbf` crate to parse planet file to fix `osmconvert` truncation problem
- [x] Merge #21 and update `run.sh` to use new method
",True,23,https://api.github.com/repos/organicmaps/wikiparser/pulls/23,https://github.com/organicmaps/wikiparser/pull/23,closed,1231,495,13,6,10,9,0,0,[],2023-08-03 13:57:54+00:00,2023-08-10 13:37:59+00:00,603605.0,"6 days, 23:40:05","[{'comment_id': 1289056307, 'comment_body': 'Why is it not in one line?', 'comment_created': datetime.datetime(2023, 8, 9, 18, 50, 27, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1289057019, 'comment_body': 'A constant to avoid copy-paste?', 'comment_created': datetime.datetime(2023, 8, 9, 18, 50, 50, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1289057395, 'comment_body': 'A constant to avoid copy-paste?', 'comment_created': datetime.datetime(2023, 8, 9, 18, 51, 2, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1289086623, 'comment_body': ""I'm not sure, renaming it to the shorter `Title` must have altered `rustfmt`'s heuristics."", 'comment_created': datetime.datetime(2023, 8, 9, 19, 16, 18, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1289219106, 'comment_body': 'Does it make sense to print wrong hosts in a log to fix/support them?', 'comment_created': datetime.datetime(2023, 8, 9, 21, 29, 57, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1289219300, 'comment_body': 'ditto', 'comment_created': datetime.datetime(2023, 8, 9, 21, 30, 14, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1289222525, 'comment_body': ""What is the benefit of hiding errors under a threshold? Isn't it beneficial to see all errors and be able to estimate/compare the quality of the dump, and to easily grep/find what is most important, or feed the whole log to contributors for fixes?"", 'comment_created': datetime.datetime(2023, 8, 9, 21, 33, 40, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1289270642, 'comment_body': 'They are caught at a higher level and logged/saved with the full string', 'comment_created': datetime.datetime(2023, 8, 9, 22, 22, 4, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1289324486, 'comment_body': ""The threshold only determines if the message is `info` vs `error` level.\r\nWhen you use the `run.sh` script with multiple languages it prints a copy of the hundreds of errors for each language.\r\nI think writing the parse errors to a file separately will be easier to read and deal with.\r\n\r\nI'm open to other ideas."", 'comment_created': datetime.datetime(2023, 8, 9, 22, 40, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}]","[{'commit_sha': 'a2c113a8850ced2c115fe2065ad169956ece49fa', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0ac935c175a161a1e60ac7ebe917d858721df308', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3d48c39793cc39129d37d0e07851c2488160f3c4', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2532d1365e5d50e77e533963df27f6465f6618cc', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '29cdbe2301dce8a615505ba4442208e50127f33d', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b250dd4b1390bf23c673923f4e9a65f8c6d3a32e', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}]",Evan Lloyd New-Schmidt,10699964,,User,,78,,108,33
1433575985,Add script for running with map generator,"Closes #17.

Remaining work:
- [x] document env variable configuration
- [x] test from scratch with new maps build",True,21,https://api.github.com/repos/organicmaps/wikiparser/pulls/21,https://github.com/organicmaps/wikiparser/pull/21,closed,268,10,8,4,0,29,0,0,[],2023-07-13 18:08:59+00:00,2023-08-07 21:05:04+00:00,2170565.0,"25 days, 2:56:05","[{'comment_id': 1269120296, 'comment_body': 'Why is it needed?', 'comment_created': datetime.datetime(2023, 7, 20, 8, 24, 59, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1269124180, 'comment_body': ""```suggestion\r\nset -euxo pipefail\r\n```\r\nChecking pipe failures helps.\r\nIf -x echo doesn't hurt, then it can be always used, for better understanding what magic is going under the hood."", 'comment_created': datetime.datetime(2023, 7, 20, 8, 28, 9, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1269125140, 'comment_body': 'Is it a copy-paste between scripts that can be reused? :)', 'comment_created': datetime.datetime(2023, 7, 20, 8, 28, 58, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1269126947, 'comment_body': 'Can echo be used here?', 'comment_created': datetime.datetime(2023, 7, 20, 8, 30, 31, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1269127777, 'comment_body': 'Why is it better than echo?\r\n\r\nP.S. This and the previous comment are also related to another script.', 'comment_created': datetime.datetime(2023, 7, 20, 8, 31, 11, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1269128541, 'comment_body': 'nit: here and in other places\r\n```suggestion\r\nif [ -z ""${MAPS_DIR+}"" ]; then\r\n```', 'comment_created': datetime.datetime(2023, 7, 20, 8, 31, 49, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1269130556, 'comment_body': 'Why colon is needed? What is the purpose of this line?', 'comment_created': datetime.datetime(2023, 7, 20, 8, 33, 29, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1269133259, 'comment_body': 'ditto: why printf is better than echo?', 'comment_created': datetime.datetime(2023, 7, 20, 8, 35, 47, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1269135640, 'comment_body': 'It would be great to clarify why the latest map build is needed at all.', 'comment_created': datetime.datetime(2023, 7, 20, 8, 37, 49, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1269137260, 'comment_body': 'Am I correctly understanding the issue with the current approach?\r\n1. Generator builds maps and creates csv/txt wiki ids files.\r\n2. Wikiparser runs and generates articles.\r\n3. Generator should be run again to reuse generated articles?..', 'comment_created': datetime.datetime(2023, 7, 20, 8, 39, 5, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1269682509, 'comment_body': ""`pipefail` is unique to bash, I wrote this as a posix sh script. Happy to switch if you'd rather use bash."", 'comment_created': datetime.datetime(2023, 7, 20, 16, 7, 58, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1269683065, 'comment_body': 'Yes, should I put this in a third file and `source` it?', 'comment_created': datetime.datetime(2023, 7, 20, 16, 8, 29, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1269690350, 'comment_body': ""I used printf because echo doesn't handle escaped characters like `\\t` and `\\n` in a portable way, and you can format numbers and other things nicely. "", 'comment_created': datetime.datetime(2023, 7, 20, 16, 15, 16, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1269709561, 'comment_body': 'It sets `MAPS_BUILD_ROOT` to `~/maps_build` if it doesn\'t exist already, the colon is a builtin no-op so that the expansion is evaluated but not used.\r\n\r\nIt could be replaced with:\r\n\r\n```sh\r\nif [ -z ""${MAPS_BUILD_ROOT+}"" ]; then\r\n    MAPS_BUILD_ROOT=""$HOME/maps_build""\r\nfi\r\n```', 'comment_created': datetime.datetime(2023, 7, 20, 16, 34, 12, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1269717912, 'comment_body': 'To replace the workflow with the scraper:\r\n1. The generator needs to be run at least to the [""Features"" stage](https://github.com/organicmaps/organicmaps/blob/a1b596bdc64ed5db3eabf6b1e331411aa2e4ab03/tools/python/maps_generator/generator/stages_declaration.py#L116) in order to generate the wiki files.\r\n2. Running Wikiparser is a replacement for the [""DownloadDescriptions"" stage](https://github.com/organicmaps/organicmaps/blob/a1b596bdc64ed5db3eabf6b1e331411aa2e4ab03/tools/python/maps_generator/generator/stages_declaration.py#L144)\r\n3. The generator can now be run from the [""Descriptions"" stage](https://github.com/organicmaps/organicmaps/blob/a1b596bdc64ed5db3eabf6b1e331411aa2e4ab03/tools/python/maps_generator/generator/stages_declaration.py#L292).\r\n\r\nSo if you\'d like to do it in one run, we could update the generator to call wikiparser.\r\nOr we could tweak the generator to only output the descriptions and continue so you can run wikiparser out-of-band.', 'comment_created': datetime.datetime(2023, 7, 20, 16, 42, 43, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1270255146, 'comment_body': 'Bash is the default shell used in [many companies](https://google.github.io/styleguide/shellguide.html). It is a good practice to write bash scripts and use bash features.\r\n\r\n`#!/usr/bin/env bash`', 'comment_created': datetime.datetime(2023, 7, 21, 5, 59, 7, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1270255767, 'comment_body': ""Yes, there is nothing wrong with that approach to avoid copy-paste. Imagine you'll introduce a third script, or split your current one into parts."", 'comment_created': datetime.datetime(2023, 7, 21, 6, 0, 13, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1270257097, 'comment_body': '1. Do we really expect newlines and tabs in logs? Is it good practice?\r\n2. What kind of number formatting happens in this line?', 'comment_created': datetime.datetime(2023, 7, 21, 6, 2, 25, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1270258151, 'comment_body': '1. More readable if form is preferred to the less known colon.\r\n2. Will `FOO=""${FOO:-default value}""` work here?', 'comment_created': datetime.datetime(2023, 7, 21, 6, 4, 21, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1270262211, 'comment_body': ""Updating the generator to call wikiparser means that generator should wait until wikiparser finishes, right? This approach may be a good temporary start, but we aim to speed up the map generation process as much as possible. That's why the ideal solution would (likely) be to start generator and wikiparser in parallel, as soon as a new osm planet dump is available (or maybe start wikiparser before the generator). So when the generator needs descriptions they will already be available.\r\nThat's why it was important to focus on speedy articles extraction/processing from the start.\r\n\r\n WDYT?"", 'comment_created': datetime.datetime(2023, 7, 21, 6, 11, 30, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1273687353, 'comment_body': ""I've found it useful in the past to embed the git commit in the binary so that when I'm looking through logs I can tell what version was running.\r\nI can remove it if you don't think it's useful."", 'comment_created': datetime.datetime(2023, 7, 25, 15, 2, 51, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1273695439, 'comment_body': 'Yes, `:-` should work', 'comment_created': datetime.datetime(2023, 7, 25, 15, 5, 40, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1273704857, 'comment_body': ""Yes, it replaces the blocking scraper script in it's current form.\r\nI thought it would be better to start with this working and then separate and speed up the process.\r\nTo separate fully from the generator we need to finish #19.\r\n\r\nIn the meantime we could also use the outputs from an old map build and run the wikiparser ahead of time/in parallel."", 'comment_created': datetime.datetime(2023, 7, 25, 15, 11, 37, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1273710234, 'comment_body': ""1. I think newlines and tabs are helpful for separating long content, but I don't think it's a necessity here.\r\n2. On line 25 it wrapper around printf, so any call to log can use printf's formatting string\r\n\r\nIn other places I've heard printf recommended over echo, but if we're using bash explicitly then the portability concerns are not relevant."", 'comment_created': datetime.datetime(2023, 7, 25, 15, 15, 30, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1283251690, 'comment_body': '@biodranik do you want to use gnu `parallel` or something else here instead of a serial for loop?', 'comment_created': datetime.datetime(2023, 8, 3, 14, 0, 36, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1283458810, 'comment_body': ""Maybe ideal workflow would be to start wikiparser in parallel with the map generation and make generator aware of when wikiparser finishes (assuming that it finishes faster than generator requires wiki articles).\n\nIf wikiparser takes a lot of time, then it's better to run it out of band in advance, considering that its data is rarely updated anyway. "", 'comment_created': datetime.datetime(2023, 8, 3, 16, 39, 35, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1283461096, 'comment_body': ""Won't using & and `wait` at the end be enough?"", 'comment_created': datetime.datetime(2023, 8, 3, 16, 41, 40, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1283477000, 'comment_body': ""As long as the machine it's running on has enough cores, fine for the maps server but it will bog down on my laptop. I'll use `&` for now."", 'comment_created': datetime.datetime(2023, 8, 3, 16, 55, 36, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1283516343, 'comment_body': 'You may add an option to use only one core. ', 'comment_created': datetime.datetime(2023, 8, 3, 17, 34, 24, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}]","[{'commit_sha': '8fe572b7a202b7aca05a5171ed7f9d6bfbce8c88', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b924301bfebca4ce72e6ac22643bdd762bbfbfe5', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e495afa7436342ecaf11d8e4d5e38814e1885564', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b722e7d8370414f994b5252b13567e087d972a74', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}]",Evan Lloyd New-Schmidt,10699964,,User,,78,,108,33
1439817564,Download script,"Closes #12 

Remaining work:
- [x] Add handling for error conditions
- [x] Document config and error codes",True,22,https://api.github.com/repos/organicmaps/wikiparser/pulls/22,https://github.com/organicmaps/wikiparser/pull/22,closed,228,1,3,28,7,24,0,0,[],2023-07-18 19:28:30+00:00,2023-09-26 15:45:08+00:00,6034598.0,"69 days, 20:16:38","[{'comment_id': 1268971365, 'comment_body': 'In case no new dumps are available, it should just make sure that the latest ones are already downloaded and exit gracefully (and print that).', 'comment_created': datetime.datetime(2023, 7, 20, 6, 3, 51, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1268971797, 'comment_body': '`set -euxo pipefail` is helpful if decide to use pipes in the script.', 'comment_created': datetime.datetime(2023, 7, 20, 6, 4, 33, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1268972618, 'comment_body': 'nit: fewer lines of code are easier to read.\r\n```suggestion\r\nif [ -z ""${LANGUAGES+}"" ]; then\r\n```', 'comment_created': datetime.datetime(2023, 7, 20, 6, 5, 55, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1268972976, 'comment_body': 'nit: (here and below)\r\n```suggestion\r\nfor lang in $LANGUAGES; do\r\n```', 'comment_created': datetime.datetime(2023, 7, 20, 6, 6, 30, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1268973357, 'comment_body': '""Latest dumps are already downloaded""?', 'comment_created': datetime.datetime(2023, 7, 20, 6, 7, 6, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1268974798, 'comment_body': 'TMPDIR?', 'comment_created': datetime.datetime(2023, 7, 20, 6, 9, 11, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1269108656, 'comment_body': 'get_wiki_dump.sh: line 11: 1: unbound variable', 'comment_created': datetime.datetime(2023, 7, 20, 8, 15, 10, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1296439032, 'comment_body': ""If URLS is empty, then none of the specified languages could be found for the latest dump.\r\n\r\nIf a newer dump isn't available, it will still check the sizes of the last downloaded dump, and exit with 0."", 'comment_created': datetime.datetime(2023, 8, 16, 21, 17, 25, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1296487916, 'comment_body': 'Good! The goal is to make a cron script that will update files automatically when they are published (and delete old files).\r\n\r\nAnother question: should previously generated HTML and other temporary files be deleted before relaunching the wikiparser? Does it make sense to cover it in the run script?', 'comment_created': datetime.datetime(2023, 8, 16, 22, 11, 40, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1296491375, 'comment_body': 'nit: Can array be used here without a warning?', 'comment_created': datetime.datetime(2023, 8, 16, 22, 17, 42, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1296492040, 'comment_body': 'Do you really need to store runs.html on disk and then clean it up?', 'comment_created': datetime.datetime(2023, 8, 16, 22, 18, 52, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1296536200, 'comment_body': ""Good point, I had it like that for POSIX sh because there's no pipefail. With bash it shouldn't be a problem."", 'comment_created': datetime.datetime(2023, 8, 16, 23, 38, 52, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1296548287, 'comment_body': 'To convert it to an array with the same semantics it would need to suppress another warning:\r\n```\r\n# shellcheck disable=SC2206 # Intentionally split on whitespace.\r\nLANGUAGES=( $LANGUAGES )\r\n```', 'comment_created': datetime.datetime(2023, 8, 17, 0, 5, 40, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1297351056, 'comment_body': ""They shouldn't _need_ to be.\r\n\r\nThe temporary files are regenerated each time.\r\nThe generated HTML will be overwritten if it is referenced in the new planet file.\r\n\r\nIf an article isn't extracted from the dump due to #24 or something else, then having the old copy still available might be useful.\r\n\r\nBut if the HTML simplification is changed, and older articles are no longer referenced in OSM, then they will remain on disk unchanged."", 'comment_created': datetime.datetime(2023, 8, 17, 14, 58, 47, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1297355856, 'comment_body': 'Do you want the script to handle this?\r\n\r\nIf it will be running on a cron job, then it might be good to keep 2 copies around.\r\nOtherwise the script could delete the last dump as wikiparser is using it?', 'comment_created': datetime.datetime(2023, 8, 17, 15, 2, 10, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1297853981, 'comment_body': ""1. Aren't files that were open before their deletion on Linux still accessible?\r\n2. Dumps are produced regularly, right? We can set a specific schedule.\r\n3. Script may have an option to automatically delete older dumps."", 'comment_created': datetime.datetime(2023, 8, 17, 23, 46, 19, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1298675137, 'comment_body': ""> 1. Aren't files that were open before their deletion on Linux still accessible?\r\n\r\nYou're right, as long as `run.sh` is started before `download.sh` deletes them, it will be able to access the files.\r\n\r\n> 2. Dumps are produced regularly, right? We can set a specific schedule.\r\n\r\nYes, they're started on the 1st and the 20th of each month, and finished within 3 days it looks like.\r\n\r\n> 3. Script may have an option to automatically delete older dumps.\r\n\r\n:+1: "", 'comment_created': datetime.datetime(2023, 8, 18, 17, 6, 30, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1298740238, 'comment_body': ""I've added a new option:\r\n\r\n```\r\n-D      Delete all old dump subdirectories if the latest is downloaded\r\n```\r\n"", 'comment_created': datetime.datetime(2023, 8, 18, 18, 28, 28, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1300633978, 'comment_body': '`-c 1`, `-c 2` and no option behave in the same way with wget2 installed. ', 'comment_created': datetime.datetime(2023, 8, 21, 20, 53, 18, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1300635199, 'comment_body': 'Will wikiparser generator properly find/load newer versions from the latest dir without specifying explicit file names?', 'comment_created': datetime.datetime(2023, 8, 21, 20, 54, 51, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1300645690, 'comment_body': ""For the `run.sh` script, you'll provide a glob of the latest directory:\r\n```\r\n./run.sh descriptions/ planet.osm.pdf $DUMP_DIR/latest/*\r\n```\r\nIt doesn't have any special handling for the `$DUMP_DIR` layout."", 'comment_created': datetime.datetime(2023, 8, 21, 21, 8, 15, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1300647119, 'comment_body': ""Correct, I'll clarify that."", 'comment_created': datetime.datetime(2023, 8, 21, 21, 9, 41, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}, {'comment_id': 1309390617, 'comment_body': 'Can spaces be added here?', 'comment_created': datetime.datetime(2023, 8, 29, 21, 54, 31, tzinfo=datetime.timezone.utc), 'commenter': 'biodranik', 'type': 'User'}, {'comment_id': 1313217313, 'comment_body': ""I haven't seen an example with spaces in the name. All of the browser user agents use CamelCase instead of spaces."", 'comment_created': datetime.datetime(2023, 9, 1, 16, 2, 39, tzinfo=datetime.timezone.utc), 'commenter': 'newsch', 'type': 'User'}]","[{'commit_sha': '28c17a28eb12a620585b4a3067e93cdb9189c0b1', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7254bc3ec8147325ac7ae724a5456b82f501f411', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3a4d1214dc858aeb32d128c0314e47018c2b004a', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9ee1e8d59456782282d3c358caedba8d8093e4f0', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bae03b91c8012f447dd0c9f01462cb1a22c4cad8', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fe295b2379d45804f5f81eafa2d920eae63ea6f6', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0a1e0592ffa93a1e537fed55e4d4ead1408f5d64', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '27ff9cb4dc952f255848a29d7f6f1a850cc19b71', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '54727b968d700e57fcf2da9016ac1c7d68a5bcbf', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bce44d1ab91aa6b6bef40e7b2a96368dc0bfc33f', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5077ed02f248590db8baf4c7e76de3fe87f208b8', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4c2c6e97ff03f0a17594ff25f52e64145c87f445', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'af80f2ad757da52a1ca2005f4975b306f6e5db77', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '98d5a8a95fc8bb261e9e6af38835a2ad2075c011', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '30b19caeefb96baea61722ae87fdec8721ca6d45', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '187e294d99813a0cf25133d5f475c8c493691b90', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b06167f9d92b83c5862d7d1b19cffca23ef4857a', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '38faebbc541ac362b040d93d59454a6cea3261bd', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '82f2993b2158629f3a71fe0242efbbb95e20acdc', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9d2d2e5f3966a1d9b0bf457fdcf860cd4a685b14', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4d9199235aa98165d8a46a57574aa019c8246f39', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5c8be743024db0f23fbc171496a6786171df3f1a', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e7b5c19426201be556550b3f5b1af23c8b531543', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cf1ac059be5a8d2663f9777891081e2d00511f1f', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4be00cd39206ca75fd1f3b335eca221a7da7772c', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f08fd7d479dfebb334c37628575e59a078ba8d29', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b118724892afa6fee226df74c3b6924dfc27bcfe', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b21a999da7475be96e55e64a52624d8f6c044cc2', 'committer_username': 'newsch', 'committer_name': 'Evan Lloyd New-Schmidt', 'committer_email': None, 'commit_date': datetime.datetime(2015, 1, 25, 21, 35, 25, tzinfo=datetime.timezone.utc)}]",Evan Lloyd New-Schmidt,10699964,,User,,78,,108,33

Project_ID,Name,Full_name,Language,Forks,Stars,Watchers,contributors,commits,issues,branches,PRs_count,contributor pullrequests
647310843,wikiparser,organicmaps/wikiparser,HTML,3,11,4,5,76,13,1,1,"[{'id': 1958473405, 'number': 48, 'closed': datetime.datetime(2024, 7, 8, 21, 13, 55, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2024, 7, 8, 19, 8, 38, tzinfo=datetime.timezone.utc), 'time_taken': 7517.0, 'time_delta': '2:05:17', 'additions': 38, 'deletions': 317, 'state': 'closed'}, {'id': 1916892016, 'number': 46, 'closed': datetime.datetime(2024, 6, 12, 19, 57, 13, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2024, 6, 12, 15, 48, 4, tzinfo=datetime.timezone.utc), 'time_taken': 14949.0, 'time_delta': '4:09:09', 'additions': 44, 'deletions': 0, 'state': 'closed'}, {'id': 1844545661, 'number': 45, 'closed': datetime.datetime(2024, 4, 30, 16, 17, 59, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2024, 4, 28, 18, 27, 42, tzinfo=datetime.timezone.utc), 'time_taken': 165017.0, 'time_delta': '1 day, 21:50:17', 'additions': 91, 'deletions': 83, 'state': 'closed'}, {'id': 1833389947, 'number': 44, 'closed': datetime.datetime(2024, 4, 24, 14, 34, 11, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2024, 4, 21, 16, 42, 58, tzinfo=datetime.timezone.utc), 'time_taken': 251473.0, 'time_delta': '2 days, 21:51:13', 'additions': 57, 'deletions': 8, 'state': 'closed'}, {'id': 1694416831, 'number': 39, 'closed': datetime.datetime(2024, 4, 28, 18, 20, 52, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2024, 1, 24, 20, 43, 7, tzinfo=datetime.timezone.utc), 'time_taken': 8199465.0, 'time_delta': '94 days, 21:37:45', 'additions': 245, 'deletions': 115, 'state': 'closed'}, {'id': 1694408939, 'number': 38, 'closed': datetime.datetime(2024, 4, 28, 1, 12, 23, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2024, 1, 24, 20, 39, 11, tzinfo=datetime.timezone.utc), 'time_taken': 8137992.0, 'time_delta': '94 days, 4:33:12', 'additions': 71, 'deletions': 26, 'state': 'closed'}, {'id': 1540339716, 'number': 35, 'closed': datetime.datetime(2023, 10, 4, 16, 20, 40, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 10, 3, 16, 34, 32, tzinfo=datetime.timezone.utc), 'time_taken': 85568.0, 'time_delta': '23:46:08', 'additions': 36, 'deletions': 8, 'state': 'closed'}, {'id': 1538216773, 'number': 34, 'closed': datetime.datetime(2023, 10, 2, 15, 56, 20, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 10, 2, 15, 26, 41, tzinfo=datetime.timezone.utc), 'time_taken': 1779.0, 'time_delta': '0:29:39', 'additions': 9, 'deletions': 0, 'state': 'closed'}, {'id': 1531180124, 'number': 33, 'closed': datetime.datetime(2024, 1, 24, 20, 45, 24, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 9, 26, 19, 13, 31, tzinfo=datetime.timezone.utc), 'time_taken': 10373513.0, 'time_delta': '120 days, 1:31:53', 'additions': 392, 'deletions': 120, 'state': 'closed'}, {'id': 1531054930, 'number': 32, 'closed': datetime.datetime(2023, 9, 29, 20, 12, 37, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 9, 26, 17, 47, 45, tzinfo=datetime.timezone.utc), 'time_taken': 267892.0, 'time_delta': '3 days, 2:24:52', 'additions': 11, 'deletions': 4, 'state': 'closed'}, {'id': 1530809770, 'number': 31, 'closed': datetime.datetime(2023, 9, 26, 15, 13, 29, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 9, 26, 15, 9, 35, tzinfo=datetime.timezone.utc), 'time_taken': 234.0, 'time_delta': '0:03:54', 'additions': 13, 'deletions': 5, 'state': 'closed'}, {'id': 1500527379, 'number': 30, 'closed': datetime.datetime(2023, 9, 26, 19, 8, 34, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 9, 4, 2, 37, 6, tzinfo=datetime.timezone.utc), 'time_taken': 1960288.0, 'time_delta': '22 days, 16:31:28', 'additions': 80, 'deletions': 10, 'state': 'closed'}, {'id': 1490272631, 'number': 29, 'closed': datetime.datetime(2023, 9, 1, 16, 3, 58, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 8, 25, 20, 30, 27, tzinfo=datetime.timezone.utc), 'time_taken': 588811.0, 'time_delta': '6 days, 19:33:31', 'additions': 53, 'deletions': 8, 'state': 'closed'}, {'id': 1490265127, 'number': 28, 'closed': datetime.datetime(2023, 9, 29, 20, 11, 29, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 8, 25, 20, 21, 45, tzinfo=datetime.timezone.utc), 'time_taken': 3023384.0, 'time_delta': '34 days, 23:49:44', 'additions': 454, 'deletions': 201, 'state': 'closed'}, {'id': 1470713130, 'number': 26, 'closed': datetime.datetime(2023, 8, 15, 22, 37, 43, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 8, 10, 17, 24, 29, tzinfo=datetime.timezone.utc), 'time_taken': 450794.0, 'time_delta': '5 days, 5:13:14', 'additions': 1022, 'deletions': 61, 'state': 'closed'}, {'id': 1461522598, 'number': 23, 'closed': datetime.datetime(2023, 8, 10, 13, 37, 59, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 8, 3, 13, 57, 54, tzinfo=datetime.timezone.utc), 'time_taken': 603605.0, 'time_delta': '6 days, 23:40:05', 'additions': 1231, 'deletions': 495, 'state': 'closed'}, {'id': 1439817564, 'number': 22, 'closed': datetime.datetime(2023, 9, 26, 15, 45, 8, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 18, 19, 28, 30, tzinfo=datetime.timezone.utc), 'time_taken': 6034598.0, 'time_delta': '69 days, 20:16:38', 'additions': 228, 'deletions': 1, 'state': 'closed'}, {'id': 1433575985, 'number': 21, 'closed': datetime.datetime(2023, 8, 7, 21, 5, 4, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 13, 18, 8, 59, tzinfo=datetime.timezone.utc), 'time_taken': 2170565.0, 'time_delta': '25 days, 2:56:05', 'additions': 268, 'deletions': 10, 'state': 'closed'}, {'id': 1429795758, 'number': 20, 'closed': datetime.datetime(2023, 7, 13, 18, 4, 52, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 11, 16, 23, 50, tzinfo=datetime.timezone.utc), 'time_taken': 178862.0, 'time_delta': '2 days, 1:41:02', 'additions': 133, 'deletions': 34, 'state': 'closed'}, {'id': 1413875025, 'number': 10, 'closed': datetime.datetime(2023, 7, 10, 14, 58, 56, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 6, 29, 20, 2, 31, tzinfo=datetime.timezone.utc), 'time_taken': 932185.0, 'time_delta': '10 days, 18:56:25', 'additions': 103, 'deletions': 12, 'state': 'closed'}, {'id': 1405729219, 'number': 6, 'closed': datetime.datetime(2023, 7, 10, 14, 34, 21, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 6, 23, 22, 0, 14, tzinfo=datetime.timezone.utc), 'time_taken': 1442047.0, 'time_delta': '16 days, 16:34:07', 'additions': 233, 'deletions': 95, 'state': 'closed'}, {'id': 1374697557, 'number': 3, 'closed': datetime.datetime(2023, 6, 23, 19, 50, 5, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 6, 1, 20, 0, 39, tzinfo=datetime.timezone.utc), 'time_taken': 1900166.0, 'time_delta': '21 days, 23:49:26', 'additions': 1685, 'deletions': 66, 'state': 'closed'}, {'id': 1371137070, 'number': 2, 'closed': datetime.datetime(2023, 6, 1, 7, 25, 35, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 5, 30, 21, 7, tzinfo=datetime.timezone.utc), 'time_taken': 123515.0, 'time_delta': '1 day, 10:18:35', 'additions': 38, 'deletions': 4, 'state': 'closed'}, {'id': 1370762766, 'number': 1, 'closed': datetime.datetime(2023, 5, 30, 17, 0, 5, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 5, 30, 16, 29, 4, tzinfo=datetime.timezone.utc), 'time_taken': 1861.0, 'time_delta': '0:31:01', 'additions': 257, 'deletions': 0, 'state': 'closed'}]"
