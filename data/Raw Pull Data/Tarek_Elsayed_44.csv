pr_id,pr_title,pr_body,is_merged,pr_number,pr_url,pr_html_url,pr_state,additions,deletions,pr_changed_files,pr_commits_count,pr_comments_count,pr_review_comments_count,pr_labels_count,pr_assignees_count,pr_labels,pr_created_at,pr_closed_at,time_taken,time_delta,pr_review_comments,pr_commits,contributor,contributor_id,contributor_email,contributor_type,contributions,contributor_public_repos,contributor_private_repos,contributor_followings,contributor_followers
1386308542,Reinforcement Learning: Deep Deterministic Policy Gradient ,"# Description

This pull request implements the DDPG (Deep Deterministic Policy Gradient) algorithm, along with 2 test cases. 

## Implementation details
DDPG is an actor-critic algorithm designed for continuous action spaces. It combines deep neural networks with deterministic policy gradients to learn optimal policies in a continuous control setting.

Implemented four networks:
- `policyNetwork` (actor network)
- `targetPNetwork` (target actor network)
- `learningQNetwork` (critic network)
- `targetQNetwork` (target critic network)

# How Has This Been Tested?

- [x] Included a Pendulum test that successfully passes on my machine. The average reward achieved in the Pendulum environment is approximately -500, indicating successful learning.
- [x] Additionally, added a test for continuous action spaces, which also passes.

The test configurations for DDPG are adapted from the SAC (Soft Actor-Critic) implementation since both DDPG and SAC are policy gradient off-policy algorithms. This ensures consistent evaluation and comparison of the algorithms.
",True,3494,https://api.github.com/repos/mlpack/mlpack/pulls/3494,https://github.com/mlpack/mlpack/pull/3494,closed,622,1,5,6,0,6,2,0,"[{'name': 'c: methods'}, {'name': 't: added feature'}]",2023-06-09 14:36:42+00:00,2023-06-20 12:04:11+00:00,941249.0,"10 days, 21:27:29","[{'comment_id': 1225958849, 'comment_body': '```suggestion\r\n  * Reinforcement Learning: Deep Deterministic Policy Gradient (#3494).\r\n```\r\n\r\nRemove extra space.', 'comment_created': datetime.datetime(2023, 6, 11, 23, 42, 23, tzinfo=datetime.timezone.utc), 'commenter': 'zoq', 'type': 'User'}, {'comment_id': 1225985124, 'comment_body': 'Done!\r\nI also rebased on top of master.', 'comment_created': datetime.datetime(2023, 6, 12, 1, 7, 26, tzinfo=datetime.timezone.utc), 'commenter': 'tareknaser', 'type': 'User'}, {'comment_id': 1227535169, 'comment_body': 'tparam not matching', 'comment_created': datetime.datetime(2023, 6, 13, 5, 5, 54, tzinfo=datetime.timezone.utc), 'commenter': 'shubham1206agra', 'type': 'User'}, {'comment_id': 1228100417, 'comment_body': 'Thanks for pointing this out.', 'comment_created': datetime.datetime(2023, 6, 13, 13, 5, 24, tzinfo=datetime.timezone.utc), 'commenter': 'tareknaser', 'type': 'User'}, {'comment_id': 1228901213, 'comment_body': 'Do you think, we can reduce the size of the second linear layer, to make the test faster?', 'comment_created': datetime.datetime(2023, 6, 14, 2, 33, 11, tzinfo=datetime.timezone.utc), 'commenter': 'zoq', 'type': 'User'}, {'comment_id': 1229676167, 'comment_body': 'We can take out the second layer for the tests.\r\nThe networks used in the original PR are taken from the paper but we can always add an example later in the `examples` repository to demonstrate that.\r\n\r\nFor now, the modified networks converge as well. The average return for the Pendulum task is -970. ', 'comment_created': datetime.datetime(2023, 6, 14, 14, 3, 42, tzinfo=datetime.timezone.utc), 'commenter': 'tareknaser', 'type': 'User'}]","[{'commit_sha': '884134afc853ffac63cc433755cc967887f9fcef', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bd25f579e54d186c271646823e09ef6962a63900', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': '07e56a73e250d8e2e4e009a823949dfe7a07ef2e', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2187ba291e088f49576cb393685697eda3f2cd16', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': '465f0216144e468754d42702a167dc7e81cc88cd', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': '355f667479d69e3c373ff489bab8cfd4e8b2470e', 'committer_username': 'shubham1206agra', 'committer_name': 'Shubham Agrawal', 'committer_email': None, 'commit_date': datetime.datetime(2019, 12, 2, 5, 23, 7, tzinfo=datetime.timezone.utc)}]",Tarek Elsayed,60650661,,User,,31,,3,8
1400087346,Reinforcement Learning: Ornstein-Uhlenbeck noise,"# Description

This pull request implements the Ornstein-Uhlenbeck noise class, along with a unit test. 

## Implementation details
The Ornstein-Uhlenbeck process is a process that generates temporally correlated noise via a random walk with damping, which is commonly used in reinforcement learning algorithms.

- I added a new file for convenience to include new noise classes in the future. 
- The `OUNoise` class provides a `reset()` function that sets the internal state of the noise process to the specified mean (mu).
- It offers a `sample()` function to update the internal state based on the mean reversion rate (theta) and standard deviation (sigma), and returns the current state as a noise sample.

# How Has This Been Tested?

- [x] The `OUNoiseTest` verifies the functionality of the `OUNoise` class by testing the `reset()` function and the generation of noise samples, ensuring that the sampled state has the expected size and is not equal to the reset state.",True,3499,https://api.github.com/repos/mlpack/mlpack/pulls/3499,https://github.com/mlpack/mlpack/pull/3499,closed,171,7,7,3,0,0,2,0,"[{'name': 'c: methods'}, {'name': 't: added feature'}]",2023-06-20 15:28:16+00:00,2023-07-01 15:55:07+00:00,952011.0,"11 days, 0:26:51",[],"[{'commit_sha': '7bd26a8ba20ad53a0bafe20b5603a8c18e1df133', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd8b7431bde34395de6cdf864819a755029361e7a', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4646ec122d0be6ea231fd3d6ea417018eadcbf09', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}]",Tarek Elsayed,60650661,,User,,31,,3,8
1424906027,Reinforcement Learning: Twin Delayed Deep Deterministic Policy Gradient,"# Description

This pull request implements the TD3 (Twin Delayed Deep Deterministic Policy Gradient) algorithm, along with 2 test cases. 

## Implementation details
TD3 (Twin Delayed Deep Deterministic Policy Gradient) is a reinforcement learning algorithm designed for continuous action spaces. It builds upon DDPG and introduces twin critics and delayed updates to improve stability and performance. 

Implemented 6 networks:
- `policyNetwork` (actor network)
- `targetPNetwork` (target actor network)
- `learningQ1Network` (first critic network)
- `targetQ1Network` (first target critic network)
- `learningQ2Network` (second critic network)
- `targetQ2Network` (second target critic network)

# How Has This Been Tested?

- [x] Included a Pendulum test that successfully passes with different configuration values. 
    - With TargetNetworkSyncInterval = 1 ——> -1081.52
    - With TargetNetworkSyncInterval = 2 ——> -508.788
    - With TargetNetworkSyncInterval = 3 ——> -1209.31
- [x] Additionally, added a test for continuous action spaces, which also passes.

The networks for the 2 tests are the same for DDPG and SAC for comparison.",True,3512,https://api.github.com/repos/mlpack/mlpack/pulls/3512,https://github.com/mlpack/mlpack/pull/3512,closed,647,1,5,2,1,0,1,0,[{'name': 'c: methods'}],2023-07-07 16:10:58+00:00,2023-07-21 14:31:30+00:00,1203632.0,"13 days, 22:20:32",[],"[{'commit_sha': 'f2d8f51f62d5b9ad42f4b8c8106bb0dacbc6ee81', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9feb73e1674960ee7337de56cd30a7f5474df824', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}]",Tarek Elsayed,60650661,,User,,31,,3,8
1448829426,Reinforcement Learning: Gaussian noise,"This pull request introduces the `GaussianNoise` class and reorganizes the reinforcement learning test suite.

# Implementation Details:

- `GaussianNoise` Class:
    - `GaussianNoise` class provides the necessary noise object required for the DDPG algorithm.
    - The flexibility of the `GaussianNoise` class allows for potential usage in other RL algorithms, where a noise object might be necessary for training and convergence.
- Reorganization of Tests:
    - The reinforcement learning test suite has been split into two files for improved code organization.
    - The new `policy_gradient_test.cpp` file includes the tests for DDPG, TD3, and SAC. 

# How Has This Been Tested?:
- The DDPG algorithm is tested with the integration of the `GaussianNoise` class for exploration.
- Another test to verify that the returned noise sample has the right mean and standard deviation.
",True,3515,https://api.github.com/repos/mlpack/mlpack/pulls/3515,https://github.com/mlpack/mlpack/pull/3515,closed,546,349,7,6,1,1,2,0,"[{'name': 'c: methods'}, {'name': 't: added feature'}]",2023-07-25 17:14:43+00:00,2023-08-01 20:38:23+00:00,617020.0,"7 days, 3:23:40","[{'comment_id': 1277058429, 'comment_body': 'Do you mind to align this with the `size` parameter, also, should we use `const` for every parameter?', 'comment_created': datetime.datetime(2023, 7, 28, 3, 40, 45, tzinfo=datetime.timezone.utc), 'commenter': 'zoq', 'type': 'User'}]","[{'commit_sha': '431feaf1cd65a607c6bb9368dee6c6ce4470a4a9', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': '12e276ae8f6fd017bfd04ed515af8aba176a4841', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2ae3c08246a7fc0047b6f6d5c4560258442b529c', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': '19a8700c2323e3e38006f39d4ad852741946fa33', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd1cb16943a53b873ccce3cfb7e6e49f134856d22', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'be237b6a8503829918c7a8c6d159f3ce4e844105', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}]",Tarek Elsayed,60650661,,User,,31,,3,8
1464419703,Reinforcement Learning: Introducing Agent-Specific Tutorials,"## Description:
This pull request separates agent documentation into individual files and adds tutorials for DDPG, TD3, and SAC algorithms.

## **Changes Made:**
- Moved agent documentation from the main reinforcement learning doc file to separate files for each agent.
- Added new documentation files for each agent:
    - `doc/tutorials/reinforcement_learning/ddpg.md` for Deep Deterministic Policy Gradient (DDPG).
    - `doc/tutorials/reinforcement_learning/td3.md` for Twin Delayed Deep Deterministic Policy Gradient (TD3).
    - `doc/tutorials/reinforcement_learning/sac.md` for Soft Actor-Critic (SAC).",True,3520,https://api.github.com/repos/mlpack/mlpack/pulls/3520,https://github.com/mlpack/mlpack/pull/3520,closed,790,277,6,4,1,0,2,0,"[{'name': 'c: documentation'}, {'name': 't: added feature'}]",2023-08-07 01:32:46+00:00,2023-09-01 14:27:19+00:00,2206473.0,"25 days, 12:54:33",[],"[{'commit_sha': 'cc334b0e0ce2d109073870c8be6f785d51d70cf9', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bf91c4e59592e73b846d2878b957b483d1469767', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b2232019b6c6d0b7eb71202db6d34539681220f4', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a89180fa29f1425e36bdd09b321dd9c3435e104b', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}]",Tarek Elsayed,60650661,,User,,31,,3,8
1475016739,Reinforcement Learning: Examples for `DDPG` and `TD3` with Gymnasium Environments,"## Description:
This PR updates the reinforcement learning examples with the latest Gymnasium Python server. Two significant examples have been added.
### `DDPG` on `MountainCarContinuous-v0`:
- The agent was trained for a total of 100,000 steps.
### `TD3` on `Pendulum-v1`:
- The training was performed for 50,000 steps.

> Note: Both examples include locally saved videos that will be utilized in my final GSoC report.",True,214,https://api.github.com/repos/mlpack/examples/pulls/214,https://github.com/mlpack/examples/pull/214,closed,462,0,4,3,2,3,1,0,[{'name': 'c: examples'}],2023-08-14 19:56:56+00:00,2023-09-04 03:45:28+00:00,1756112.0,"20 days, 7:48:32","[{'comment_id': 1306875031, 'comment_body': 'I think, we should remove the server, and make the gym tcp a dependency. ', 'comment_created': datetime.datetime(2023, 8, 28, 3, 37, 56, tzinfo=datetime.timezone.utc), 'commenter': 'zoq', 'type': 'User'}, {'comment_id': 1306875224, 'comment_body': 'See the comment above.', 'comment_created': datetime.datetime(2023, 8, 28, 3, 38, 26, tzinfo=datetime.timezone.utc), 'commenter': 'zoq', 'type': 'User'}, {'comment_id': 1307737307, 'comment_body': 'Yes I think this one slipped\r\nI was using it from the same folder for convenience and pushed it ', 'comment_created': datetime.datetime(2023, 8, 28, 17, 55, 43, tzinfo=datetime.timezone.utc), 'commenter': 'tareknaser', 'type': 'User'}]","[{'commit_sha': '16f3d523f2e5aae17f57cb49a8b27072c9a049ff', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c69eb1e2ed4d649840ae30bc46e322e247f4de16', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dc6f7d8482844cbee44719e95c6ee01886c8ce16', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}]",Tarek Elsayed,60650661,,User,,31,,3,8
1557120831,Reinforcement Learning: `TD3` with `HalfCheetah-v3` Gymnasium Environment,"## Description:
This PR adds an example to the reinforcement learning examples set 
### `TD3` on `HalfCheetah-v3`:
- The training was performed for 150,000 steps.

> The video for the result can be found in my [GSoC final report](https://github.com/tareknaser/GSoC23-mlpack-RL-Report)",False,216,https://api.github.com/repos/mlpack/examples/pulls/216,https://github.com/mlpack/examples/pull/216,closed,230,0,2,1,4,0,3,0,"[{'name': 's: stale'}, {'name': 'c: examples'}, {'name': 'notebook'}]",2023-10-14 19:54:02+00:00,2023-11-25 13:18:53+00:00,3605091.0,"41 days, 17:24:51",[],"[{'commit_sha': '0d33a4ab7547c1c18b27d0ec30ea4bdf823173bc', 'committer_username': 'tareknaser', 'committer_name': 'Tarek Elsayed', 'committer_email': None, 'commit_date': datetime.datetime(2020, 2, 4, 10, 45, 56, tzinfo=datetime.timezone.utc)}]",Tarek Elsayed,60650661,,User,,31,,3,8

Project_ID,Name,Full_name,Language,Forks,Stars,Watchers,contributors,commits,issues,branches,PRs_count,contributor pullrequests
28149409,mlpack,mlpack/mlpack,C++,1589,4976,183,321,30199,24,11,15,"[{'id': 1464419703, 'number': 3520, 'closed': datetime.datetime(2023, 9, 1, 14, 27, 19, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 8, 7, 1, 32, 46, tzinfo=datetime.timezone.utc), 'time_taken': 2206473.0, 'time_delta': '25 days, 12:54:33', 'additions': 790, 'deletions': 277, 'state': 'closed'}, {'id': 1448829426, 'number': 3515, 'closed': datetime.datetime(2023, 8, 1, 20, 38, 23, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 25, 17, 14, 43, tzinfo=datetime.timezone.utc), 'time_taken': 617020.0, 'time_delta': '7 days, 3:23:40', 'additions': 546, 'deletions': 349, 'state': 'closed'}, {'id': 1424906027, 'number': 3512, 'closed': datetime.datetime(2023, 7, 21, 14, 31, 30, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 7, 16, 10, 58, tzinfo=datetime.timezone.utc), 'time_taken': 1203632.0, 'time_delta': '13 days, 22:20:32', 'additions': 647, 'deletions': 1, 'state': 'closed'}, {'id': 1400087346, 'number': 3499, 'closed': datetime.datetime(2023, 7, 1, 15, 55, 7, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 6, 20, 15, 28, 16, tzinfo=datetime.timezone.utc), 'time_taken': 952011.0, 'time_delta': '11 days, 0:26:51', 'additions': 171, 'deletions': 7, 'state': 'closed'}, {'id': 1386308542, 'number': 3494, 'closed': datetime.datetime(2023, 6, 20, 12, 4, 11, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 6, 9, 14, 36, 42, tzinfo=datetime.timezone.utc), 'time_taken': 941249.0, 'time_delta': '10 days, 21:27:29', 'additions': 622, 'deletions': 1, 'state': 'closed'}, {'id': 1116694782, 'number': 3307, 'closed': datetime.datetime(2023, 3, 16, 0, 56, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 11, 9, 21, 31, 59, tzinfo=datetime.timezone.utc), 'time_taken': 10898641.0, 'time_delta': '126 days, 3:24:01', 'additions': 972, 'deletions': 789, 'state': 'closed'}, {'id': 1033608681, 'number': 3260, 'closed': datetime.datetime(2022, 12, 16, 11, 53, 31, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 23, 3, 56, 38, tzinfo=datetime.timezone.utc), 'time_taken': 9964613.0, 'time_delta': '115 days, 7:56:53', 'additions': 50, 'deletions': 35, 'state': 'closed'}, {'id': 1033279932, 'number': 3259, 'closed': datetime.datetime(2022, 8, 23, 3, 56, 54, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 22, 19, 58, 18, tzinfo=datetime.timezone.utc), 'time_taken': 28716.0, 'time_delta': '7:58:36', 'additions': 13, 'deletions': 18, 'state': 'closed'}, {'id': 1031383847, 'number': 3258, 'closed': datetime.datetime(2022, 8, 23, 3, 57, 4, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 19, 16, 59, 22, tzinfo=datetime.timezone.utc), 'time_taken': 298662.0, 'time_delta': '3 days, 10:57:42', 'additions': 8, 'deletions': 13, 'state': 'closed'}]"
92665356,examples,mlpack/examples,Jupyter Notebook,89,116,24,37,1035,5,1,2,"[{'id': 1557120831, 'number': 216, 'closed': datetime.datetime(2023, 11, 25, 13, 18, 53, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 10, 14, 19, 54, 2, tzinfo=datetime.timezone.utc), 'time_taken': 3605091.0, 'time_delta': '41 days, 17:24:51', 'additions': 230, 'deletions': 0, 'state': 'closed'}, {'id': 1475016739, 'number': 214, 'closed': datetime.datetime(2023, 9, 4, 3, 45, 28, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 8, 14, 19, 56, 56, tzinfo=datetime.timezone.utc), 'time_taken': 1756112.0, 'time_delta': '20 days, 7:48:32', 'additions': 462, 'deletions': 0, 'state': 'closed'}, {'id': 1028023008, 'number': 205, 'closed': datetime.datetime(2022, 8, 22, 11, 29, 34, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 16, 20, 1, 51, tzinfo=datetime.timezone.utc), 'time_taken': 487663.0, 'time_delta': '5 days, 15:27:43', 'additions': 3788, 'deletions': 0, 'state': 'closed'}, {'id': 1019524820, 'number': 202, 'closed': datetime.datetime(2022, 8, 16, 14, 38, 12, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 7, 11, 14, 5, tzinfo=datetime.timezone.utc), 'time_taken': 789847.0, 'time_delta': '9 days, 3:24:07', 'additions': 3129, 'deletions': 0, 'state': 'closed'}, {'id': 1008934080, 'number': 201, 'closed': datetime.datetime(2022, 7, 28, 13, 2, 38, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 7, 26, 20, 26, 33, tzinfo=datetime.timezone.utc), 'time_taken': 146165.0, 'time_delta': '1 day, 16:36:05', 'additions': 1, 'deletions': 0, 'state': 'closed'}, {'id': 1006540834, 'number': 200, 'closed': datetime.datetime(2022, 7, 28, 17, 9, 36, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 7, 24, 13, 1, 49, tzinfo=datetime.timezone.utc), 'time_taken': 360467.0, 'time_delta': '4 days, 4:07:47', 'additions': 2490, 'deletions': 0, 'state': 'closed'}, {'id': 887383974, 'number': 196, 'closed': datetime.datetime(2022, 4, 2, 19, 57, 14, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 3, 23, 15, 48, 13, tzinfo=datetime.timezone.utc), 'time_taken': 878941.0, 'time_delta': '10 days, 4:09:01', 'additions': 1462, 'deletions': 0, 'state': 'closed'}, {'id': 885279392, 'number': 195, 'closed': datetime.datetime(2022, 3, 23, 15, 42, 37, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 3, 21, 20, 22, 24, tzinfo=datetime.timezone.utc), 'time_taken': 156013.0, 'time_delta': '1 day, 19:20:13', 'additions': 1525, 'deletions': 0, 'state': 'closed'}]"
