pr_id,pr_title,pr_body,is_merged,pr_number,pr_url,pr_html_url,pr_state,additions,deletions,pr_changed_files,pr_commits_count,pr_comments_count,pr_review_comments_count,pr_labels_count,pr_assignees_count,pr_labels,pr_created_at,pr_closed_at,time_taken,time_delta,pr_review_comments,pr_commits,contributor,contributor_id,contributor_email,contributor_type,contributions,contributor_public_repos,contributor_private_repos,contributor_followings,contributor_followers
911208416,Add ROUGE Metric,"Resolves https://github.com/keras-team/keras-nlp/issues/67

@mattdangerw, @chenmoneygithub, this PR is now ready for review :)",True,122,https://api.github.com/repos/keras-team/keras-nlp/pulls/122,https://github.com/keras-team/keras-nlp/pull/122,closed,1000,0,7,31,14,52,0,0,[],2022-04-16 05:47:29+00:00,2022-06-17 17:04:14+00:00,5397405.0,"62 days, 11:16:45","[{'comment_id': 853459495, 'comment_body': 'Prefer use a well-defined example rather than random data so that users can manually calculate the F1 score.', 'comment_created': datetime.datetime(2022, 4, 19, 20, 28, 5, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 853460110, 'comment_body': 'In docstring it defaults to float32, which mismatches the default value `None`. Please fix it.', 'comment_created': datetime.datetime(2022, 4, 19, 20, 28, 56, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 853466742, 'comment_body': 'rename `num_samples` to `batch_size`, we should be consistent with the naming.', 'comment_created': datetime.datetime(2022, 4, 19, 20, 38, 23, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 853472945, 'comment_body': 'We do not actually need to make this a standalone util. Just write \r\n```\r\nf1_scores, precisions, recalls = tf_text.metrics.rouge_l(\r\n        y_pred, y_true, alpha=alpha\r\n    )\r\n```\r\nin the RougeL class.', 'comment_created': datetime.datetime(2022, 4, 19, 20, 46, 34, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 853473962, 'comment_body': 'This is a bit verbose, we can rename to `test_initialization()`.', 'comment_created': datetime.datetime(2022, 4, 19, 20, 47, 51, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 854094040, 'comment_body': ""I was following the format of metrics given in Keras. For example, check this out: https://github.com/keras-team/keras/blob/master/keras/metrics/metrics.py#L220, https://github.com/keras-team/keras/blob/master/keras/metrics/metrics.py#L3331.\r\n\r\nThe class helps in aggregating the ROUGE score (the user can iterate over the dataset, and the class will return the avg. ROUGE score).\r\n\r\nThe function allows string inputs, I think. \r\n\r\nI've changed it to this for the time being: \r\n```\r\nf1_scores, precisions, recalls = tf_text.metrics.rouge_l(\r\n        y_pred, y_true, alpha=alpha\r\n    )\r\n```\r\nbut let me know which one is more appropriate and I'll change it to that. Thanks!"", 'comment_created': datetime.datetime(2022, 4, 20, 12, 49, 29, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 854094321, 'comment_body': 'My bad. Changed!', 'comment_created': datetime.datetime(2022, 4, 20, 12, 49, 45, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 854097792, 'comment_body': 'Done!', 'comment_created': datetime.datetime(2022, 4, 20, 12, 53, 30, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 854375709, 'comment_body': ""Thanks! My guess is there was some need to directly call the metrics computing function, which I am not sure if still applies. Let's keep it simple for now, and we can add the util if we find it is necessary. "", 'comment_created': datetime.datetime(2022, 4, 20, 17, 16, 14, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 854380659, 'comment_body': 'Great! Let me know if further changes are required', 'comment_created': datetime.datetime(2022, 4, 20, 17, 22, 22, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 882337680, 'comment_body': 'space after period', 'comment_created': datetime.datetime(2022, 5, 26, 5, 50, 50, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 882337840, 'comment_body': 'rouge_score -> rouge-score', 'comment_created': datetime.datetime(2022, 5, 26, 5, 51, 13, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 882341574, 'comment_body': 'a keras metric can just return a dict of scalars I believe, should we just return a dict here and from the metric overall?', 'comment_created': datetime.datetime(2022, 5, 26, 5, 58, 58, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 882341920, 'comment_body': 'Why is this variable called rouge_l when it could be rouge_l or rouge_n? Seems like a confusing name.', 'comment_created': datetime.datetime(2022, 5, 26, 5, 59, 42, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 882356973, 'comment_body': 'This feels too tricky, particularly with the order of RougeN being a hidden parameter of the string passed to this argument.\r\n\r\nWhat about making a separate RougeL and RougeN metric class?', 'comment_created': datetime.datetime(2022, 5, 26, 6, 28, 8, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 884067836, 'comment_body': 'Yeah, this is a typo. Corrected in the latest commit!', 'comment_created': datetime.datetime(2022, 5, 28, 4, 16, 50, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 884067972, 'comment_body': 'Discussed offline with Matt. We are going ahead with separate classes for ROUGE-N and ROUGE-L!', 'comment_created': datetime.datetime(2022, 5, 28, 4, 18, 24, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 884104863, 'comment_body': 'Ah, I was not aware of this. Will return a dictionary', 'comment_created': datetime.datetime(2022, 5, 28, 8, 58, 7, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 884111103, 'comment_body': '@mattdangerw, looks like this doesn\'t work; an error is thrown when I return a dictionary from `result()`. Have a look at this snippet of code:\r\n\r\n```\r\n>>> import keras_nlp\r\n>>> y_true = ""hey, this is great fun""\r\n>>> y_pred = ""great fun indeed""\r\n>>> rouge = keras_nlp.metrics.RougeN(order=2)\r\n>>> rouge(y_true, y_pred)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/abheesht/python_envs/keras_nlp/lib/python3.8/site-packages/keras/metrics/base_metric.py"", line 200, in __call__\r\n    return distributed_training_utils.call_replica_local_fn(\r\n  File ""/home/abheesht/python_envs/keras_nlp/lib/python3.8/site-packages/keras/distribute/distributed_training_utils.py"", line 60, in call_replica_local_fn\r\n    return fn(*args, **kwargs)\r\n  File ""/home/abheesht/python_envs/keras_nlp/lib/python3.8/site-packages/keras/metrics/base_metric.py"", line 196, in replica_local_fn\r\n    result_t._metric_obj = self  # pylint: disable=protected-access\r\nAttributeError: \'dict\' object has no attribute \'_metric_obj\'\r\n```\r\n\r\nHowever, this works:\r\n\r\n```\r\n>>> import keras_nlp\r\n>>> y_true = ""hey, this is great fun""\r\n>>> y_pred = ""great fun indeed""\r\n>>> rouge = keras_nlp.metrics.RougeN(order=2)\r\n>>> rouge.update_state(y_true, y_pred)\r\n>>> rouge.result()\r\n{\'rouge_n_precision\': <tf.Tensor: shape=(), dtype=float32, numpy=0.5>, \'rouge_n_recall\': <tf.Tensor: shape=(), dtype=float32, numpy=0.25>, \'rouge_n_f1_score\': <tf.Tensor: shape=(), dtype=float32, numpy=0.33333334>}\r\n```\r\nReverting back to the `metric_type` implementation', 'comment_created': datetime.datetime(2022, 5, 28, 9, 53, 10, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 887232961, 'comment_body': ""Why not just\r\n\r\n```\r\nif not isinstance(y_true, tf.Tensor):\r\n    inputs = tf.convert_to_tensor(y_true)\r\nif not isinstance(y_pred, tf.Tensor):\r\n    inputs = tf.convert_to_tensor(y_pred)\r\n```\r\n\r\nI don't think we should do the rank coercion in this test case. That would seem to then support scalar inputs only if you had not tensor inputs, but not support scalar inputs if tensors, which is weird behavior.\r\n\r\nConvert to tensor first, then fix rank."", 'comment_created': datetime.datetime(2022, 6, 1, 19, 37, 8, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887233540, 'comment_body': 'It seems like we also support scalar inputs, is that true?\r\n\r\nWe should probably move some of this discussion on supported shape into the docstring.', 'comment_created': datetime.datetime(2022, 6, 1, 19, 37, 59, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887251826, 'comment_body': 'This would fail if shape is `[batch_size, 2]` right now in a not very helpful way. The most friendly thing here might be to do a check if we have a supported shape (rank 0, rank 1 or rank 2 with shape[-1] == 1), and if not give a friendly error message.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 3, 12, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887255952, 'comment_body': ""This feels fragile, we are essentially testing what the base metric class has in it's config. Currently it is only dtype and name, but that could change.\r\n\r\nMaybe just assert the contents of the config you expect, metric_type and use_stemmer."", 'comment_created': datetime.datetime(2022, 6, 1, 20, 8, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887257104, 'comment_body': 'We should test this with a model (which maybe just passes through inputs), and a batched tf.data.Dataset.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 10, 23, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887261895, 'comment_body': ""Open question, how are these rouge scores usually aggregated when reported in a paper? We may want to do a little bit of a dive into this, to understand what the critical user journeys are when reporting an aggregate score.\r\n\r\nLooking at the aggregation code in the package, they have a lot more there...\r\nhttps://github.com/google-research/google-research/blob/master/rouge/scoring.py#L61\r\n\r\nOK if we don't need all that, but we should make sure we understand what people will want when using this metric."", 'comment_created': datetime.datetime(2022, 6, 1, 20, 17, 10, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887268386, 'comment_body': 'I think it would still be a good idea to do some code sharing between these two metrics.\r\n\r\nWhat if we do this...\r\n - Move everything back into `rouge.py` and `rouge_test.py`.\r\n - Add a base class `RougeBase` that contains most of the logic.\r\n - In __init__.py, only export `RougeL` and `RougeN`\r\n\r\nThat would similar to how core Keras handle Conv2D and Conv3D, for example.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 25, 45, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887269092, 'comment_body': 'Add some docstring examples! I think the `>>>` style with actual output, would be useful in this case.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 26, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887413488, 'comment_body': '+1', 'comment_created': datetime.datetime(2022, 6, 2, 1, 1, 15, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887416627, 'comment_body': ""We should test passing this to model.compile()'s metrics arg."", 'comment_created': datetime.datetime(2022, 6, 2, 1, 10, 8, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887417814, 'comment_body': 'just curious - is this [1, 9] a requirement from rouge-score package?', 'comment_created': datetime.datetime(2022, 6, 2, 1, 13, 46, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887418414, 'comment_body': 'Reading through both class implementation, most code can be shared between, so it looks doable to me to have a RougeBase class.', 'comment_created': datetime.datetime(2022, 6, 2, 1, 15, 35, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 888856232, 'comment_body': 'Done! However, I have kept two separate files for unit tests - `rouge_n_test.py` and `rouge_l_test.py` since `rougeN` and `rougeL` are what will eventually be exposed to the user. Let me know if you want only one test script (for `RougeBase`).', 'comment_created': datetime.datetime(2022, 6, 3, 11, 25, 3, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 888856370, 'comment_body': 'Yep, I\'ve written ""Python string"" on line number 96.\r\n\r\nSure, will move it to the doc-string!', 'comment_created': datetime.datetime(2022, 6, 3, 11, 25, 16, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 888856552, 'comment_body': 'Right! Changes made ðŸ‘ðŸ¼ ', 'comment_created': datetime.datetime(2022, 6, 3, 11, 25, 34, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 888856749, 'comment_body': 'Yep! \r\n\r\n```\r\n>>> from rouge_score import rouge_scorer\r\n>>> rg = rouge_scorer.RougeScorer(rouge_types=[""rouge10""])\r\n>>> rg.score(""hey"", ""hey, hello"")\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/abheesht/python_envs/keras_nlp/lib/python3.8/site-packages/rouge_score/rouge_scorer.py"", line 119, in score\r\n    raise ValueError(""Invalid rouge type: %s"" % rouge_type)\r\nValueError: Invalid rouge type: rouge10\r\n```', 'comment_created': datetime.datetime(2022, 6, 3, 11, 25, 54, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 888906180, 'comment_body': ""Yeah, I forgot to do this. I've added examples in the new commit!"", 'comment_created': datetime.datetime(2022, 6, 3, 12, 40, 50, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 888927479, 'comment_body': 'Hmmm, good point. I went through some examples online. In particular, I went through PyTorch Ignite\'s ROUGE metric. Have a look: https://pytorch.org/ignite/_modules/ignite/metrics/nlp/rouge.html#Rouge (`_BaseRouge` class).\r\n\r\nThey take the average:\r\n```\r\n    def compute(self) -> Mapping:\r\n        if self._num_examples == 0:\r\n            raise NotComputableError(""Rouge metric must have at least one example before be computed"")\r\n\r\n        return {\r\n            f""{self._metric_name()}-P"": float(self._precision / self._num_examples),\r\n            f""{self._metric_name()}-R"": float(self._recall / self._num_examples),\r\n            f""{self._metric_name()}-F"": float(self._fmeasure / self._num_examples),\r\n        }\r\n```', 'comment_created': datetime.datetime(2022, 6, 3, 13, 7, 43, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 890799298, 'comment_body': 'Sounds good to me! We can always see if people open up issues. Thanks for checking!', 'comment_created': datetime.datetime(2022, 6, 7, 6, 7, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890801693, 'comment_body': 'white space before and after this paragraph', 'comment_created': datetime.datetime(2022, 6, 7, 6, 12, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890802369, 'comment_body': 'fix alignment of this line', 'comment_created': datetime.datetime(2022, 6, 7, 6, 13, 12, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890803997, 'comment_body': ""We still need to figure out why returning a dict is not working, and see if there is a bug that needs to be fixed in core keras or elsewhere.\r\n\r\nWe should not ship a API signature we don't want because of a bug we need to fix!"", 'comment_created': datetime.datetime(2022, 6, 7, 6, 16, 10, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890804297, 'comment_body': ""rougeLsum we aren't supporting right now correct?"", 'comment_created': datetime.datetime(2022, 6, 7, 6, 16, 43, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890805158, 'comment_body': 'generally trailing underscore is not a naming pattern we follow\r\n\r\nJust call this `inputs`?', 'comment_created': datetime.datetime(2022, 6, 7, 6, 18, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890806536, 'comment_body': 'I would just comment on the shapes here (not the types). So just say supports scalar and batch inputs of shape `()`, `(batch_size,)` and `(batch_size, 1)`.', 'comment_created': datetime.datetime(2022, 6, 7, 6, 20, 25, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 899721320, 'comment_body': ""Is this class necessary? It seems this is juts an alias to `dict`.\r\n\r\nAlso let's create a TODO here for future cleanup, this code is hard to maintain. "", 'comment_created': datetime.datetime(2022, 6, 17, 2, 56, 52, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 899732007, 'comment_body': 'Actually, the reason for defining a class is so that we can do `object.var_name` type assignments. \r\nIf we use a dictionary, this error crops up:\r\n\r\n```\r\n    def replica_local_fn(*args, **kwargs):\r\n      """"""Updates the state of the metric in a replica-local context.""""""\r\n      if any(\r\n          isinstance(arg, keras_tensor.KerasTensor)\r\n          for arg in tf.nest.flatten((args, kwargs))):\r\n        update_op = None\r\n      else:\r\n        update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n      update_ops = []\r\n      if update_op is not None:\r\n        update_ops.append(update_op)\r\n      with tf.control_dependencies(update_ops):\r\n        result_t = self.result()  # pylint: disable=not-callable\r\n    \r\n        # We are adding the metric object as metadata on the result tensor.\r\n        # This is required when we want to use a metric with `add_metric` API on\r\n        # a Model/Layer in graph mode. This metric instance will later be used\r\n        # to reset variable state after each epoch of training.\r\n        # Example:\r\n        #   model = Model()\r\n        #   mean = Mean()\r\n        #   model.add_metric(mean(values), name=\'mean\')\r\n>       result_t._metric_obj = self  # pylint: disable=protected-access\r\nE       AttributeError: \'dict\' object has no attribute \'_metric_obj\'\r\n```', 'comment_created': datetime.datetime(2022, 6, 17, 3, 29, 3, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 899796702, 'comment_body': 'Yeah, definitely the plan would be to remove this code after 2.10 is out!', 'comment_created': datetime.datetime(2022, 6, 17, 6, 17, 15, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 899798316, 'comment_body': 'Can we assert a whole dict structure in here? If so I would fine that a lot more readable actually than the way it\'s done here. Here and elsewhere\r\n\r\n```\r\nassertAlmostEqual(rouge_output, {\r\n    ""rouge-l_precision"": x,\r\n    ""rouge-l_recall"": y,\r\n    ""rouge-l_f1_score"": z,\r\n})\r\n```', 'comment_created': datetime.datetime(2022, 6, 17, 6, 20, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 899799380, 'comment_body': 'Quick note. I think we should remove f""{self.name}_"" part. We would like to make a change to core keras to actually join the metric name when reporting metrics in a dict.\r\n\r\nSo if they metric is called ""rouge-2"", we would join metric when returning the metric dict to ""rouge-2/recall"" or something like that.', 'comment_created': datetime.datetime(2022, 6, 17, 6, 22, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 899805580, 'comment_body': 'Should I remove it now, or later when the fix for the bug has been released?', 'comment_created': datetime.datetime(2022, 6, 17, 6, 33, 22, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 899806318, 'comment_body': 'Right - I can make a custom function for this!', 'comment_created': datetime.datetime(2022, 6, 17, 6, 34, 42, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 899852035, 'comment_body': 'Removed it for now. Let me know if you want to revert it back to `f""{self.name}_""`', 'comment_created': datetime.datetime(2022, 6, 17, 7, 41, 58, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}]","[{'commit_sha': '62324bee018058e827dd03d90ecd22ad271dd468', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3bc476a7f5520368a241d48b84be5452ccfbbf24', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b09302d0f35fc3aea45a27f735b8717e930ce956', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cadbd01056661a9ed5308b91d69c94e52f1f1213', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3e767ff467f0c3b6587c8fc61bffcf72acbac636', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b622cfe885f601d21a186efc02bf5b11b35f7e4b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '38a809fbb5173a4986d96ca9ec46ad147c1e2a01', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e3bf5030d6aef1a01eba14c820a7352c7cd1b585', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd25403bc07b1ba9019ae12006de5c25efc0c971c', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2f9a35cbbb124daa312ce64b6655eebb047637df', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9b4c1f165230f8d9fc761ebb4e35a00bb384afc6', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c59aa74b49d0d12da11bbd683468a75abca8a934', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd166ab7a238c270fdb4fa55c891df4b85c14781a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a669d219b3deb566c668ab416a1cb9149abf3d0f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '632df5d9500cc6775a931be79fd400a2adbf44b0', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7586e0002acbd5866c29aba71850abf13412bfc2', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '893aab9681b10f59dcc03795b28369e706c0ee54', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ccf33d4ac891dab50381cde4396c020f8f06b1e7', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '748df818b6a3e5530b2110b1eb7ec483a9f547a3', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a793d3d97d24f08735fdbc93439a353059ddb72b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b8dae75b58cee6cc280319d2c9f4eb4f1d962843', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '80500863b58e0a03b01cebe3007882bf20567f3a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'da44d22335d927d68922ad9df6f43d30dfe56f6c', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f8c05aacc34cfa111321173bf5298bd184347a9f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f4df42b7b6111cbde95f77722b2020b9d3512a67', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '723d8e7e7067dd803a005f15891e5cdd1ca0d78b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b0fe8bc3c972bb9bbb350a4de67052b3ee711ff1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7250617b2c8934f1b454cd92245c11ad0f1680a0', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3c5b3dc8ff7b150b6aa9dbd6caeace40ae7f9c9d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4fa518ab13802a12dab7110ee751ebb1909de5d1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '14e851fa62fe8eb9519148fb73e1b93c219f9175', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
911208416,Add ROUGE Metric,"Resolves https://github.com/keras-team/keras-nlp/issues/67

@mattdangerw, @chenmoneygithub, this PR is now ready for review :)",True,122,https://api.github.com/repos/keras-team/keras-nlp/pulls/122,https://github.com/keras-team/keras-nlp/pull/122,closed,1000,0,7,31,14,52,0,0,[],2022-04-16 05:47:29+00:00,2022-06-17 17:04:14+00:00,5397405.0,"62 days, 11:16:45","[{'comment_id': 853459495, 'comment_body': 'Prefer use a well-defined example rather than random data so that users can manually calculate the F1 score.', 'comment_created': datetime.datetime(2022, 4, 19, 20, 28, 5, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 853460110, 'comment_body': 'In docstring it defaults to float32, which mismatches the default value `None`. Please fix it.', 'comment_created': datetime.datetime(2022, 4, 19, 20, 28, 56, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 853466742, 'comment_body': 'rename `num_samples` to `batch_size`, we should be consistent with the naming.', 'comment_created': datetime.datetime(2022, 4, 19, 20, 38, 23, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 853472945, 'comment_body': 'We do not actually need to make this a standalone util. Just write \r\n```\r\nf1_scores, precisions, recalls = tf_text.metrics.rouge_l(\r\n        y_pred, y_true, alpha=alpha\r\n    )\r\n```\r\nin the RougeL class.', 'comment_created': datetime.datetime(2022, 4, 19, 20, 46, 34, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 853473962, 'comment_body': 'This is a bit verbose, we can rename to `test_initialization()`.', 'comment_created': datetime.datetime(2022, 4, 19, 20, 47, 51, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 854094040, 'comment_body': ""I was following the format of metrics given in Keras. For example, check this out: https://github.com/keras-team/keras/blob/master/keras/metrics/metrics.py#L220, https://github.com/keras-team/keras/blob/master/keras/metrics/metrics.py#L3331.\r\n\r\nThe class helps in aggregating the ROUGE score (the user can iterate over the dataset, and the class will return the avg. ROUGE score).\r\n\r\nThe function allows string inputs, I think. \r\n\r\nI've changed it to this for the time being: \r\n```\r\nf1_scores, precisions, recalls = tf_text.metrics.rouge_l(\r\n        y_pred, y_true, alpha=alpha\r\n    )\r\n```\r\nbut let me know which one is more appropriate and I'll change it to that. Thanks!"", 'comment_created': datetime.datetime(2022, 4, 20, 12, 49, 29, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 854094321, 'comment_body': 'My bad. Changed!', 'comment_created': datetime.datetime(2022, 4, 20, 12, 49, 45, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 854097792, 'comment_body': 'Done!', 'comment_created': datetime.datetime(2022, 4, 20, 12, 53, 30, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 854375709, 'comment_body': ""Thanks! My guess is there was some need to directly call the metrics computing function, which I am not sure if still applies. Let's keep it simple for now, and we can add the util if we find it is necessary. "", 'comment_created': datetime.datetime(2022, 4, 20, 17, 16, 14, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 854380659, 'comment_body': 'Great! Let me know if further changes are required', 'comment_created': datetime.datetime(2022, 4, 20, 17, 22, 22, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 882337680, 'comment_body': 'space after period', 'comment_created': datetime.datetime(2022, 5, 26, 5, 50, 50, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 882337840, 'comment_body': 'rouge_score -> rouge-score', 'comment_created': datetime.datetime(2022, 5, 26, 5, 51, 13, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 882341574, 'comment_body': 'a keras metric can just return a dict of scalars I believe, should we just return a dict here and from the metric overall?', 'comment_created': datetime.datetime(2022, 5, 26, 5, 58, 58, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 882341920, 'comment_body': 'Why is this variable called rouge_l when it could be rouge_l or rouge_n? Seems like a confusing name.', 'comment_created': datetime.datetime(2022, 5, 26, 5, 59, 42, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 882356973, 'comment_body': 'This feels too tricky, particularly with the order of RougeN being a hidden parameter of the string passed to this argument.\r\n\r\nWhat about making a separate RougeL and RougeN metric class?', 'comment_created': datetime.datetime(2022, 5, 26, 6, 28, 8, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 884067836, 'comment_body': 'Yeah, this is a typo. Corrected in the latest commit!', 'comment_created': datetime.datetime(2022, 5, 28, 4, 16, 50, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 884067972, 'comment_body': 'Discussed offline with Matt. We are going ahead with separate classes for ROUGE-N and ROUGE-L!', 'comment_created': datetime.datetime(2022, 5, 28, 4, 18, 24, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 884104863, 'comment_body': 'Ah, I was not aware of this. Will return a dictionary', 'comment_created': datetime.datetime(2022, 5, 28, 8, 58, 7, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 884111103, 'comment_body': '@mattdangerw, looks like this doesn\'t work; an error is thrown when I return a dictionary from `result()`. Have a look at this snippet of code:\r\n\r\n```\r\n>>> import keras_nlp\r\n>>> y_true = ""hey, this is great fun""\r\n>>> y_pred = ""great fun indeed""\r\n>>> rouge = keras_nlp.metrics.RougeN(order=2)\r\n>>> rouge(y_true, y_pred)\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/abheesht/python_envs/keras_nlp/lib/python3.8/site-packages/keras/metrics/base_metric.py"", line 200, in __call__\r\n    return distributed_training_utils.call_replica_local_fn(\r\n  File ""/home/abheesht/python_envs/keras_nlp/lib/python3.8/site-packages/keras/distribute/distributed_training_utils.py"", line 60, in call_replica_local_fn\r\n    return fn(*args, **kwargs)\r\n  File ""/home/abheesht/python_envs/keras_nlp/lib/python3.8/site-packages/keras/metrics/base_metric.py"", line 196, in replica_local_fn\r\n    result_t._metric_obj = self  # pylint: disable=protected-access\r\nAttributeError: \'dict\' object has no attribute \'_metric_obj\'\r\n```\r\n\r\nHowever, this works:\r\n\r\n```\r\n>>> import keras_nlp\r\n>>> y_true = ""hey, this is great fun""\r\n>>> y_pred = ""great fun indeed""\r\n>>> rouge = keras_nlp.metrics.RougeN(order=2)\r\n>>> rouge.update_state(y_true, y_pred)\r\n>>> rouge.result()\r\n{\'rouge_n_precision\': <tf.Tensor: shape=(), dtype=float32, numpy=0.5>, \'rouge_n_recall\': <tf.Tensor: shape=(), dtype=float32, numpy=0.25>, \'rouge_n_f1_score\': <tf.Tensor: shape=(), dtype=float32, numpy=0.33333334>}\r\n```\r\nReverting back to the `metric_type` implementation', 'comment_created': datetime.datetime(2022, 5, 28, 9, 53, 10, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 887232961, 'comment_body': ""Why not just\r\n\r\n```\r\nif not isinstance(y_true, tf.Tensor):\r\n    inputs = tf.convert_to_tensor(y_true)\r\nif not isinstance(y_pred, tf.Tensor):\r\n    inputs = tf.convert_to_tensor(y_pred)\r\n```\r\n\r\nI don't think we should do the rank coercion in this test case. That would seem to then support scalar inputs only if you had not tensor inputs, but not support scalar inputs if tensors, which is weird behavior.\r\n\r\nConvert to tensor first, then fix rank."", 'comment_created': datetime.datetime(2022, 6, 1, 19, 37, 8, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887233540, 'comment_body': 'It seems like we also support scalar inputs, is that true?\r\n\r\nWe should probably move some of this discussion on supported shape into the docstring.', 'comment_created': datetime.datetime(2022, 6, 1, 19, 37, 59, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887251826, 'comment_body': 'This would fail if shape is `[batch_size, 2]` right now in a not very helpful way. The most friendly thing here might be to do a check if we have a supported shape (rank 0, rank 1 or rank 2 with shape[-1] == 1), and if not give a friendly error message.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 3, 12, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887255952, 'comment_body': ""This feels fragile, we are essentially testing what the base metric class has in it's config. Currently it is only dtype and name, but that could change.\r\n\r\nMaybe just assert the contents of the config you expect, metric_type and use_stemmer."", 'comment_created': datetime.datetime(2022, 6, 1, 20, 8, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887257104, 'comment_body': 'We should test this with a model (which maybe just passes through inputs), and a batched tf.data.Dataset.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 10, 23, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887261895, 'comment_body': ""Open question, how are these rouge scores usually aggregated when reported in a paper? We may want to do a little bit of a dive into this, to understand what the critical user journeys are when reporting an aggregate score.\r\n\r\nLooking at the aggregation code in the package, they have a lot more there...\r\nhttps://github.com/google-research/google-research/blob/master/rouge/scoring.py#L61\r\n\r\nOK if we don't need all that, but we should make sure we understand what people will want when using this metric."", 'comment_created': datetime.datetime(2022, 6, 1, 20, 17, 10, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887268386, 'comment_body': 'I think it would still be a good idea to do some code sharing between these two metrics.\r\n\r\nWhat if we do this...\r\n - Move everything back into `rouge.py` and `rouge_test.py`.\r\n - Add a base class `RougeBase` that contains most of the logic.\r\n - In __init__.py, only export `RougeL` and `RougeN`\r\n\r\nThat would similar to how core Keras handle Conv2D and Conv3D, for example.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 25, 45, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887269092, 'comment_body': 'Add some docstring examples! I think the `>>>` style with actual output, would be useful in this case.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 26, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887413488, 'comment_body': '+1', 'comment_created': datetime.datetime(2022, 6, 2, 1, 1, 15, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887416627, 'comment_body': ""We should test passing this to model.compile()'s metrics arg."", 'comment_created': datetime.datetime(2022, 6, 2, 1, 10, 8, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887417814, 'comment_body': 'just curious - is this [1, 9] a requirement from rouge-score package?', 'comment_created': datetime.datetime(2022, 6, 2, 1, 13, 46, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887418414, 'comment_body': 'Reading through both class implementation, most code can be shared between, so it looks doable to me to have a RougeBase class.', 'comment_created': datetime.datetime(2022, 6, 2, 1, 15, 35, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 888856232, 'comment_body': 'Done! However, I have kept two separate files for unit tests - `rouge_n_test.py` and `rouge_l_test.py` since `rougeN` and `rougeL` are what will eventually be exposed to the user. Let me know if you want only one test script (for `RougeBase`).', 'comment_created': datetime.datetime(2022, 6, 3, 11, 25, 3, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 888856370, 'comment_body': 'Yep, I\'ve written ""Python string"" on line number 96.\r\n\r\nSure, will move it to the doc-string!', 'comment_created': datetime.datetime(2022, 6, 3, 11, 25, 16, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 888856552, 'comment_body': 'Right! Changes made ðŸ‘ðŸ¼ ', 'comment_created': datetime.datetime(2022, 6, 3, 11, 25, 34, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 888856749, 'comment_body': 'Yep! \r\n\r\n```\r\n>>> from rouge_score import rouge_scorer\r\n>>> rg = rouge_scorer.RougeScorer(rouge_types=[""rouge10""])\r\n>>> rg.score(""hey"", ""hey, hello"")\r\nTraceback (most recent call last):\r\n  File ""<stdin>"", line 1, in <module>\r\n  File ""/home/abheesht/python_envs/keras_nlp/lib/python3.8/site-packages/rouge_score/rouge_scorer.py"", line 119, in score\r\n    raise ValueError(""Invalid rouge type: %s"" % rouge_type)\r\nValueError: Invalid rouge type: rouge10\r\n```', 'comment_created': datetime.datetime(2022, 6, 3, 11, 25, 54, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 888906180, 'comment_body': ""Yeah, I forgot to do this. I've added examples in the new commit!"", 'comment_created': datetime.datetime(2022, 6, 3, 12, 40, 50, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 888927479, 'comment_body': 'Hmmm, good point. I went through some examples online. In particular, I went through PyTorch Ignite\'s ROUGE metric. Have a look: https://pytorch.org/ignite/_modules/ignite/metrics/nlp/rouge.html#Rouge (`_BaseRouge` class).\r\n\r\nThey take the average:\r\n```\r\n    def compute(self) -> Mapping:\r\n        if self._num_examples == 0:\r\n            raise NotComputableError(""Rouge metric must have at least one example before be computed"")\r\n\r\n        return {\r\n            f""{self._metric_name()}-P"": float(self._precision / self._num_examples),\r\n            f""{self._metric_name()}-R"": float(self._recall / self._num_examples),\r\n            f""{self._metric_name()}-F"": float(self._fmeasure / self._num_examples),\r\n        }\r\n```', 'comment_created': datetime.datetime(2022, 6, 3, 13, 7, 43, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 890799298, 'comment_body': 'Sounds good to me! We can always see if people open up issues. Thanks for checking!', 'comment_created': datetime.datetime(2022, 6, 7, 6, 7, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890801693, 'comment_body': 'white space before and after this paragraph', 'comment_created': datetime.datetime(2022, 6, 7, 6, 12, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890802369, 'comment_body': 'fix alignment of this line', 'comment_created': datetime.datetime(2022, 6, 7, 6, 13, 12, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890803997, 'comment_body': ""We still need to figure out why returning a dict is not working, and see if there is a bug that needs to be fixed in core keras or elsewhere.\r\n\r\nWe should not ship a API signature we don't want because of a bug we need to fix!"", 'comment_created': datetime.datetime(2022, 6, 7, 6, 16, 10, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890804297, 'comment_body': ""rougeLsum we aren't supporting right now correct?"", 'comment_created': datetime.datetime(2022, 6, 7, 6, 16, 43, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890805158, 'comment_body': 'generally trailing underscore is not a naming pattern we follow\r\n\r\nJust call this `inputs`?', 'comment_created': datetime.datetime(2022, 6, 7, 6, 18, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 890806536, 'comment_body': 'I would just comment on the shapes here (not the types). So just say supports scalar and batch inputs of shape `()`, `(batch_size,)` and `(batch_size, 1)`.', 'comment_created': datetime.datetime(2022, 6, 7, 6, 20, 25, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 899721320, 'comment_body': ""Is this class necessary? It seems this is juts an alias to `dict`.\r\n\r\nAlso let's create a TODO here for future cleanup, this code is hard to maintain. "", 'comment_created': datetime.datetime(2022, 6, 17, 2, 56, 52, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 899732007, 'comment_body': 'Actually, the reason for defining a class is so that we can do `object.var_name` type assignments. \r\nIf we use a dictionary, this error crops up:\r\n\r\n```\r\n    def replica_local_fn(*args, **kwargs):\r\n      """"""Updates the state of the metric in a replica-local context.""""""\r\n      if any(\r\n          isinstance(arg, keras_tensor.KerasTensor)\r\n          for arg in tf.nest.flatten((args, kwargs))):\r\n        update_op = None\r\n      else:\r\n        update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable\r\n      update_ops = []\r\n      if update_op is not None:\r\n        update_ops.append(update_op)\r\n      with tf.control_dependencies(update_ops):\r\n        result_t = self.result()  # pylint: disable=not-callable\r\n    \r\n        # We are adding the metric object as metadata on the result tensor.\r\n        # This is required when we want to use a metric with `add_metric` API on\r\n        # a Model/Layer in graph mode. This metric instance will later be used\r\n        # to reset variable state after each epoch of training.\r\n        # Example:\r\n        #   model = Model()\r\n        #   mean = Mean()\r\n        #   model.add_metric(mean(values), name=\'mean\')\r\n>       result_t._metric_obj = self  # pylint: disable=protected-access\r\nE       AttributeError: \'dict\' object has no attribute \'_metric_obj\'\r\n```', 'comment_created': datetime.datetime(2022, 6, 17, 3, 29, 3, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 899796702, 'comment_body': 'Yeah, definitely the plan would be to remove this code after 2.10 is out!', 'comment_created': datetime.datetime(2022, 6, 17, 6, 17, 15, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 899798316, 'comment_body': 'Can we assert a whole dict structure in here? If so I would fine that a lot more readable actually than the way it\'s done here. Here and elsewhere\r\n\r\n```\r\nassertAlmostEqual(rouge_output, {\r\n    ""rouge-l_precision"": x,\r\n    ""rouge-l_recall"": y,\r\n    ""rouge-l_f1_score"": z,\r\n})\r\n```', 'comment_created': datetime.datetime(2022, 6, 17, 6, 20, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 899799380, 'comment_body': 'Quick note. I think we should remove f""{self.name}_"" part. We would like to make a change to core keras to actually join the metric name when reporting metrics in a dict.\r\n\r\nSo if they metric is called ""rouge-2"", we would join metric when returning the metric dict to ""rouge-2/recall"" or something like that.', 'comment_created': datetime.datetime(2022, 6, 17, 6, 22, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 899805580, 'comment_body': 'Should I remove it now, or later when the fix for the bug has been released?', 'comment_created': datetime.datetime(2022, 6, 17, 6, 33, 22, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 899806318, 'comment_body': 'Right - I can make a custom function for this!', 'comment_created': datetime.datetime(2022, 6, 17, 6, 34, 42, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 899852035, 'comment_body': 'Removed it for now. Let me know if you want to revert it back to `f""{self.name}_""`', 'comment_created': datetime.datetime(2022, 6, 17, 7, 41, 58, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}]","[{'commit_sha': '62324bee018058e827dd03d90ecd22ad271dd468', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3bc476a7f5520368a241d48b84be5452ccfbbf24', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b09302d0f35fc3aea45a27f735b8717e930ce956', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cadbd01056661a9ed5308b91d69c94e52f1f1213', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3e767ff467f0c3b6587c8fc61bffcf72acbac636', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b622cfe885f601d21a186efc02bf5b11b35f7e4b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '38a809fbb5173a4986d96ca9ec46ad147c1e2a01', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e3bf5030d6aef1a01eba14c820a7352c7cd1b585', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd25403bc07b1ba9019ae12006de5c25efc0c971c', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2f9a35cbbb124daa312ce64b6655eebb047637df', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9b4c1f165230f8d9fc761ebb4e35a00bb384afc6', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c59aa74b49d0d12da11bbd683468a75abca8a934', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd166ab7a238c270fdb4fa55c891df4b85c14781a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a669d219b3deb566c668ab416a1cb9149abf3d0f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '632df5d9500cc6775a931be79fd400a2adbf44b0', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7586e0002acbd5866c29aba71850abf13412bfc2', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '893aab9681b10f59dcc03795b28369e706c0ee54', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ccf33d4ac891dab50381cde4396c020f8f06b1e7', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '748df818b6a3e5530b2110b1eb7ec483a9f547a3', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a793d3d97d24f08735fdbc93439a353059ddb72b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b8dae75b58cee6cc280319d2c9f4eb4f1d962843', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '80500863b58e0a03b01cebe3007882bf20567f3a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'da44d22335d927d68922ad9df6f43d30dfe56f6c', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f8c05aacc34cfa111321173bf5298bd184347a9f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f4df42b7b6111cbde95f77722b2020b9d3512a67', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '723d8e7e7067dd803a005f15891e5cdd1ca0d78b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b0fe8bc3c972bb9bbb350a4de67052b3ee711ff1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7250617b2c8934f1b454cd92245c11ad0f1680a0', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3c5b3dc8ff7b150b6aa9dbd6caeace40ae7f9c9d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4fa518ab13802a12dab7110ee751ebb1909de5d1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '14e851fa62fe8eb9519148fb73e1b93c219f9175', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
965195260,Add BLEU Score ,"Resolves #65 

**Notebooks:**

- OLD: https://colab.research.google.com/drive/1TZ8XnrmMcU8ZE2J-3amb44p-U-hxER53?usp=sharing

**Edit (commit e90bef3c3fc6ca72d6229f6eaf860c0df9239338):**

We have decided to go ahead with a `tf.py_function` implementation. Yet to add UTs, but otherwise, it's done.

**Notebooks:**

- Graph ops vs Python ops: https://colab.research.google.com/drive/1TZ8XnrmMcU8ZE2J-3amb44p-U-hxER53?usp=sharing (preserving this here for posterity, in case someone attempts to implement BLEU score using Graph ops)

- Trial run on WMT14 en-de dataset: https://colab.research.google.com/drive/1jBN56tc6EdCctr50Em7Xk7lx8_y45QV_?usp=sharing",True,222,https://api.github.com/repos/keras-team/keras-nlp/pulls/222,https://github.com/keras-team/keras-nlp/pull/222,closed,697,7,8,18,1,45,0,0,[],2022-06-13 02:46:55+00:00,2022-07-11 23:35:17+00:00,2494102.0,"28 days, 20:48:22","[{'comment_id': 911360343, 'comment_body': 'What happens it you pass a tokenizer layer here, will that work? Say byte tokenizer for simplicity.', 'comment_created': datetime.datetime(2022, 6, 30, 19, 1, 15, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911360874, 'comment_body': 'We should probably mention more prominently that this will replicate sacrebleu by default, but can be used with other tokenizers e.g. for other languages.', 'comment_created': datetime.datetime(2022, 6, 30, 19, 1, 54, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911361584, 'comment_body': 'summary sentence for a docstring should always fit on a single line.', 'comment_created': datetime.datetime(2022, 6, 30, 19, 2, 58, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911407296, 'comment_body': 'Why is this public? do we expect this to be called explicitly? If so can you show the use case?', 'comment_created': datetime.datetime(2022, 6, 30, 20, 9, 16, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911410030, 'comment_body': 'where are we getting this number from?', 'comment_created': datetime.datetime(2022, 6, 30, 20, 13, 22, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911424678, 'comment_body': ""It seems like corpus bleu is the better option here? I see that sacrebleu exposes methods for both of these, but does not seems to document the sentence one. Huggingface looks like it might not even have an option for this (is that true?).\r\n\r\nI guess I'm wondering if it might make sense to not even expose this, and wait till someone asks for the sentence option."", 'comment_created': datetime.datetime(2022, 6, 30, 20, 35, 22, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911426651, 'comment_body': 'Can we describe this better? Lin et al. 2004 with a period in the middle of the docstring does not read very well. Also please add to reference section.', 'comment_created': datetime.datetime(2022, 6, 30, 20, 38, 17, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911430623, 'comment_body': ""So we've confirmed at this point that the Counter approach is much more efficient correct? Can we open an issue to remove the py_function from bleu as a follow up?\r\n\r\nDescribe a bit of the slowdown we saw when trying to do that today. Probably not urgent, but maybe someday we could collaborate with tf.text to ship an efficient non-python version of this op."", 'comment_created': datetime.datetime(2022, 6, 30, 20, 44, 17, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911431452, 'comment_body': 'There are a lot of nested function that are quite long all over. Not very readable.\r\n\r\nCan we either pull them out of the class entirely, or make them class methods?', 'comment_created': datetime.datetime(2022, 6, 30, 20, 45, 26, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911537562, 'comment_body': ""After doing a survey, this is what I found:\r\n\r\n- NLTK exposes Sentence BLEU, but the function merely takes in one sample and calls the corpus BLEU function: https://github.com/nltk/nltk/blob/develop/nltk/translate/bleu_score.py#L20. This answer explains it well: https://stackoverflow.com/a/40650581. I think they expect the user to average the metric on their own.\r\n- PyTorch Ignite does provide an option for using Corpus BLEU/Sentence BLEU: https://pytorch.org/ignite/generated/ignite.metrics.Bleu.html (`average = macro_average`, and surprisingly, Sentence BLEU is the default).\r\n- Hugging Face's SacreBLEU implementation does not explicitly provide an option, but have a look at this parameter: https://github.com/huggingface/datasets/blob/master/metrics/sacrebleu/sacrebleu.py#L66. \r\n\r\n\r\nConclusion: I think the expectation is that, if users want to compute the Sentence BLEU score, they can do so by passing one sample at a time, and average over the returned scores. \r\n\r\n**Some additional notes**\r\nHowever, another point to note is that HF provides two options with all its metrics:\r\n- `.compute()` - user can pass one sample at a time, get the BLEU scores, and average over them for computing Sentence BLEU.\r\n- `.add_batch()` - will compute the Corpus BLEU score across all samples across batches.\r\n\r\nWe use Keras metrics similar to the `add_batch()` function. So, if the user wants to compute the Sentence BLEU score, he/she/they will have to re-initialise the metric for every sample. PyTorch Ignite metrics also work similar to the `add_batch` function, which is why they have provided an option for macro/micro-averaging. So, I am just wondering whether HF and NLTK do not provide explicit options to macro-average the BLEU scores because the user can average the BLEU scores. But with Ignite, the user can't do that without re-initialising before every sample, which is why an option has been provided,\r\n"", 'comment_created': datetime.datetime(2022, 7, 1, 0, 52, 5, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 911541430, 'comment_body': 'Right! Opened an issue for this: https://github.com/keras-team/keras-nlp/issues/247 :)', 'comment_created': datetime.datetime(2022, 7, 1, 1, 4, 38, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 912058624, 'comment_body': ""Hmmm, it won't work with byte tokeniser because we use `tensor_to_string_list` in the code. Do you want me to change that?"", 'comment_created': datetime.datetime(2022, 7, 1, 15, 25, 20, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 912068967, 'comment_body': 'All implementation use 4 as the default.\r\n\r\nhttps://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/bleu.py#L16\r\nhttps://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py#L48\r\n', 'comment_created': datetime.datetime(2022, 7, 1, 15, 40, 12, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 912083851, 'comment_body': 'Made a few of them class methods.', 'comment_created': datetime.datetime(2022, 7, 1, 16, 2, 14, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 915200028, 'comment_body': 'nit: capital case By default', 'comment_created': datetime.datetime(2022, 7, 6, 19, 49, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 915203050, 'comment_body': 'This explanation is confusing for users not familiar with BLEU. For example, ""clipped count"" is an opaque phrase. \r\n\r\nLet\'s add a link for explanation. Unfortunately the wikipedia page isn\'t that good (at least seems to me), this is a good resource: https://cloud.google.com/translate/automl/docs/evaluate#bleu', 'comment_created': datetime.datetime(2022, 7, 6, 19, 53, 6, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 915204470, 'comment_body': 'This requirement on shape match is a little weird. Suppose both `y_pred` and `y_true` are scalar, can we just convert `y_true` for them?', 'comment_created': datetime.datetime(2022, 7, 6, 19, 55, 5, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 915205615, 'comment_body': 'Is it necessary? If people are not interested in using `model.evaluate()`, can they just run it in pure eager mode?', 'comment_created': datetime.datetime(2022, 7, 6, 19, 56, 36, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 915244107, 'comment_body': 'nit: up to', 'comment_created': datetime.datetime(2022, 7, 6, 20, 49, 25, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 915245037, 'comment_body': 'Is this a string or tensor of split tokens?', 'comment_created': datetime.datetime(2022, 7, 6, 20, 50, 46, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 915245877, 'comment_body': '@mattdangerw Matt do we want formal docstring in private method? This method is complicated, so having a detailed explanation looks okay to me. But our general guideline is not to have it?', 'comment_created': datetime.datetime(2022, 7, 6, 20, 52, 1, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 915804442, 'comment_body': ""Hmmm, the idea right now is that one sample can have multiple references. So, basically, a translation can have multiple reference sentences. That's why the rank of `y_true` = rank of `y_pred` + 1."", 'comment_created': datetime.datetime(2022, 7, 7, 12, 17, 55, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 915805758, 'comment_body': 'True, but we call the tokeniser after converting the inputs to tensors. So, we have to use TF ops here such as `tf.strings.regex_replace()`.', 'comment_created': datetime.datetime(2022, 7, 7, 12, 19, 32, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 915806462, 'comment_body': 'Yeah, this should be `list`.', 'comment_created': datetime.datetime(2022, 7, 7, 12, 20, 19, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 916462375, 'comment_body': 'remove *args and **kwargs', 'comment_created': datetime.datetime(2022, 7, 8, 4, 55, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916463712, 'comment_body': 'can we add a test with `model.compile` to make sure we are ok in function compilation? I think we did that for rouge.', 'comment_created': datetime.datetime(2022, 7, 8, 4, 58, 57, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916465186, 'comment_body': ""let's just split this into a separate private method on the layer, code can stay unchanged"", 'comment_created': datetime.datetime(2022, 7, 8, 5, 3, 1, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916466512, 'comment_body': 'given that you are converting everything to numpy, would this do better as a numpy_function call?', 'comment_created': datetime.datetime(2022, 7, 8, 5, 6, 41, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916466634, 'comment_body': 'If you remove the trailing comma will this format to one line?', 'comment_created': datetime.datetime(2022, 7, 8, 5, 6, 58, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916466962, 'comment_body': ""I think it's not necessary, but fine to have if we think it adds something.  In this case it does."", 'comment_created': datetime.datetime(2022, 7, 8, 5, 7, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916468566, 'comment_body': ""I think we should either support our tokenizers or not name this argument to something else.\r\n\r\nTokenizer means something specific in our library now, if we use that name but don't support our tokenizer class that is a bad look."", 'comment_created': datetime.datetime(2022, 7, 8, 5, 11, 57, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916469741, 'comment_body': ""I think tokenizer can go on the config? I'm not sure if that will run into issues with saving, but should work if you register the custom function you pass in as a serializable."", 'comment_created': datetime.datetime(2022, 7, 8, 5, 15, 12, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916473292, 'comment_body': 'Maybe make this one a private method called `_tokenizer` that does.\r\n\r\n```\r\nif self.tokenizer:\r\n    return self.tokenizer(x)\r\n\r\nfor pattern, replacement...\r\n```\r\n\r\nThat way saving self.tokenizer in the config would work as expected.', 'comment_created': datetime.datetime(2022, 7, 8, 5, 24, 59, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916474574, 'comment_body': ""How common is it to call bleu with only a single reference translation (is that unusual, or the normal case?). If it's usual, could we consider supporting the case that y_pred and y_true have the same shape? That might lead to a simpler overall usage in that case.\r\n\r\nOpen question, I'm not sure if that is true."", 'comment_created': datetime.datetime(2022, 7, 8, 5, 28, 3, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916580544, 'comment_body': ""Well, it is called often with one reference...but all implementations take input the way I've mentioned in the doc-string, whether it be SacreBLEU, or HF, or even NLTK."", 'comment_created': datetime.datetime(2022, 7, 8, 8, 21, 16, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 916582385, 'comment_body': ""We do support our tokenisers. I've added a unit test here: https://github.com/keras-team/keras-nlp/blob/0b6ebfafe2a819bf39061d07f6382d4f0727d55e/keras_nlp/metrics/bleu_test.py#L105"", 'comment_created': datetime.datetime(2022, 7, 8, 8, 23, 30, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 916601038, 'comment_body': 'My bad - I forgot to add it.', 'comment_created': datetime.datetime(2022, 7, 8, 8, 45, 47, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 916606466, 'comment_body': ""I'll remove `calculate_bleu_score` from here. Let's keep `validate_and_fix_rank` inside this function for homogeneity (since we have done the same for ROUGE and Edit Distance)?"", 'comment_created': datetime.datetime(2022, 7, 8, 8, 52, 17, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 916732938, 'comment_body': ""Hmmm, I think it won't work because we have ragged tensors? Numpy has no support for ragged matrices, I think."", 'comment_created': datetime.datetime(2022, 7, 8, 11, 35, 39, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 916774688, 'comment_body': 'I think it fails to convert ragged input tensors to numpy arrays:\r\n```  \r\nFile ""/home/abheesht/repos/keras-nlp/keras_nlp/metrics/bleu.py"", line 224, in _corpus_bleu\r\n    for (references, translation) in zip(\r\nTypeError: in user code:\r\n\r\n    File ""/home/abheesht/repos/keras-nlp/keras_nlp/metrics/bleu.py"", line 349, in update_state  *\r\n        (\r\n    File ""/home/abheesht/repos/keras-nlp/keras_nlp/metrics/bleu.py"", line 302, in _calculate_bleu_score\r\n        ) = self._corpus_bleu(\r\n    File ""/home/abheesht/repos/keras-nlp/keras_nlp/metrics/bleu.py"", line 224, in _corpus_bleu\r\n        for (references, translation) in zip(\r\n\r\n    TypeError: iteration over a 0-d array\r\n```', 'comment_created': datetime.datetime(2022, 7, 8, 12, 36, 28, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 916825793, 'comment_body': 'Facing some issues with this. Will discuss offline.\r\n\r\nEdit: Fixed!', 'comment_created': datetime.datetime(2022, 7, 8, 13, 38, 23, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 917168323, 'comment_body': 'a ragged tensor with shape `(batch_size, None)`', 'comment_created': datetime.datetime(2022, 7, 8, 22, 23, 27, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 917168689, 'comment_body': ""sg, let's stick with py_function for now"", 'comment_created': datetime.datetime(2022, 7, 8, 22, 24, 37, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 917168743, 'comment_body': 'sg!', 'comment_created': datetime.datetime(2022, 7, 8, 22, 24, 50, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 917168935, 'comment_body': 'remember to fix this!', 'comment_created': datetime.datetime(2022, 7, 8, 22, 25, 25, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': 'f151982e6b3e01818d72236b62a0ab2fb4fde37d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7708cd9ae2135566414784bef8ccf32a44cbe941', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0f757d5e524afce90ae9593690b1b667e94373cc', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e90bef3c3fc6ca72d6229f6eaf860c0df9239338', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'eface1e48d829581d3850d5ab57d5cc2f2de0955', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dc2110ef6e14ffdd0bd2621da8f2bcae633b29a0', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e18cc508dfa6d8c7f2c7f0fabe49036256341cf4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd59058b353303bb5be99608dbd2842660a11f9d3', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5ddcfa74106e207301b2abaf092904f505063b5e', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b2d0822b25669dcfb6bebc306aca3e1f7f40819f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '45136fb446a7b5954681b54dd0c035528baf204a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '23f9a2f43099e6e994633dee3a7866f85eaa3949', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0217b71f0c0c352bf043739cf6d2844c61ea5214', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0b6ebfafe2a819bf39061d07f6382d4f0727d55e', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'be897dd952137cc6f5d20fed56fe17a541f027b9', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '363da3a515512999853fdb6c1ba1581917d7ca5e', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fa2c65818ac12634c492d3f80203e417d9cf6187', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0acaae5ece1f46fec6f89cdc568265c700ab0ae5', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
971446220,Add Edit Distance Metric,"@mattdangerw, @chenmoneygithub, here is the PR for edit distance.

I did a deep-dive on different implementations of WER/CER. I've created a document for the same:
https://docs.google.com/document/d/1IujxboocZbiv_ZAZwfxAG77Aql8O5waUazLrvRxmh-4/edit?usp=sharing

All implementations seem to use basic preprocessing/tokenisers. Please let me if I should implement WER/CER as well. If not, I'll add UTs for Edit Distance. Thanks!

`tf.vectorized_map` did not work. Hence, I've used `tf.map_fn`:

```
>>> ed = EditDistance()
>>> ed(character_splitter([""thisisfun"", ""bear""]), character_splitter([""yesfun"", ""beer""]))
...
        File ""/home/abheesht/python_envs/keras_nlp/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 2133, in __init__
          self._traceback = tf_stack.extract_stack_for_node(self._c_op)
        File ""/home/abheesht/python_envs/keras_nlp/lib/python3.8/site-packages/tensorflow/python/util/tf_stack.py"", line 183, in extract_stack_for_node
          return _tf_stack.extract_stack_for_node(
    Node: 'loop_body/RaggedFromVariant/RaggedTensorFromVariant/pfor/TensorListConcatV2'
    PartialTensorShape: Incompatible shapes during merge: [9] vs. [4]
         [[{{node loop_body/RaggedFromVariant/RaggedTensorFromVariant/pfor/TensorListConcatV2}}]] [Op:__inference_f_819]
```



**Edit (https://github.com/keras-team/keras-nlp/pull/231/commits/8d51626d8f2a3f95ee31b380caeac225a570e4f8):**

Trial run and verifying with other open source implementations: https://colab.research.google.com/drive/1fo0jLJogVW-fooO6Nk6A8-dPPppxZxYl?usp=sharing
",True,231,https://api.github.com/repos/keras-team/keras-nlp/pulls/231,https://github.com/keras-team/keras-nlp/pull/231,closed,529,4,7,11,0,12,0,0,[],2022-06-19 09:42:22+00:00,2022-07-01 19:21:29+00:00,1071547.0,"12 days, 9:39:07","[{'comment_id': 911325794, 'comment_body': 'I know we keep copying this around, but I don\'t think we need to keep listing our parent class in the module level docstring. Maybe just ""Edit distrance metric.""', 'comment_created': datetime.datetime(2022, 6, 30, 18, 19, 21, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911325949, 'comment_body': 'Edit distance metric.', 'comment_created': datetime.datetime(2022, 6, 30, 18, 19, 32, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911327497, 'comment_body': 'This class implements edit distance, sometime called Levenshtein Distance, as a `tf.keras.Metric`.\r\n\r\nbackticked symbol names will get autolinked in our keras.io docs. So listing key symbols will help users navigate.', 'comment_created': datetime.datetime(2022, 6, 30, 18, 21, 14, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911327879, 'comment_body': 'replace the ""/"" with the word ""and""', 'comment_created': datetime.datetime(2022, 6, 30, 18, 21, 40, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911330644, 'comment_body': 'It looks like https://www.researchgate.net/publication/221478089 works as a slightly more abbreviated link', 'comment_created': datetime.datetime(2022, 6, 30, 18, 24, 43, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911331734, 'comment_body': 'By default, this metric will compute a normalized version, where....', 'comment_created': datetime.datetime(2022, 6, 30, 18, 25, 54, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 911333484, 'comment_body': 'I think the 1. and 1.1. and a. is a little confusing and will probably be hard to maintain. Can we just have unordered titles?\r\n\r\nRank 1 input.\r\n...\r\n\r\nUsing `model.compile()`.', 'comment_created': datetime.datetime(2022, 6, 30, 18, 27, 48, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 912173401, 'comment_body': 'nit: above => edit distance for clear reference.', 'comment_created': datetime.datetime(2022, 7, 1, 18, 28, 57, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 912182204, 'comment_body': 'Curious - is the return value required for `map_fn`? seems this return value is not being used', 'comment_created': datetime.datetime(2022, 7, 1, 18, 37, 59, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 912185463, 'comment_body': 'It can also be list?', 'comment_created': datetime.datetime(2022, 7, 1, 18, 42, 58, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 912188803, 'comment_body': ""Yep, but I think we've stuck to specifying TensorFlow data structures in all other layers. For example, https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/tokenizers/sentence_piece_tokenizer.py#L35-L40 and https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/metrics/rouge_l.py#L29-L31."", 'comment_created': datetime.datetime(2022, 7, 1, 18, 48, 10, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 912190879, 'comment_body': ""The issue is that we can't return None. This is the function spec for `tf.map_fn`:\r\n\r\n```\r\ntf.map_fn(\r\n    fn,\r\n    elems,\r\n    dtype=None,\r\n    parallel_iterations=None,\r\n    back_prop=True,\r\n    swap_memory=False,\r\n    infer_shape=True,\r\n    name=None,\r\n    fn_output_signature=None\r\n)\r\n```\r\n\r\nIn particular, have a look at `fn_output_signature`:\r\n```\r\nThe output signature of fn. Must be specified if fn's input and output signatures are different (i.e., if their structures, dtypes, or tensor types do not match). fn_output_signature can be specified using any of the following:\r\nA tf.DType or tf.TensorSpec (to describe a tf.Tensor)\r\nA tf.RaggedTensorSpec (to describe a tf.RaggedTensor)\r\nA tf.SparseTensorSpec (to describe a tf.sparse.SparseTensor)\r\nA (possibly nested) tuple, list, or dict containing the above types.\r\n```"", 'comment_created': datetime.datetime(2022, 7, 1, 18, 51, 32, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}]","[{'commit_sha': 'a1f4e23c39e6e9a05334acc23a2053bf5ec30dfe', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0981a0f6271c36ef1799a468b5d3c597f1aec641', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a514c886d4fa3ed91aa280869c12dd0d3919e0ae', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8970c39149334d7fa0ab4f784059a2a652482b8e', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8d51626d8f2a3f95ee31b380caeac225a570e4f8', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3a9e30d995372f0d3c01727d35093c9660566d4d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e57aa3884ef4b47f1cde261caaf9b65dca9f4369', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd86b3ee96d54882833f58207dafafead893b866d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '44cf82cacbd92b13c138663ccb4530f9ad4d19fd', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3168c320dabcc3ef6f109720c828ba3f00796589', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cc95007c51d2023c57bcf55b9c53ab276336dd77', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
896186721,Add Byte Tokenizer,"Resolves #79.

Hello, @mattdangerw, @chenmoneygithub! This PR is ready for review. Thanks!",True,80,https://api.github.com/repos/keras-team/keras-nlp/pulls/80,https://github.com/keras-team/keras-nlp/pull/80,closed,523,0,3,16,3,40,0,0,[],2022-03-31 17:51:24+00:00,2022-04-07 17:59:32+00:00,605288.0,"7 days, 0:08:08","[{'comment_id': 840088784, 'comment_body': 'move everything in a tab from the Args section', 'comment_created': datetime.datetime(2022, 3, 31, 22, 52, 6, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840091055, 'comment_body': 'dtype will be set on the layer, just access it as `self.compute_dtype`', 'comment_created': datetime.datetime(2022, 3, 31, 22, 57, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840094291, 'comment_body': ""I don't think you are actually demonstrating a invalid byte sequence. You are just showing the that ï¿½ will map back and forth which is what's expected without replacement. Example colab shows and actual invalid sequence."", 'comment_created': datetime.datetime(2022, 3, 31, 23, 5, 46, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840094574, 'comment_body': 'I think given that we can see what ""hello"" maps to above, we should rewrite this with just the detokenize call, and take as input an integer list.', 'comment_created': datetime.datetime(2022, 3, 31, 23, 6, 33, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840094873, 'comment_body': 'add a comment that this is to fix errors.', 'comment_created': datetime.datetime(2022, 3, 31, 23, 7, 19, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840095068, 'comment_body': 'For properties like this that were passed in, keep these public. `self.lowercase`', 'comment_created': datetime.datetime(2022, 3, 31, 23, 7, 51, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840095461, 'comment_body': ""I think we should leave token_to_id and id_to_token off here. I can't think of a great use case, and it's confusing what to do if the string is multiple bytes. Simpler to just leave this out."", 'comment_created': datetime.datetime(2022, 3, 31, 23, 8, 54, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840095911, 'comment_body': 'If we remove a few ""squares"" from that input, can we get this to format as one line?', 'comment_created': datetime.datetime(2022, 3, 31, 23, 10, 1, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840096151, 'comment_body': 'Same here, prefer a shorter example that formats to one line.', 'comment_created': datetime.datetime(2022, 3, 31, 23, 10, 35, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840097584, 'comment_body': 'come up with a more descriptive name than _1 and _2 here, or fold into one test.', 'comment_created': datetime.datetime(2022, 3, 31, 23, 14, 21, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840098827, 'comment_body': 'We should actually do the same fix here as https://github.com/keras-team/keras-nlp/issues/49 here and document it.\r\n\r\nSo if we get a scalar, rank 0 string as input and sequence length is not set, output a dense rank 1 tensor with None as the static shape.', 'comment_created': datetime.datetime(2022, 3, 31, 23, 17, 26, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840099445, 'comment_body': ""Let's not include this, I don't think there's a use case really."", 'comment_created': datetime.datetime(2022, 3, 31, 23, 19, 4, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840099584, 'comment_body': 'I would actually just return 256 here, so this is really obvious to people reading the code.', 'comment_created': datetime.datetime(2022, 3, 31, 23, 19, 28, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840099830, 'comment_body': 'why not just make this a constant in init?', 'comment_created': datetime.datetime(2022, 3, 31, 23, 20, 3, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840200836, 'comment_body': 'Makes sense. Changes made!', 'comment_created': datetime.datetime(2022, 4, 1, 3, 23, 24, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 840201288, 'comment_body': ""Ah, interesting. Didn't know that. Thanks!"", 'comment_created': datetime.datetime(2022, 4, 1, 3, 24, 48, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 840201852, 'comment_body': 'Sure, removed this ðŸ‘ðŸ¼ ', 'comment_created': datetime.datetime(2022, 4, 1, 3, 26, 34, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 840202428, 'comment_body': ""Yeah, this was done because `get_vocabulary()` returns a list. But now that we've decided to remove it, I'll make it a constant in `__init__()`."", 'comment_created': datetime.datetime(2022, 4, 1, 3, 28, 31, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 841967878, 'comment_body': 'Should we set the input as an integer sequence? This example is now decoding ""hello"" to ""hello"", which looks strange.', 'comment_created': datetime.datetime(2022, 4, 4, 17, 12, 5, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 841968153, 'comment_body': 'Why is this an invalid input that triggers replacement? ', 'comment_created': datetime.datetime(2022, 4, 4, 17, 12, 27, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 841984615, 'comment_body': 'There is a minor problem here - if any inputs is out of range, then the error message will be thrown by tf.gather, which is a bit vague. Maybe add a check, or a try catch block? ', 'comment_created': datetime.datetime(2022, 4, 4, 17, 33, 19, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 841986737, 'comment_body': 'Add a comment why this is invalid. ', 'comment_created': datetime.datetime(2022, 4, 4, 17, 36, 21, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 842201350, 'comment_body': ""It might be good to add a line..\r\n\r\n```\r\nif not isinstance(inputs, (tf.Tensor, tf.RaggedTensor)):\r\n    inputs = tf.convert_to_tensor(inputs)\r\n```\r\n\r\nThat would allow us to call this directly on non-tensors I think. E.g. `tokenize('test')` or `tokenize(['test'])`"", 'comment_created': datetime.datetime(2022, 4, 4, 22, 58, 29, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 842207518, 'comment_body': 'A string scalar constant can still be a tensor, so this this is a little misleading. Maybe say If the input is a batch of strings instead?', 'comment_created': datetime.datetime(2022, 4, 4, 23, 12, 6, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 844198795, 'comment_body': 'Right. Changed ðŸ‘ðŸ¼ ', 'comment_created': datetime.datetime(2022, 4, 6, 17, 21, 4, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 844203454, 'comment_body': ""Ah, yes. Sorry, I saw Matt's comment and forgot to change it. I've changed it now. Thank you for noticing!"", 'comment_created': datetime.datetime(2022, 4, 6, 17, 26, 38, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 844219309, 'comment_body': ""Matt's notebook: https://colab.sandbox.google.com/gist/mattdangerw/99e8f3795e37fe539731dcbfc1c09d47/bytetokenizer.ipynb\r\n\r\nI think it is an invalid UTF-8 character?"", 'comment_created': datetime.datetime(2022, 4, 6, 17, 42, 23, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 844224091, 'comment_body': 'Ah, right. Added!', 'comment_created': datetime.datetime(2022, 4, 6, 17, 47, 55, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 844224249, 'comment_body': 'Added a UT as well', 'comment_created': datetime.datetime(2022, 4, 6, 17, 48, 6, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 844240566, 'comment_body': 'Ah, makes sense. This is the new error thrown after adding a try...except block:\r\n\r\n```\r\nValueError: Exception encountered when calling layer ""byte_tokenizer"" (type ByteTokenizer).\r\n\r\nAll entries in `inputs` must lie in the range [0, 256). Values at indices [[0 0]\r\n [0 2]\r\n [0 3]] lie outside this range.\r\n```\r\nHope this is fine.', 'comment_created': datetime.datetime(2022, 4, 6, 18, 6, 29, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 844431272, 'comment_body': ""yeah, it's an invalid byte sequence for utf-8, should be fine to leave as is."", 'comment_created': datetime.datetime(2022, 4, 6, 21, 18, 58, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 844442491, 'comment_body': 'Missing space before Specifies', 'comment_created': datetime.datetime(2022, 4, 6, 21, 36, 8, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 844445025, 'comment_body': 'You could just do a simple replacement character here. Replacement char=88. Assert output ""heXllo"".', 'comment_created': datetime.datetime(2022, 4, 6, 21, 40, 1, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 844453293, 'comment_body': 'Actually I think we could make this clearer if we decode the tensors back to python strings here.\r\n\r\nI added a commit with a suggestion to this branch, a long with a few other minor improvements.', 'comment_created': datetime.datetime(2022, 4, 6, 21, 53, 48, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 844454617, 'comment_body': ""I think when errors='ignore', we can actually skip this call right? It won't do anything."", 'comment_created': datetime.datetime(2022, 4, 6, 21, 56, 4, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 844456997, 'comment_body': ""Actually I don't think a try/catch will work for function tracing. If you, for example, baked this into a model, you would only see this error if your bad data was in the very first batch of input.\r\n\r\nI'd suggest we revert to what we had (no try/catch) and open up a follow up issue for figuring this out. We could do this with `tf.debugging.Assert`, but it would come with a performance cost."", 'comment_created': datetime.datetime(2022, 4, 6, 22, 0, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 844563068, 'comment_body': 'My bad. Fixed ðŸ‘ðŸ¼ ', 'comment_created': datetime.datetime(2022, 4, 7, 1, 40, 35, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 844564609, 'comment_body': 'ðŸ‘ðŸ¼ . Removed it for now', 'comment_created': datetime.datetime(2022, 4, 7, 1, 45, 6, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 844567683, 'comment_body': 'Hey, ""ignore"" removes the invalid characters. So, we should keep it, I think', 'comment_created': datetime.datetime(2022, 4, 7, 1, 54, 6, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 844585950, 'comment_body': 'Yup you are right. Thanks!', 'comment_created': datetime.datetime(2022, 4, 7, 2, 44, 10, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '33c0ebfeb6a5d32de566f3061f6b420bb2f47d13', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3a940990857b20e8b7437dcab2eb2966d159d247', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1849b79839a78a46bbefaee94fd337648e8d3477', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fe42165c46086ab44c6bc6f8a9df4a6cae951222', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3540ebfd5b9b77dd39db6cd76f3dea6329e62266', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0ed957bea94b0e3b4b08be69d55d88a782f25577', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fae4fddfed29ee30619bc098bcc6b165752926c4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9fbc39433753fab6d4f1bcf2df0bab9ceb3beb69', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '70c60de1afc34ca971a751f6eb6554b45a04de1d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5080e3b3a64e4c6d86255c2e8cdd7ea4fd93b29e', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1a1915c5a7c41614fc2dbcbd8212417873ba97d4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '41cc24f3561fb422f14df04aa0b1d0a08de5105f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4896e38e9380af93228dea567a3c2a2d27fa57f4', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6185f6344642e5755ac8e5153cdcff609f7b00f4', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8f05607ea894247a2eaf4a5d0a66d1ee46367c36', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'be6056cc0c89a592cd33953e50e06e5a2c67fe3a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1038572431,Add Support for CJK Char Splitting for WordPiece Tokenizer,"Resolves https://github.com/keras-team/keras-nlp/issues/307

A few comments:

- [CJK regex](https://github.com/google-research/bert/blob/master/tokenization.py#L274-L281) was already present in [`PUNCTUATION_REGEX`](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/tokenizers/word_piece_tokenizer.py#L46-L54). Shifted it to a new variable, and added an arg for splitting on CJK (`split_on_cjk`).
- Made similar changes in WordPiece trainer.
",True,318,https://api.github.com/repos/keras-team/keras-nlp/pulls/318,https://github.com/keras-team/keras-nlp/pull/318,closed,97,7,4,10,0,10,0,0,[],2022-08-27 06:43:35+00:00,2022-08-29 20:24:49+00:00,222074.0,"2 days, 13:41:14","[{'comment_id': 957614807, 'comment_body': ""Let's add a link to the place we got this from in the BERT repo. It's a good reference to track."", 'comment_created': datetime.datetime(2022, 8, 29, 17, 31, 37, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 957615356, 'comment_body': 'nit: add a trailing comma so this formats to multiple lines', 'comment_created': datetime.datetime(2022, 8, 29, 17, 32, 13, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 957633977, 'comment_body': 'Wow interesting, it looks like we were already splitting on these. I think in that case we could actually avoid this `regex_replace` call and just keep two different split regexes.\r\n\r\n`WHITESPACE_AND_PUNCTUATION_REGEX` and `WHITESPACE_PUNCTUATION_AND_CJK_REGEX`\r\n\r\n```\r\nif split:\r\n    if split_on_cjk:\r\n         split_pattern = WHITESPACE_PUNCTUATION_AND_CJK_REGEX\r\n    else:\r\n         split_pattern = WHITESPACE_AND_PUNCTUATION_REGEX\r\n    text = tf_text.regex_split(...)\r\n```\r\n\r\nWould this work?', 'comment_created': datetime.datetime(2022, 8, 29, 17, 44, 57, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 957665079, 'comment_body': ""Hmmm, yeah. But we'll have to create a third REGEX, namely, `PUNCTUATION_AND_CJK_REGEX` to pass to `keep_delim_regex_pattern`. Isn't it easier to just keep `tf.regex_replace()`, then? What do you think?"", 'comment_created': datetime.datetime(2022, 8, 29, 18, 22, 11, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 957665298, 'comment_body': ""Pushing changes for now, let me know if it's better to revert back to the original."", 'comment_created': datetime.datetime(2022, 8, 29, 18, 22, 26, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 957667615, 'comment_body': 'delim_regex_pattern I guess? since you are agreeing with the other arg name', 'comment_created': datetime.datetime(2022, 8, 29, 18, 25, 22, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 957668466, 'comment_body': 'This looks fine to me. Original is simpler, but we probably should care about efficiency here (and cutting an op should help that).', 'comment_created': datetime.datetime(2022, 8, 29, 18, 26, 23, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 957669391, 'comment_body': 'Hehe, I just changed it to `keep_split_pattern` :P', 'comment_created': datetime.datetime(2022, 8, 29, 18, 27, 29, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 957670258, 'comment_body': 'Cool!', 'comment_created': datetime.datetime(2022, 8, 29, 18, 28, 33, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 957674639, 'comment_body': 'that works too!', 'comment_created': datetime.datetime(2022, 8, 29, 18, 33, 33, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '4a8344117b6b89507299f7463f0baee05aca3a4b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2fc078612383419ea20a883b9bd7e0b7e374edda', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ead2d7a3d0a150dcea064b2ecd186b2eb79e02a9', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '912f11a522aa459716cbf942dfddb00f1af88678', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c9eaa7ea6fea59d45644c487874ec34098b96063', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ca0816c8b9cb16d8ad9f7a154355f09f736a6b22', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '04918c83f5c194e9bc9855240d7a5ee9c7bdcf96', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '76e3a72af366b84fc88a1b8fd058d8a234bdf375', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '006c069d45fe55b9868f3a00a9ba91255db5e574', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd3edab30e792cc7272239f20241c0cc2b2be3163', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1038637660,"`bert_base_zh`, `bert_base_multi_cased`: Add BERT Base Variants","Resolves https://github.com/keras-team/keras-nlp/issues/308

Converted model weights and vocab have been uploaded here: https://drive.google.com/drive/folders/19APUi7fobORdQjoe8YDyk8ou0x_L6hWu?usp=sharing.

Edit: Merging https://github.com/keras-team/keras-nlp/pull/320 with this PR. Might cause conflicts while merging with `master`, otherwise.",True,319,https://api.github.com/repos/keras-team/keras-nlp/pulls/319,https://github.com/keras-team/keras-nlp/pull/319,closed,2026,16,4,17,2,1,0,0,[],2022-08-27 13:44:25+00:00,2022-09-01 19:44:12+00:00,453587.0,"5 days, 5:59:47","[{'comment_id': 960161007, 'comment_body': ""This is using a variable (i) that is not out of scope. Can't you just do num_layers + X?\r\n\r\nSame comment for other colab."", 'comment_created': datetime.datetime(2022, 9, 1, 2, 16, 32, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '04342e5f291b82be7ef25530d1a0e6270ac6fcc3', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c879aa6d725bb38a3e235b2efc95cb31eaffb0ff', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9f1f2de73547c751e72ead67e0aff7642adafcc6', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3536d1543b0228b414c612104d75d82bf4d10ddc', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd5f57f7f4c5b35db1a7de206694556f6d197cb42', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2c4b3c56e474145f3504cf2664b8fb8c09e73b5f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3dc5653695a3534ef07b8bb69a166ff8cfa4644f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7c3716cbefe80975dbf7ad7d8efeaf7a63780043', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b96f6c6b948fffe1fc4780e128a5811484cdda63', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd5a089864d597450fed371153262c107e54fae70', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '787762cd38a33110e4d0a8deb051157db3d83124', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd3086063d586d09d3189affafb896cc4b820b97b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b84400cffc57a236482500b68c527763160f85a1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dd88964d43a8b73d780521897491681fe3e450ca', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f02114811ffeb3829838a0a5eb64e734ccc5ca36', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3b615d21c47a87e56de11189f70e895eb03e1b9c', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '85113961ab68a30044790bb19bd6b452200edd8b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1042377413,Add BERT Large,Resolves https://github.com/keras-team/keras-nlp/issues/334,True,331,https://api.github.com/repos/keras-team/keras-nlp/pulls/331,https://github.com/keras-team/keras-nlp/pull/331,closed,2554,50,5,23,1,8,0,0,[],2022-08-31 13:12:08+00:00,2022-09-08 18:06:37+00:00,708869.0,"8 days, 4:54:29","[{'comment_id': 965180021, 'comment_body': 'Given that we are going to be doing this for BertMedium, BertSmall, etc. on a subsequent PR, it probably does make sense to refactor this a little.\r\n\r\nMy vote, rather than a big function that takes in every bert hyperparameter, would be to split off a function to validate and fetch the proper `weights` and `vocabulary_size`. Then the body of each of these can look something like.\r\n\r\nNote that weights should be the local filepath after the call to `_handle_weights_and_vocab_size`, and all error messages can live in this function.\r\n\r\n```python\r\nweights, vocabulary_size = _handle_weights_and_vocab_size(""bert_base"", weights, vocabulary_size)\r\n\r\nmodel = BertCustom(\r\n    vocabulary_size=vocabulary_size,\r\n    num_layers=24,\r\n    num_heads=16,\r\n    hidden_dim=1024,\r\n    intermediate_dim=4096,\r\n    dropout=0.1,\r\n    max_sequence_length=512,\r\n    name=name,\r\n    trainable=trainable,\r\n)\r\n\r\nif weights:\r\n    model.load_weights(weights)\r\n\r\nreturn model\r\n```', 'comment_created': datetime.datetime(2022, 9, 7, 19, 1, 56, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965197380, 'comment_body': 'I think the MODEL_DOCSTRING, still hardcodes BertBase in the usage examples, could you try formatting large in there?', 'comment_created': datetime.datetime(2022, 9, 7, 19, 26, 15, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965424048, 'comment_body': 'I would just call this `weights`, to match the `load_weights` function. Conceptually you are getting the ""updated"" versions of both these parameters after checking for pretrained names. And at some point we may allow the input `weights` to be a filepath as well.', 'comment_created': datetime.datetime(2022, 9, 8, 1, 30, 23, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965424831, 'comment_body': 'Just move this whole todo into the helper function next to the `get_file` call. `load_weights` already supports tf checkpoints, so we really would change this inside the helper not here I believe.', 'comment_created': datetime.datetime(2022, 9, 8, 1, 32, 38, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965429262, 'comment_body': 'Re comment on other PR, why not a single parameterize test for bert.\r\n\r\n```\r\n@parameterized.named_parameters(\r\n    (""base_uncased_en"", keras_nlp.models.BertBase, ""uncased_en""),\r\n    (""large_uncased_en"", keras_nlp.models.BertLarge, ""uncased_en""),\r\n\r\n)\r\ndef test_load_bert_base(self, model, weights):\r\n```\r\n\r\nThat feels readable, and easily maintainable.', 'comment_created': datetime.datetime(2022, 9, 8, 1, 44, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965440627, 'comment_body': 'small docstring, since this is a little complex?\r\n\r\n""""""\r\nLookup pretrained defaults for weights and vocabulary_size.\r\n\r\nThe helper will validate the weights and vocabulary_size argument, and fully\r\nresolve them in the case we are loading pretrained weights.\r\n""""""', 'comment_created': datetime.datetime(2022, 9, 8, 2, 14, 47, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 966190045, 'comment_body': 'Make sure to search and replace this across the entire file if you want this change.', 'comment_created': datetime.datetime(2022, 9, 8, 16, 44, 36, tzinfo=datetime.timezone.utc), 'commenter': 'jbischof', 'type': 'User'}, {'comment_id': 966199470, 'comment_body': 'nit: These are not mg checkpoints anymore. Maybe `original_pooled_output`? You can search and replace without redoing the notebook.', 'comment_created': datetime.datetime(2022, 9, 8, 16, 55, 10, tzinfo=datetime.timezone.utc), 'commenter': 'jbischof', 'type': 'User'}]","[{'commit_sha': 'db9f7b9f67778bdaedd4e8ed8500a984a473ca2c', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ec25b7f0b7b67a02c0e44ec7423191fbb06bb8fa', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4e8e6d3cd902a65cc4ec0efc706e50a5afa4fd66', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7a2e241fb7d971ef90d96f77296afadb38025c1c', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ec10e218611383371164043ad919fdb56ca05e42', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ebd63efc4c97d16f5feac960c1d4a5f222b45828', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c40d4058c81ef9ed0c645d9292396ef95bf7ba70', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5160aff8f3f5ce47a5c3c7d14e09e352dbc2b56f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a86ee2622ddf5065ed62eac19e214f44b05a8c20', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4a42390051314619b16fa5e6c5023fd8e4e351d6', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'da994c89b04c75aff1b2ff1564da90bf47847775', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'adfa318380331e74a05dbc801822a3f1e7253b23', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '91a5e0d309db51aba34c9fe79147e40075dc7d2e', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e5da03969400447ca6f72a270b00605353963fb7', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '05233a1609533f9479e455558821df3d0c8184fa', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6154390ec79e419f9e1745a2c904cae0de816332', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a8ca6ee45e7b4745dc34411797e72424a51a3170', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '45326bc0b8ad0525ab6d2552c329fd93d272f6ee', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9fb39d2dd45f7a7e3eb578c76f77a263a6905c02', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c6bcf9ce80249ac7717612a6a137dee67e2a26f0', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bbeac31455cd1c1f9ff2d3dca8c0a831a65e1094', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '11bcdfb76d0717f54ee670776fe26ce449d921c4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b9d36bd1064e9f4fb971f5d3ce5f78a09726835d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1045273951,Add Small BERT Variants,Resolves #336 ,True,338,https://api.github.com/repos/keras-team/keras-nlp/pulls/338,https://github.com/keras-team/keras-nlp/pull/338,closed,2672,2,6,16,1,1,0,0,[],2022-09-02 20:52:43+00:00,2022-09-09 23:43:22+00:00,615039.0,"7 days, 2:50:39","[{'comment_id': 965199427, 'comment_body': 'Could we just make one big parameterized test? Might actually improve the readability overall, and save some lines.\r\n\r\n    @parameterized.named_parameters(\r\n        (""base_uncased_en"", keras_nlp.models.BertBase, ""uncased_en""),\r\n    )\r\n    def test_load_bert_base(self, model, weights):', 'comment_created': datetime.datetime(2022, 9, 7, 19, 29, 1, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '6666a6a272908c60f759bd264c4a7ab8cf8d23cf', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '93a0f35d2f942ec33009b6eb0a0fb8edb599934f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '65b6c9e82e9ba12b607cc9470d4c0f5ae284ed67', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fb5a0e3629e6494868550dadfd4350aa333f4f48', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '416ad14533e4a8cc6b96c48cdd16d571ea3aad54', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8be40af9f12ef7ad355a5deb29d27aa1168118de', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0f800601ba9c22bfe522b0a8a69936734c989dd0', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bf3c80de23fe266c6e5a30b6e53ab0adfc1ae95d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '66dd054f1076b4add184b21ab8b57500ece709b8', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'eb6225f16bae020cadca8e99b7b53ff8f7706642', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0374e019c3c4406d809b9ed2c48a5bc12ffcf849', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '764336c008c230a37bf0e7c6562ebac7d87bae8a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ef511a51135bd36e2fc2fde3488aa4f1c2dbee62', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2a0434ded345ca83f0b3db9d1099a4499c27df71', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ea9067e5ffc4d726ad536505358dae08a0699f0a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '20bba107640277e8b46c7f1009787514d9b5002b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1042294723,"BERT, RoBERTa: Add `model.compile` UTs",Resolves #325 ,True,330,https://api.github.com/repos/keras-team/keras-nlp/pulls/330,https://github.com/keras-team/keras-nlp/pull/330,closed,86,12,2,5,0,3,0,0,[],2022-08-31 11:52:11+00:00,2022-08-31 21:03:00+00:00,33049.0,9:10:49,"[{'comment_id': 959816492, 'comment_body': 'You should be able to `from_tensor_slices` directly from a dict I think? We could probably make this more concise.\r\n\r\nWould `self.input_dataset = tf.data.Dataset.from_tensor_slices(input_data).repeat(5)` work?', 'comment_created': datetime.datetime(2022, 8, 31, 16, 55, 5, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 959818006, 'comment_body': ""Also, let's maybe clean up the naming of these two input variables for clarity? `self.input_batch` for the one and `self.input_dataset` for this one maybe?"", 'comment_created': datetime.datetime(2022, 8, 31, 16, 56, 43, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 959887568, 'comment_body': 'Ah, yes. It works on dicts. Did not know that.\r\n\r\n```\r\ninput_batch = {\r\n    ""token_ids"": tf.ones(\r\n        (8, 512), dtype=""int32""\r\n    ),\r\n    ""segment_ids"": tf.ones(\r\n        (8, 512), dtype=""int32""\r\n    ),\r\n    ""padding_mask"": tf.ones(\r\n        (8, 512), dtype=""int32""\r\n    ),\r\n}\r\n\r\ninput_dataset = tf.data.Dataset.from_tensor_slices(input_batch).batch(2)\r\n\r\nfor e in input_dataset:\r\n    print(e[""token_ids""].shape)\r\n```\r\n\r\n```\r\n(2, 512)\r\n(2, 512)\r\n(2, 512)\r\n(2, 512)\r\n```', 'comment_created': datetime.datetime(2022, 8, 31, 18, 19, 16, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}]","[{'commit_sha': 'eb6a252c0d43f6859d78d7dec1fad8af55aab22d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dc79ab3579343e86fcbca3d5924945841a8cbff7', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7642b5830d0310fb8ea798d0f514e15707897bdc', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a6bda677a5f9f1a055d4a81d20459e786b9c3fca', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4b3206c637ab1c3ce95cc8d01f162354d9487cc0', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1049689870,Add normalize_first arg to Transformer Layers,"Partially resolves #337 

For the `TransformerDecoder` layer, I went ahead and normalized the input to the cross-attention layer as well (other than self-attention and feedforward layers). Considering `normalize_first` for the self-attention layer and feedforward layer and not considering it for the cross-attention layer seemed incomplete.

Some references for the above:

- https://fossies.org/linux/tensorflow-official-models/official/projects/detr/modeling/transformer.py (LN 818-819)
- PyTorch (check `class TransformerDecoderLayer`): https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#TransformerDecoderLayer
```
        if self.norm_first:
            x = x + self._sa_block(self.norm1(x), tgt_mask, tgt_key_padding_mask)
            x = x + self._mha_block(self.norm2(x), memory, memory_mask, memory_key_padding_mask)
```
- https://github.com/seetaresearch/dragon/blob/494774d3a545f807d483fd9e6e4563cedec6dda5/torch/core/nn/modules/transformer.py#L183-L184",True,350,https://api.github.com/repos/keras-team/keras-nlp/pulls/350,https://github.com/keras-team/keras-nlp/pull/350,closed,85,35,4,4,2,8,0,0,[],2022-09-08 07:50:57+00:00,2022-09-09 18:53:15+00:00,126138.0,"1 day, 11:02:18","[{'comment_id': 966127026, 'comment_body': 'I think we should probably get rid of `_add_and_norm`, especially with this change I think it makes the code less readable not moreso.', 'comment_created': datetime.datetime(2022, 9, 8, 15, 40, 29, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 966134241, 'comment_body': 'Maybe toss this in the config and saving tests? No need to parameterize, just set this to `True` and then check it is preserved. Same for decoder.', 'comment_created': datetime.datetime(2022, 9, 8, 15, 47, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 966148266, 'comment_body': ""We should not have variables that only exist inside concurrent if blocks, that's confusing. We should probably have something more like.\r\n\r\n```\r\ninput_decoder_sequence = decoder_sequence\r\nif self.normalize_first:\r\n    decoder_sequence = self._decoder_attention_layernorm(decoder_sequence)\r\n\r\n...\r\n\r\nattention_output = input_decoder_sequence + self_attended\r\nif not self.normalize_first:\r\n    attention_output = self._decoder_attention_layernorm(attention_output)\r\n```"", 'comment_created': datetime.datetime(2022, 9, 8, 16, 0, 44, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 966465199, 'comment_body': 'Move the ""Defaults to False."" right after ""bool."", that will match the other other args in this file.', 'comment_created': datetime.datetime(2022, 9, 8, 22, 9, 48, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 966466018, 'comment_body': 'feedforward -> feed_forward in this name to match others', 'comment_created': datetime.datetime(2022, 9, 8, 22, 11, 14, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 966468805, 'comment_body': 'same here, default sentence after bool sentence', 'comment_created': datetime.datetime(2022, 9, 8, 22, 17, 2, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 966525222, 'comment_body': '""Feedforward"" is one word. So, I\'ll change `feed_forward` to `feedforward`.', 'comment_created': datetime.datetime(2022, 9, 9, 0, 35, 16, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 966546076, 'comment_body': 'Sg! As long as we are consistent (preferably across decoder and encoder).', 'comment_created': datetime.datetime(2022, 9, 9, 1, 31, 57, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '98b25f8919a704aa57055f917f11619d7c7ae084', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0333f080dc41388b7071dbe5df6c282625334e2a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd1320e5bea426827f26c5539ad7c5b800e7d073a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3dc6344d818f4f598b6505e29efe39551a95de43', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
995799943,Make Decoding Functions Graph-compatible (with XLA Support!),"Resolves #241 

Partially resolves https://github.com/keras-team/keras-nlp/issues/277


- [x] Greedy Search
- [ ] Beam Search (will probably open a separate PR for this)
- [x] Top-p Search
- [x] Top-k Search
- [x] Random Search

Will have to think a bit more about Beam Search.",True,271,https://api.github.com/repos/keras-team/keras-nlp/pulls/271,https://github.com/keras-team/keras-nlp/pull/271,closed,421,72,2,12,7,21,0,0,[],2022-07-13 18:10:53+00:00,2022-08-16 19:15:24+00:00,2941471.0,"34 days, 1:04:31","[{'comment_id': 926118128, 'comment_body': 'would just using mode.predict work? that would still hit all the compiled function paths, and allow you to avoid all this dummy metric stuff, which is hard to read', 'comment_created': datetime.datetime(2022, 7, 20, 23, 0, 21, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 926123379, 'comment_body': 'Can we just pass `tf.TensorShape([None, None])` as the shape invariant? Generally we should support a static batch size of `None`, tf data does this by default after calling `.batch()` for example. Might simplify the code a bit.', 'comment_created': datetime.datetime(2022, 7, 20, 23, 13, 7, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 926124447, 'comment_body': 'Could also be good to test the call on a batched dataset (where batch size not statically known), and on a single constant input, as you are doing here.', 'comment_created': datetime.datetime(2022, 7, 20, 23, 14, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 926125509, 'comment_body': 'If you add `jit_compile=True` does the test still pass?\r\n\r\nIf yes, we should test this with both jit_compile=True and False, using https://docs.pytest.org/en/6.2.x/parametrize.html\r\n\r\nIf no, we should either try to fix things with jit_compilation, or make sure we track that on a follow up issue.', 'comment_created': datetime.datetime(2022, 7, 20, 23, 17, 28, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 927004701, 'comment_body': 'Ah, man. Stupid me. Should have used `model.predict` :P', 'comment_created': datetime.datetime(2022, 7, 21, 18, 49, 16, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 927078312, 'comment_body': ""@abheesht17 Let's try add a test case for `jit_compile=True`, and we can run it on GPU. We recently add GPU test support in this repo."", 'comment_created': datetime.datetime(2022, 7, 21, 20, 31, 51, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 927713036, 'comment_body': ""It's not working with `jit_compile = True`. Complete error logs: https://p.ip.fi/2TNt.\r\n\r\nLooks like it won't work with `shape_invariants`."", 'comment_created': datetime.datetime(2022, 7, 22, 14, 32, 47, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 942715834, 'comment_body': 'can we avoid the state dict and just do `loops_vars=(length, prompt)` here? might be a little more readable', 'comment_created': datetime.datetime(2022, 8, 10, 17, 25, 28, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 942767508, 'comment_body': 'I feel like we should be able to make this simpler, we are just padding a batched tensor with pad_token_id to the sequence length right? We should not need a map_fn for this', 'comment_created': datetime.datetime(2022, 8, 10, 18, 24, 54, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 942769828, 'comment_body': 'do we even need this function anymore, if we are just starting with the correct sized tensor filled with pad_token_id?', 'comment_created': datetime.datetime(2022, 8, 10, 18, 27, 46, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943507082, 'comment_body': 'hmm I guess we do to avoid random tokens after the end_token_id', 'comment_created': datetime.datetime(2022, 8, 11, 13, 47, 51, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943540643, 'comment_body': 'Yep', 'comment_created': datetime.datetime(2022, 8, 11, 14, 16, 56, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 943752675, 'comment_body': 'nit: maybe split this into two lines for readability?\r\n\r\npadding = tf.fill((tf.shape(prompt)[0], max_length - length), pad_token_id)\r\nprompt = tf.concat((prompt, padding), axis=-1)', 'comment_created': datetime.datetime(2022, 8, 11, 17, 40, 59, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943756401, 'comment_body': 'can we just do something like\r\n```\r\nbatch_size, length = tf.shape(x)\r\n```\r\nAnd use that below? Then length and batch size are both tensors from the start.', 'comment_created': datetime.datetime(2022, 8, 11, 17, 45, 17, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943758138, 'comment_body': 'length, prompt = tf.while_loop(...)\r\n\r\njust to avoid that `[1]` which is not super readable', 'comment_created': datetime.datetime(2022, 8, 11, 17, 47, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943759661, 'comment_body': 'is there a reason you have to do training switch here? it looks like you are never actually testing the training=True branch, might be nice to clean up the test a bit', 'comment_created': datetime.datetime(2022, 8, 11, 17, 48, 54, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943772820, 'comment_body': ""Hmmm, I'll split this into two lines:\r\n\r\n```\r\nbatch_size = tf.shape(prompt)[0]\r\nlength = tf.shape(prompt)[1]\r\n```\r\n\r\n```\r\nbatch_size, length = tf.shape(x)\r\n```\r\ndoes not work in graph mode."", 'comment_created': datetime.datetime(2022, 8, 11, 18, 4, 50, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 943775264, 'comment_body': 'Stack trace: https://p.ip.fi/6YAg', 'comment_created': datetime.datetime(2022, 8, 11, 18, 5, 32, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 943856422, 'comment_body': ""ah destructuring is too fancy for autograph, I forgot. let's do\r\n\r\nshape = tf.shape(prompt)\r\nbatch_size = shape[0]\r\nlength = shape[1]"", 'comment_created': datetime.datetime(2022, 8, 11, 19, 29, 43, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 945981794, 'comment_body': 'Do we need to change the number here? The original value seems to be more general?', 'comment_created': datetime.datetime(2022, 8, 15, 17, 42, 6, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 946204029, 'comment_body': ""Ah, yes. This was done to take care of accelerator testing. Seeded generation does not work, so, we've made the probability 1."", 'comment_created': datetime.datetime(2022, 8, 15, 22, 57, 29, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}]","[{'commit_sha': 'c2bb422281e50089fed206c89a1a111b016ebed5', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b2d258e11c3419b88e85c6b88e37da95d368f873', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'db54b85294af64d2605a177c5b133feabf8d47fd', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ea1cce33064bd5a30f3df2d97c983faefc43213d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c58e21ede4f39d4106ba28070e9ad28da6c8399b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ed5c9449bc3058c7165013220cf22ffbb57e2fed', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '49e198faf085b7b4f3ae95929f5116ee24bc4b1d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9c3caf60a7955778b9d71c023c7d93e140557893', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5e053a5aab00c564e1783ded89ad69ba2b4d81ad', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'da2f9a6d4ca6cd246cd71be6a88b9431c1f49395', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2bc4f25130253d0c5c549afbefc9eef158a654a1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0cf4bee6d259f8d7c871df8f6d8e5dc440742478', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1046620705,Beam Search: Add Ragged and XLA Support,"Resolves https://github.com/keras-team/keras-nlp/issues/277

Sanity check: https://colab.research.google.com/drive/1Q7rzsHhFkI9AXXqp2MaOQfKhcAM951F0?usp=sharing",True,341,https://api.github.com/repos/keras-team/keras-nlp/pulls/341,https://github.com/keras-team/keras-nlp/pull/341,closed,171,81,2,7,0,20,0,0,[],2022-09-05 15:46:57+00:00,2022-09-13 19:21:26+00:00,704069.0,"8 days, 3:34:29","[{'comment_id': 964255704, 'comment_body': 'maybe use `tf.math.maximum` here?\r\n\r\n```\r\nextra_space = tf.math.maxiumum(0, max_length - shape[1])\r\npad_shape = (shape[0], extra_space)\r\n```', 'comment_created': datetime.datetime(2022, 9, 6, 23, 32, 41, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964257723, 'comment_body': ""probably better to just have a consistent return type, and ignore it in functions that don't need it"", 'comment_created': datetime.datetime(2022, 9, 6, 23, 37, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964259980, 'comment_body': 'What is the overall shape/format you are trying to achieve here with `indices`? Feel this could be simplified, or at least made more readable.', 'comment_created': datetime.datetime(2022, 9, 6, 23, 42, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964261956, 'comment_body': 'split this into multiple lines\r\n\r\n```\r\nflattened_beams = tf.reshape()\r\nlogits = token_probability_fn()\r\nlogits = tf.reshape(logits...)\r\n```', 'comment_created': datetime.datetime(2022, 9, 6, 23, 47, 51, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964262672, 'comment_body': ""Are we changing the expectation around the `token_probability_fn` from what it was? it should support a input with a first dim of batch_size * num_beams of something like that?\r\n\r\nThat's probably ok, especially if we have performance data to back this up, but we should make sure to clearly document expectation for the function."", 'comment_created': datetime.datetime(2022, 9, 6, 23, 49, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964264326, 'comment_body': 'what does num_beams == 1 even mean? can we just route to greedy or something simpler in that case at the top of this fn?', 'comment_created': datetime.datetime(2022, 9, 6, 23, 52, 44, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964274760, 'comment_body': ""Ideally, that shouldn't matter, right? Isn't `token_probability_fn` a function which just takes a number of samples and returns the set of next tokens for every sample? Won't most users just do a forward pass on the model, in which case, the number of samples should not matter? \r\n\r\nWhat is the alternative here? `tf.map_fn` over the beams? One potential issue with the current reshaping solution could be memory issues, which won't happen if we iterate over the beams. The downside is of course, worse performance (probably; since we are using a `map_fn`)."", 'comment_created': datetime.datetime(2022, 9, 7, 0, 19, 17, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964277136, 'comment_body': 'So, assume batch_size = 2, num_beams = 3, length = 2.\r\n\r\n```\r\ntf.cast(tf.where(tf.ones((batch_size, num_beams), tf.bool)), dtype=tf.int32)\r\n```\r\n\r\nwill give us\r\n\r\n```\r\n<tf.Tensor: shape=(6, 2), dtype=int32, numpy=\r\narray([[0, 0],\r\n       [0, 1],\r\n       [0, 2],\r\n       [1, 0],\r\n       [1, 1],\r\n       [1, 2]], dtype=int32)>\r\n```\r\n\r\nWe then concatenate every row with length (i.e., the index of the ""column"" to be updated):\r\n\r\n```\r\n<tf.Tensor: shape=(6, 3), dtype=int32, numpy=\r\narray([[0, 0, 2],\r\n       [0, 1, 2],\r\n       [0, 2, 2],\r\n       [1, 0, 2],\r\n       [1, 1, 2],\r\n       [1, 2, 2]], dtype=int32)>\r\n```\r\n\r\nSo, essentially, iterating over all possible values of `(:batch_size, :num_beams)`, keeping the length fixed.', 'comment_created': datetime.datetime(2022, 9, 7, 0, 25, 46, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964277622, 'comment_body': 'Makes sense. Will do', 'comment_created': datetime.datetime(2022, 9, 7, 0, 26, 59, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964283367, 'comment_body': ""I think it probably makes sense to use `token_probability_fn` as you do here, this is probably just a documentation problem.\r\n\r\nWe are changing this function (which is user inputted), to have a different shape expectation than all the other utilities fns. A good model does not need to encode a fixed batch size, but that won't stop some users from building models that barf on a different batch size.. We should make sure to document our shape expectations."", 'comment_created': datetime.datetime(2022, 9, 7, 0, 42, 40, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964286142, 'comment_body': 'Great! Will mention it in the documentation', 'comment_created': datetime.datetime(2022, 9, 7, 0, 50, 25, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964522885, 'comment_body': ""I've split it into two lines...let me know if it's okay."", 'comment_created': datetime.datetime(2022, 9, 7, 8, 8, 34, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 965229794, 'comment_body': 'I\'m not sure what ""where batch_size is variable"" is attempting to communicate, let\'s just remove.', 'comment_created': datetime.datetime(2022, 9, 7, 20, 11, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965230468, 'comment_body': 'extra backtick', 'comment_created': datetime.datetime(2022, 9, 7, 20, 12, 13, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965239147, 'comment_body': ""Are you looking for `tf.float32.min` here?\r\n\r\nI'm noticing you drop the softmax here, which I think is safe right `top_k(x) == top_k(softmax(x)`? Maybe we should just remove the argument for `from_logits`, same as greedy search."", 'comment_created': datetime.datetime(2022, 9, 7, 20, 24, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965282321, 'comment_body': 'Suggest for this whole block something more like this...\r\n\r\n```python\r\n# Build a new column of updates to scatter into the beam tensor.\r\nnext_tokens = tf.where(\r\n    condition=mask[..., length, tf.newaxis],\r\n    x=beams[..., length],\r\n    y=next_tokens,\r\n)\r\nnext_tokens = tf.flatten(next_tokens)\r\n\r\n# Generate `(batch_index, beam_index)` tuples for each beam. \r\nbeam_indices = tf.where(tf.ones((batch_size, num_beams), tf.bool))\r\nbeam_indices = tf.cast(beam_indices, length.dtype)\r\n# Build a tensor repeated `length` values.\r\nlength_indices = tf.fill((batch_size * num_beams, 1), length)\r\n# Concatenate to a triplet of `(batch_index, beam_index, length)`.\r\nindices = tf.concat([beam_indices, length_indices], axis=1)\r\n\r\nbeams = tf.tensor_scatter_nd_update(\r\n    tensor=beams,\r\n    indices=indices,\r\n    updates=next_tokens,\r\n)\r\n```\r\n\r\nFeel free to rename as needed, but overall you should look out for a few things.\r\n\r\n - Break up complex nested lines into intermediate lines that fit our line length when possible. \r\n - Use helpful naming for intermediate variables to indicate the flow.\r\n - Do not add comments everywhere, but add brief comments in places the code itself could use clarification.\r\n\r\nThe end goal should be making this very complex list of ops someone more readable to a newcomer trying to figure this out.', 'comment_created': datetime.datetime(2022, 9, 7, 20, 46, 7, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 966177981, 'comment_body': 'Agreed, probably meant to say that `batch_size` can be `None`. Removed!', 'comment_created': datetime.datetime(2022, 9, 8, 16, 31, 11, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 966182297, 'comment_body': ""> Are you looking for tf.float32.min here?\r\n\r\nYep, thanks!\r\n\r\n> I'm noticing you drop the softmax here, which I think is safe right top_k(x) == top_k(softmax(x)? Maybe we should just remove the argument for from_logits, same as greedy search.\r\n\r\n@mattdangerw - oooh, good spot. Up here, on line 327, a large negative number is okay because we are taking values on the log scale. \r\n\r\nBut, we should keep the `from_logits` arg, and do a softmax in `one_step()`, I think. We do a log there; `log(-ve number)` will error out since logits can have negative values. Let me know what you think about this.\r\n\r\nhttps://github.com/keras-team/keras-nlp/blob/master/keras_nlp/utils/text_generation.py#L328-L329\r\n"", 'comment_created': datetime.datetime(2022, 9, 8, 16, 36, 3, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 966211881, 'comment_body': 'Will keep that in mind, thanks a ton! Looks cleaner now', 'comment_created': datetime.datetime(2022, 9, 8, 17, 7, 18, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 967385288, 'comment_body': ""logits isn't a variable anymore. preds?"", 'comment_created': datetime.datetime(2022, 9, 9, 19, 5, 21, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '85f8afb1afbdffe0531534a0f8229563294ae8e8', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2182177c0098eb00b2698f73169b17535ca257d3', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f630bb6db446c0b26bdacad5e3821bd2da657188', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '866043b7cfd0ce4a37e07d93e02e7c2d33b5cca4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7656693abc657077d08a05f7ea2d93374921f844', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '05a3ceb790bf20ac734f70ec64e5ea91acbe3dbe', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a26c666c66f67d09d50d93b522238de623c998a1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
995799943,Make Decoding Functions Graph-compatible (with XLA Support!),"Resolves #241 

Partially resolves https://github.com/keras-team/keras-nlp/issues/277


- [x] Greedy Search
- [ ] Beam Search (will probably open a separate PR for this)
- [x] Top-p Search
- [x] Top-k Search
- [x] Random Search

Will have to think a bit more about Beam Search.",True,271,https://api.github.com/repos/keras-team/keras-nlp/pulls/271,https://github.com/keras-team/keras-nlp/pull/271,closed,421,72,2,12,7,21,0,0,[],2022-07-13 18:10:53+00:00,2022-08-16 19:15:24+00:00,2941471.0,"34 days, 1:04:31","[{'comment_id': 926118128, 'comment_body': 'would just using mode.predict work? that would still hit all the compiled function paths, and allow you to avoid all this dummy metric stuff, which is hard to read', 'comment_created': datetime.datetime(2022, 7, 20, 23, 0, 21, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 926123379, 'comment_body': 'Can we just pass `tf.TensorShape([None, None])` as the shape invariant? Generally we should support a static batch size of `None`, tf data does this by default after calling `.batch()` for example. Might simplify the code a bit.', 'comment_created': datetime.datetime(2022, 7, 20, 23, 13, 7, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 926124447, 'comment_body': 'Could also be good to test the call on a batched dataset (where batch size not statically known), and on a single constant input, as you are doing here.', 'comment_created': datetime.datetime(2022, 7, 20, 23, 14, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 926125509, 'comment_body': 'If you add `jit_compile=True` does the test still pass?\r\n\r\nIf yes, we should test this with both jit_compile=True and False, using https://docs.pytest.org/en/6.2.x/parametrize.html\r\n\r\nIf no, we should either try to fix things with jit_compilation, or make sure we track that on a follow up issue.', 'comment_created': datetime.datetime(2022, 7, 20, 23, 17, 28, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 927004701, 'comment_body': 'Ah, man. Stupid me. Should have used `model.predict` :P', 'comment_created': datetime.datetime(2022, 7, 21, 18, 49, 16, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 927078312, 'comment_body': ""@abheesht17 Let's try add a test case for `jit_compile=True`, and we can run it on GPU. We recently add GPU test support in this repo."", 'comment_created': datetime.datetime(2022, 7, 21, 20, 31, 51, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 927713036, 'comment_body': ""It's not working with `jit_compile = True`. Complete error logs: https://p.ip.fi/2TNt.\r\n\r\nLooks like it won't work with `shape_invariants`."", 'comment_created': datetime.datetime(2022, 7, 22, 14, 32, 47, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 942715834, 'comment_body': 'can we avoid the state dict and just do `loops_vars=(length, prompt)` here? might be a little more readable', 'comment_created': datetime.datetime(2022, 8, 10, 17, 25, 28, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 942767508, 'comment_body': 'I feel like we should be able to make this simpler, we are just padding a batched tensor with pad_token_id to the sequence length right? We should not need a map_fn for this', 'comment_created': datetime.datetime(2022, 8, 10, 18, 24, 54, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 942769828, 'comment_body': 'do we even need this function anymore, if we are just starting with the correct sized tensor filled with pad_token_id?', 'comment_created': datetime.datetime(2022, 8, 10, 18, 27, 46, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943507082, 'comment_body': 'hmm I guess we do to avoid random tokens after the end_token_id', 'comment_created': datetime.datetime(2022, 8, 11, 13, 47, 51, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943540643, 'comment_body': 'Yep', 'comment_created': datetime.datetime(2022, 8, 11, 14, 16, 56, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 943752675, 'comment_body': 'nit: maybe split this into two lines for readability?\r\n\r\npadding = tf.fill((tf.shape(prompt)[0], max_length - length), pad_token_id)\r\nprompt = tf.concat((prompt, padding), axis=-1)', 'comment_created': datetime.datetime(2022, 8, 11, 17, 40, 59, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943756401, 'comment_body': 'can we just do something like\r\n```\r\nbatch_size, length = tf.shape(x)\r\n```\r\nAnd use that below? Then length and batch size are both tensors from the start.', 'comment_created': datetime.datetime(2022, 8, 11, 17, 45, 17, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943758138, 'comment_body': 'length, prompt = tf.while_loop(...)\r\n\r\njust to avoid that `[1]` which is not super readable', 'comment_created': datetime.datetime(2022, 8, 11, 17, 47, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943759661, 'comment_body': 'is there a reason you have to do training switch here? it looks like you are never actually testing the training=True branch, might be nice to clean up the test a bit', 'comment_created': datetime.datetime(2022, 8, 11, 17, 48, 54, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943772820, 'comment_body': ""Hmmm, I'll split this into two lines:\r\n\r\n```\r\nbatch_size = tf.shape(prompt)[0]\r\nlength = tf.shape(prompt)[1]\r\n```\r\n\r\n```\r\nbatch_size, length = tf.shape(x)\r\n```\r\ndoes not work in graph mode."", 'comment_created': datetime.datetime(2022, 8, 11, 18, 4, 50, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 943775264, 'comment_body': 'Stack trace: https://p.ip.fi/6YAg', 'comment_created': datetime.datetime(2022, 8, 11, 18, 5, 32, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 943856422, 'comment_body': ""ah destructuring is too fancy for autograph, I forgot. let's do\r\n\r\nshape = tf.shape(prompt)\r\nbatch_size = shape[0]\r\nlength = shape[1]"", 'comment_created': datetime.datetime(2022, 8, 11, 19, 29, 43, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 945981794, 'comment_body': 'Do we need to change the number here? The original value seems to be more general?', 'comment_created': datetime.datetime(2022, 8, 15, 17, 42, 6, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 946204029, 'comment_body': ""Ah, yes. This was done to take care of accelerator testing. Seeded generation does not work, so, we've made the probability 1."", 'comment_created': datetime.datetime(2022, 8, 15, 22, 57, 29, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}]","[{'commit_sha': 'c2bb422281e50089fed206c89a1a111b016ebed5', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b2d258e11c3419b88e85c6b88e37da95d368f873', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'db54b85294af64d2605a177c5b133feabf8d47fd', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ea1cce33064bd5a30f3df2d97c983faefc43213d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c58e21ede4f39d4106ba28070e9ad28da6c8399b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ed5c9449bc3058c7165013220cf22ffbb57e2fed', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '49e198faf085b7b4f3ae95929f5116ee24bc4b1d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9c3caf60a7955778b9d71c023c7d93e140557893', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5e053a5aab00c564e1783ded89ad69ba2b4d81ad', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'da2f9a6d4ca6cd246cd71be6a88b9431c1f49395', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2bc4f25130253d0c5c549afbefc9eef158a654a1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0cf4bee6d259f8d7c871df8f6d8e5dc440742478', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1046620705,Beam Search: Add Ragged and XLA Support,"Resolves https://github.com/keras-team/keras-nlp/issues/277

Sanity check: https://colab.research.google.com/drive/1Q7rzsHhFkI9AXXqp2MaOQfKhcAM951F0?usp=sharing",True,341,https://api.github.com/repos/keras-team/keras-nlp/pulls/341,https://github.com/keras-team/keras-nlp/pull/341,closed,171,81,2,7,0,20,0,0,[],2022-09-05 15:46:57+00:00,2022-09-13 19:21:26+00:00,704069.0,"8 days, 3:34:29","[{'comment_id': 964255704, 'comment_body': 'maybe use `tf.math.maximum` here?\r\n\r\n```\r\nextra_space = tf.math.maxiumum(0, max_length - shape[1])\r\npad_shape = (shape[0], extra_space)\r\n```', 'comment_created': datetime.datetime(2022, 9, 6, 23, 32, 41, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964257723, 'comment_body': ""probably better to just have a consistent return type, and ignore it in functions that don't need it"", 'comment_created': datetime.datetime(2022, 9, 6, 23, 37, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964259980, 'comment_body': 'What is the overall shape/format you are trying to achieve here with `indices`? Feel this could be simplified, or at least made more readable.', 'comment_created': datetime.datetime(2022, 9, 6, 23, 42, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964261956, 'comment_body': 'split this into multiple lines\r\n\r\n```\r\nflattened_beams = tf.reshape()\r\nlogits = token_probability_fn()\r\nlogits = tf.reshape(logits...)\r\n```', 'comment_created': datetime.datetime(2022, 9, 6, 23, 47, 51, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964262672, 'comment_body': ""Are we changing the expectation around the `token_probability_fn` from what it was? it should support a input with a first dim of batch_size * num_beams of something like that?\r\n\r\nThat's probably ok, especially if we have performance data to back this up, but we should make sure to clearly document expectation for the function."", 'comment_created': datetime.datetime(2022, 9, 6, 23, 49, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964264326, 'comment_body': 'what does num_beams == 1 even mean? can we just route to greedy or something simpler in that case at the top of this fn?', 'comment_created': datetime.datetime(2022, 9, 6, 23, 52, 44, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964274760, 'comment_body': ""Ideally, that shouldn't matter, right? Isn't `token_probability_fn` a function which just takes a number of samples and returns the set of next tokens for every sample? Won't most users just do a forward pass on the model, in which case, the number of samples should not matter? \r\n\r\nWhat is the alternative here? `tf.map_fn` over the beams? One potential issue with the current reshaping solution could be memory issues, which won't happen if we iterate over the beams. The downside is of course, worse performance (probably; since we are using a `map_fn`)."", 'comment_created': datetime.datetime(2022, 9, 7, 0, 19, 17, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964277136, 'comment_body': 'So, assume batch_size = 2, num_beams = 3, length = 2.\r\n\r\n```\r\ntf.cast(tf.where(tf.ones((batch_size, num_beams), tf.bool)), dtype=tf.int32)\r\n```\r\n\r\nwill give us\r\n\r\n```\r\n<tf.Tensor: shape=(6, 2), dtype=int32, numpy=\r\narray([[0, 0],\r\n       [0, 1],\r\n       [0, 2],\r\n       [1, 0],\r\n       [1, 1],\r\n       [1, 2]], dtype=int32)>\r\n```\r\n\r\nWe then concatenate every row with length (i.e., the index of the ""column"" to be updated):\r\n\r\n```\r\n<tf.Tensor: shape=(6, 3), dtype=int32, numpy=\r\narray([[0, 0, 2],\r\n       [0, 1, 2],\r\n       [0, 2, 2],\r\n       [1, 0, 2],\r\n       [1, 1, 2],\r\n       [1, 2, 2]], dtype=int32)>\r\n```\r\n\r\nSo, essentially, iterating over all possible values of `(:batch_size, :num_beams)`, keeping the length fixed.', 'comment_created': datetime.datetime(2022, 9, 7, 0, 25, 46, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964277622, 'comment_body': 'Makes sense. Will do', 'comment_created': datetime.datetime(2022, 9, 7, 0, 26, 59, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964283367, 'comment_body': ""I think it probably makes sense to use `token_probability_fn` as you do here, this is probably just a documentation problem.\r\n\r\nWe are changing this function (which is user inputted), to have a different shape expectation than all the other utilities fns. A good model does not need to encode a fixed batch size, but that won't stop some users from building models that barf on a different batch size.. We should make sure to document our shape expectations."", 'comment_created': datetime.datetime(2022, 9, 7, 0, 42, 40, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964286142, 'comment_body': 'Great! Will mention it in the documentation', 'comment_created': datetime.datetime(2022, 9, 7, 0, 50, 25, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964522885, 'comment_body': ""I've split it into two lines...let me know if it's okay."", 'comment_created': datetime.datetime(2022, 9, 7, 8, 8, 34, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 965229794, 'comment_body': 'I\'m not sure what ""where batch_size is variable"" is attempting to communicate, let\'s just remove.', 'comment_created': datetime.datetime(2022, 9, 7, 20, 11, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965230468, 'comment_body': 'extra backtick', 'comment_created': datetime.datetime(2022, 9, 7, 20, 12, 13, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965239147, 'comment_body': ""Are you looking for `tf.float32.min` here?\r\n\r\nI'm noticing you drop the softmax here, which I think is safe right `top_k(x) == top_k(softmax(x)`? Maybe we should just remove the argument for `from_logits`, same as greedy search."", 'comment_created': datetime.datetime(2022, 9, 7, 20, 24, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965282321, 'comment_body': 'Suggest for this whole block something more like this...\r\n\r\n```python\r\n# Build a new column of updates to scatter into the beam tensor.\r\nnext_tokens = tf.where(\r\n    condition=mask[..., length, tf.newaxis],\r\n    x=beams[..., length],\r\n    y=next_tokens,\r\n)\r\nnext_tokens = tf.flatten(next_tokens)\r\n\r\n# Generate `(batch_index, beam_index)` tuples for each beam. \r\nbeam_indices = tf.where(tf.ones((batch_size, num_beams), tf.bool))\r\nbeam_indices = tf.cast(beam_indices, length.dtype)\r\n# Build a tensor repeated `length` values.\r\nlength_indices = tf.fill((batch_size * num_beams, 1), length)\r\n# Concatenate to a triplet of `(batch_index, beam_index, length)`.\r\nindices = tf.concat([beam_indices, length_indices], axis=1)\r\n\r\nbeams = tf.tensor_scatter_nd_update(\r\n    tensor=beams,\r\n    indices=indices,\r\n    updates=next_tokens,\r\n)\r\n```\r\n\r\nFeel free to rename as needed, but overall you should look out for a few things.\r\n\r\n - Break up complex nested lines into intermediate lines that fit our line length when possible. \r\n - Use helpful naming for intermediate variables to indicate the flow.\r\n - Do not add comments everywhere, but add brief comments in places the code itself could use clarification.\r\n\r\nThe end goal should be making this very complex list of ops someone more readable to a newcomer trying to figure this out.', 'comment_created': datetime.datetime(2022, 9, 7, 20, 46, 7, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 966177981, 'comment_body': 'Agreed, probably meant to say that `batch_size` can be `None`. Removed!', 'comment_created': datetime.datetime(2022, 9, 8, 16, 31, 11, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 966182297, 'comment_body': ""> Are you looking for tf.float32.min here?\r\n\r\nYep, thanks!\r\n\r\n> I'm noticing you drop the softmax here, which I think is safe right top_k(x) == top_k(softmax(x)? Maybe we should just remove the argument for from_logits, same as greedy search.\r\n\r\n@mattdangerw - oooh, good spot. Up here, on line 327, a large negative number is okay because we are taking values on the log scale. \r\n\r\nBut, we should keep the `from_logits` arg, and do a softmax in `one_step()`, I think. We do a log there; `log(-ve number)` will error out since logits can have negative values. Let me know what you think about this.\r\n\r\nhttps://github.com/keras-team/keras-nlp/blob/master/keras_nlp/utils/text_generation.py#L328-L329\r\n"", 'comment_created': datetime.datetime(2022, 9, 8, 16, 36, 3, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 966211881, 'comment_body': 'Will keep that in mind, thanks a ton! Looks cleaner now', 'comment_created': datetime.datetime(2022, 9, 8, 17, 7, 18, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 967385288, 'comment_body': ""logits isn't a variable anymore. preds?"", 'comment_created': datetime.datetime(2022, 9, 9, 19, 5, 21, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '85f8afb1afbdffe0531534a0f8229563294ae8e8', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2182177c0098eb00b2698f73169b17535ca257d3', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f630bb6db446c0b26bdacad5e3821bd2da657188', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '866043b7cfd0ce4a37e07d93e02e7c2d33b5cca4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7656693abc657077d08a05f7ea2d93374921f844', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '05a3ceb790bf20ac734f70ec64e5ea91acbe3dbe', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a26c666c66f67d09d50d93b522238de623c998a1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1031715141,Text Generation Utilities: Add Support for Ragged Inputs,Resolves https://github.com/keras-team/keras-nlp/issues/295,True,300,https://api.github.com/repos/keras-team/keras-nlp/pulls/300,https://github.com/keras-team/keras-nlp/pull/300,closed,250,195,2,10,1,26,0,0,[],2022-08-20 06:59:21+00:00,2022-09-01 19:35:07+00:00,1082146.0,"12 days, 12:35:46","[{'comment_id': 953054794, 'comment_body': ""The code has too many assumptions now. Let's be explicit with different conditions. \r\n\r\n"", 'comment_created': datetime.datetime(2022, 8, 23, 19, 54, 7, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 953165891, 'comment_body': '@mattdangerw I remember we explicitly wanted these to be public functions, do you remember the reason?', 'comment_created': datetime.datetime(2022, 8, 23, 22, 39, 53, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 954218983, 'comment_body': 'The general expectation we have in our codebase is to only automatically convert dense tensors. E.g.\r\nhttps://github.com/keras-team/keras-nlp/blob/master/keras_nlp/tokenizers/word_piece_tokenizer.py#L331\r\n\r\nI think that is a reasonable assumption we can carry over here. If you want ragged inputs make sure you inputs are explicitly `tf.ragged` before you call.', 'comment_created': datetime.datetime(2022, 8, 24, 19, 28, 45, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 954222962, 'comment_body': ""This feels a little weird to me to follow all these validation functions. If we can't have a single validation function, why not just leave these as two separate functions?\r\n\r\n_validate_token_probability_fn and _validate_prompt. Call them right next to each other when you need both.\r\n\r\nI would find that more obvious and readable."", 'comment_created': datetime.datetime(2022, 8, 24, 19, 32, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 954234049, 'comment_body': ""I think it was just a style guide sort of thing. We use `__init__.py` files to control what is exported, so there's no need to go about marking every helper function private. But looking at this we aren't exactly consistent about this either here or in core Keras."", 'comment_created': datetime.datetime(2022, 8, 24, 19, 41, 42, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 954251206, 'comment_body': ""these paramers are getting a little hard to read. can't just add an extra assert and avoid the extra parameter?\r\n\r\n```\r\nself.assertAllEqual(model.predict(ragged_inputs), ragged_outputs)\r\nself.assertAllEqual(model.predict(dense_inputs), dense_inputs)\r\n```\r\n\r\nalso, unrelated to this PR, but the dense outputs is hard to read. maybe we can turn this\r\n\r\n```\r\nexpected_outputs = tf.tile([[3], [0]], [1, max_length - 2])\r\nexpected_outputs = tf.concat([inputs, expected_outputs], axis=1)\r\n```\r\ninto something like\r\n```\r\nexpected_outputs = tf.constant([[0, 1, 3, 3, 3], [1, 2, 0, 0, 0]])\r\n```\r\n\r\nplease make these changes to all the test in this files where you are adding a ragged parameterized case."", 'comment_created': datetime.datetime(2022, 8, 24, 20, 1, 50, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 954284576, 'comment_body': ""Overall this feels a little hard to follow. I wonder if we can unify the flow between ragged and dense, and simplify the state. Something like...\r\n\r\n```\r\n...outside the loop...\r\n# Make sure the prompt is ragged.\r\nif isinstance(prompt, tf.Tensor):\r\n   prompt = tf.RaggedTensor.from_tensor(prompt)\r\n# Convert the prompt to dense with shape (bs, max_length), and keep a mask.\r\nmask = tf.ones_like(prompt, dtype=tf.bool).to_tensor(...)\r\nprompt = prompt.to_tensor(...)\r\n\r\n...inside the loop...\r\nnext_column = gen_next_token_fn(prompt[:, :length])\r\n# Make sure to not replace the original prompt input.\r\nnext_column = tf.where(mask[:, length + 1], prompt[:, length + 1], next_column)\r\nprompt = tf.tensor_scatter_nd_update(...)\r\n```\r\n\r\nNot sure if that's totally clear above, but basically the idea is that\r\n1) The ragged case is more general, so we can write one branch of code by converting all prompts to raggeds.\r\n2) We should also be able to simplify the code and the `token_probability_fn` by always calling the `token_probability_fn` on a dense tensor and masking out certain updates."", 'comment_created': datetime.datetime(2022, 8, 24, 20, 45, 58, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 956807500, 'comment_body': 'Ohh, I added it as a parameter because the UT for ragged input with `jit_compile = True` fails.', 'comment_created': datetime.datetime(2022, 8, 29, 0, 35, 19, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 956815459, 'comment_body': 'Have not condensed it to one branch because `.to_tensor()` does not work with XLA. Made changes for the masking logic.', 'comment_created': datetime.datetime(2022, 8, 29, 1, 13, 52, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 957694730, 'comment_body': 'if this a specific issue for beam search only? why is this getting added only here?', 'comment_created': datetime.datetime(2022, 8, 29, 18, 57, 56, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 957698473, 'comment_body': 'Does every util need this verbatim? Could we move this into `_validate_prompt` or keep it is a standalone helper `_pad_prompt`?', 'comment_created': datetime.datetime(2022, 8, 29, 19, 2, 44, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 957699791, 'comment_body': ""Ah right! That's annoying we need to so many workarounds, but agreed this is the best option!"", 'comment_created': datetime.datetime(2022, 8, 29, 19, 4, 23, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 957890645, 'comment_body': 'Yep. This check won\'t work for ragged tensors. Temporarily shifted it to beam search till we add ragged support for it.\r\n\r\nP.S. I\'d earlier added a condition for ragged tensor - something along the lines of\r\n\r\n```\r\ndef validate_length(length):\r\n    if tf.equal(tf.reduce_min(length), tf.cast(0, dtype=length.dtype)):\r\n        raise ValueError(\r\n            f""Length of at least one sequence in `prompt` is 0, ""\r\n            ""please provide a non-empty `prompt`."")\r\n```\r\n\r\nBut for some reason, this throws an error. Had a discussion with @chenmoneygithub and we took a call to remove it. Anyway, `validate_token_probability_fn` will throw an error for empty prompt.\r\n\r\nThe error thrown is:\r\n\r\n```\r\nValueError: Exception encountered when calling layer ""test_model_31"" (type TestModel).\r\n    \r\n    in user code:\r\n    \r\n        File ""<ipython-input-49-eb3e68faf80e>"", line 9, in call  *\r\n            generated = greedy_search(\r\n        File ""<ipython-input-58-2527ddd55744>"", line 36, in greedy_search  *\r\n            validate_length(length)\r\n        File ""<ipython-input-64-f8c3e5dccc3b>"", line 5, in validate_length  *\r\n            raise ValueError(\r\n    \r\n        ValueError: Length of at least one sequence in `prompt` is 0, please provide a non-empty `prompt`.\r\n```\r\n\r\nThis happens even for non-empty prompts.', 'comment_created': datetime.datetime(2022, 8, 30, 0, 6, 22, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 957893587, 'comment_body': ""Hmmm, right. I reversed all forms of refactoring for this PR. Earlier, I'd put padding in `_generate_function`, which was a common point for all text gen functions. Should we keep this as it is for now, and wait for the PR where I refactor this file?\r\n\r\nEdit: I've dumped these lines in a helper function for now."", 'comment_created': datetime.datetime(2022, 8, 30, 0, 11, 49, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 960145094, 'comment_body': 'Seems like this could just be the docstring? I would also phrase this more like...\r\n\r\n""""""\r\nPad prompt to mask_length and compute a mask for updates.\r\n\r\nThis utility will pad the (possibly ragged) prompt to max_length, and compute a mask where\r\ninput was originally set in the prompt, to avoid overwriting the original inputs when generating\r\nnew outputs.\r\n""""""', 'comment_created': datetime.datetime(2022, 9, 1, 1, 30, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 960145193, 'comment_body': 'Could this use bounding shape?\r\nhttps://www.tensorflow.org/api_docs/python/tf/RaggedTensor#bounding_shape', 'comment_created': datetime.datetime(2022, 9, 1, 1, 30, 50, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 960146943, 'comment_body': ""I'm pretty sure there should be a way to rework this scatter with the `tf.range` call, so that we are scattering the entire column in at once.\r\nhttps://www.tensorflow.org/api_docs/python/tf/tensor_scatter_nd_update#slice_updates_2\r\n\r\nSeems like that would be more efficient."", 'comment_created': datetime.datetime(2022, 9, 1, 1, 35, 59, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 960150378, 'comment_body': 'I think you can actually just say `shape=(None, max_length)`, which basically ask to only pad on the second dimension.', 'comment_created': datetime.datetime(2022, 9, 1, 1, 45, 32, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 960150600, 'comment_body': '_pad_prompt', 'comment_created': datetime.datetime(2022, 9, 1, 1, 46, 16, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 960151663, 'comment_body': ""piggybacking on above comment re using ragged bounding shape, maybe we should just split this in two?\r\n\r\n```\r\nbatch_size, length = _get_prompt_bounding_shape(prompt)\r\nprompt, mask = _pad_prompt(prompt, max_length, pad_token_id)\r\n```\r\n\r\nFor now, you will still need to read the shape inside `_pad_prompt` for the dense branch, but that's a temporary workaround anyway."", 'comment_created': datetime.datetime(2022, 9, 1, 1, 49, 2, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 960215631, 'comment_body': '`ragged_tensor.bounding_shape()` finds the maximum length, we want minimum.', 'comment_created': datetime.datetime(2022, 9, 1, 4, 50, 2, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 960216334, 'comment_body': 'Discussed offline. With `tensor_scatter_update_nd`, we can provide row index to update a row, but there is no way to provide a column index to update a column.', 'comment_created': datetime.datetime(2022, 9, 1, 4, 51, 55, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 960223320, 'comment_body': ""For now, I've done this:\r\n\r\n```    \r\n    batch_size, length = _get_prompt_length(prompt)\r\n    prompt, mask = _pad_prompt(\r\n        prompt, batch_size, length, max_length, pad_token_id\r\n    )\r\n```"", 'comment_created': datetime.datetime(2022, 9, 1, 5, 9, 16, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 960905253, 'comment_body': 'Heh, right.', 'comment_created': datetime.datetime(2022, 9, 1, 17, 6, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 960909042, 'comment_body': 'probably _get_prompt_shape, as this returns not just the length', 'comment_created': datetime.datetime(2022, 9, 1, 17, 10, 16, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 960939853, 'comment_body': ""Still think this could be simplified a bit, these are complex chains of ops and readability is important.\r\n\r\nI'll push a commit with a suggestion, take a look!"", 'comment_created': datetime.datetime(2022, 9, 1, 17, 40, 5, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': 'a20354bd64bf3707919a704b6eabb3bbc9d23009', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd1e68259e963398935f93190ecb1c469de0b3c32', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7835f5edcc6c7a57ba29012059f1ce6eb4563ede', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '771542dacb98b5787a1e49aca31b286b66130750', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'accb372610da9fff0a7d7a6c0d7cad3e3be7a165', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cfc42b49f348f400e65f53bb2957a6ee06c5d1a3', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '933a6b0d85d98ca45f7e3a6114f41eaae45bc3b1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '30afc76015a9cec43b51266101798e83c2b6d027', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5652f24a65424affd133203aebc927dfc78d9022', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9257b3cc8c308ec6e6751040bda3211a9f33fa42', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1046620705,Beam Search: Add Ragged and XLA Support,"Resolves https://github.com/keras-team/keras-nlp/issues/277

Sanity check: https://colab.research.google.com/drive/1Q7rzsHhFkI9AXXqp2MaOQfKhcAM951F0?usp=sharing",True,341,https://api.github.com/repos/keras-team/keras-nlp/pulls/341,https://github.com/keras-team/keras-nlp/pull/341,closed,171,81,2,7,0,20,0,0,[],2022-09-05 15:46:57+00:00,2022-09-13 19:21:26+00:00,704069.0,"8 days, 3:34:29","[{'comment_id': 964255704, 'comment_body': 'maybe use `tf.math.maximum` here?\r\n\r\n```\r\nextra_space = tf.math.maxiumum(0, max_length - shape[1])\r\npad_shape = (shape[0], extra_space)\r\n```', 'comment_created': datetime.datetime(2022, 9, 6, 23, 32, 41, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964257723, 'comment_body': ""probably better to just have a consistent return type, and ignore it in functions that don't need it"", 'comment_created': datetime.datetime(2022, 9, 6, 23, 37, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964259980, 'comment_body': 'What is the overall shape/format you are trying to achieve here with `indices`? Feel this could be simplified, or at least made more readable.', 'comment_created': datetime.datetime(2022, 9, 6, 23, 42, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964261956, 'comment_body': 'split this into multiple lines\r\n\r\n```\r\nflattened_beams = tf.reshape()\r\nlogits = token_probability_fn()\r\nlogits = tf.reshape(logits...)\r\n```', 'comment_created': datetime.datetime(2022, 9, 6, 23, 47, 51, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964262672, 'comment_body': ""Are we changing the expectation around the `token_probability_fn` from what it was? it should support a input with a first dim of batch_size * num_beams of something like that?\r\n\r\nThat's probably ok, especially if we have performance data to back this up, but we should make sure to clearly document expectation for the function."", 'comment_created': datetime.datetime(2022, 9, 6, 23, 49, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964264326, 'comment_body': 'what does num_beams == 1 even mean? can we just route to greedy or something simpler in that case at the top of this fn?', 'comment_created': datetime.datetime(2022, 9, 6, 23, 52, 44, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964274760, 'comment_body': ""Ideally, that shouldn't matter, right? Isn't `token_probability_fn` a function which just takes a number of samples and returns the set of next tokens for every sample? Won't most users just do a forward pass on the model, in which case, the number of samples should not matter? \r\n\r\nWhat is the alternative here? `tf.map_fn` over the beams? One potential issue with the current reshaping solution could be memory issues, which won't happen if we iterate over the beams. The downside is of course, worse performance (probably; since we are using a `map_fn`)."", 'comment_created': datetime.datetime(2022, 9, 7, 0, 19, 17, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964277136, 'comment_body': 'So, assume batch_size = 2, num_beams = 3, length = 2.\r\n\r\n```\r\ntf.cast(tf.where(tf.ones((batch_size, num_beams), tf.bool)), dtype=tf.int32)\r\n```\r\n\r\nwill give us\r\n\r\n```\r\n<tf.Tensor: shape=(6, 2), dtype=int32, numpy=\r\narray([[0, 0],\r\n       [0, 1],\r\n       [0, 2],\r\n       [1, 0],\r\n       [1, 1],\r\n       [1, 2]], dtype=int32)>\r\n```\r\n\r\nWe then concatenate every row with length (i.e., the index of the ""column"" to be updated):\r\n\r\n```\r\n<tf.Tensor: shape=(6, 3), dtype=int32, numpy=\r\narray([[0, 0, 2],\r\n       [0, 1, 2],\r\n       [0, 2, 2],\r\n       [1, 0, 2],\r\n       [1, 1, 2],\r\n       [1, 2, 2]], dtype=int32)>\r\n```\r\n\r\nSo, essentially, iterating over all possible values of `(:batch_size, :num_beams)`, keeping the length fixed.', 'comment_created': datetime.datetime(2022, 9, 7, 0, 25, 46, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964277622, 'comment_body': 'Makes sense. Will do', 'comment_created': datetime.datetime(2022, 9, 7, 0, 26, 59, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964283367, 'comment_body': ""I think it probably makes sense to use `token_probability_fn` as you do here, this is probably just a documentation problem.\r\n\r\nWe are changing this function (which is user inputted), to have a different shape expectation than all the other utilities fns. A good model does not need to encode a fixed batch size, but that won't stop some users from building models that barf on a different batch size.. We should make sure to document our shape expectations."", 'comment_created': datetime.datetime(2022, 9, 7, 0, 42, 40, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964286142, 'comment_body': 'Great! Will mention it in the documentation', 'comment_created': datetime.datetime(2022, 9, 7, 0, 50, 25, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 964522885, 'comment_body': ""I've split it into two lines...let me know if it's okay."", 'comment_created': datetime.datetime(2022, 9, 7, 8, 8, 34, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 965229794, 'comment_body': 'I\'m not sure what ""where batch_size is variable"" is attempting to communicate, let\'s just remove.', 'comment_created': datetime.datetime(2022, 9, 7, 20, 11, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965230468, 'comment_body': 'extra backtick', 'comment_created': datetime.datetime(2022, 9, 7, 20, 12, 13, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965239147, 'comment_body': ""Are you looking for `tf.float32.min` here?\r\n\r\nI'm noticing you drop the softmax here, which I think is safe right `top_k(x) == top_k(softmax(x)`? Maybe we should just remove the argument for `from_logits`, same as greedy search."", 'comment_created': datetime.datetime(2022, 9, 7, 20, 24, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965282321, 'comment_body': 'Suggest for this whole block something more like this...\r\n\r\n```python\r\n# Build a new column of updates to scatter into the beam tensor.\r\nnext_tokens = tf.where(\r\n    condition=mask[..., length, tf.newaxis],\r\n    x=beams[..., length],\r\n    y=next_tokens,\r\n)\r\nnext_tokens = tf.flatten(next_tokens)\r\n\r\n# Generate `(batch_index, beam_index)` tuples for each beam. \r\nbeam_indices = tf.where(tf.ones((batch_size, num_beams), tf.bool))\r\nbeam_indices = tf.cast(beam_indices, length.dtype)\r\n# Build a tensor repeated `length` values.\r\nlength_indices = tf.fill((batch_size * num_beams, 1), length)\r\n# Concatenate to a triplet of `(batch_index, beam_index, length)`.\r\nindices = tf.concat([beam_indices, length_indices], axis=1)\r\n\r\nbeams = tf.tensor_scatter_nd_update(\r\n    tensor=beams,\r\n    indices=indices,\r\n    updates=next_tokens,\r\n)\r\n```\r\n\r\nFeel free to rename as needed, but overall you should look out for a few things.\r\n\r\n - Break up complex nested lines into intermediate lines that fit our line length when possible. \r\n - Use helpful naming for intermediate variables to indicate the flow.\r\n - Do not add comments everywhere, but add brief comments in places the code itself could use clarification.\r\n\r\nThe end goal should be making this very complex list of ops someone more readable to a newcomer trying to figure this out.', 'comment_created': datetime.datetime(2022, 9, 7, 20, 46, 7, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 966177981, 'comment_body': 'Agreed, probably meant to say that `batch_size` can be `None`. Removed!', 'comment_created': datetime.datetime(2022, 9, 8, 16, 31, 11, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 966182297, 'comment_body': ""> Are you looking for tf.float32.min here?\r\n\r\nYep, thanks!\r\n\r\n> I'm noticing you drop the softmax here, which I think is safe right top_k(x) == top_k(softmax(x)? Maybe we should just remove the argument for from_logits, same as greedy search.\r\n\r\n@mattdangerw - oooh, good spot. Up here, on line 327, a large negative number is okay because we are taking values on the log scale. \r\n\r\nBut, we should keep the `from_logits` arg, and do a softmax in `one_step()`, I think. We do a log there; `log(-ve number)` will error out since logits can have negative values. Let me know what you think about this.\r\n\r\nhttps://github.com/keras-team/keras-nlp/blob/master/keras_nlp/utils/text_generation.py#L328-L329\r\n"", 'comment_created': datetime.datetime(2022, 9, 8, 16, 36, 3, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 966211881, 'comment_body': 'Will keep that in mind, thanks a ton! Looks cleaner now', 'comment_created': datetime.datetime(2022, 9, 8, 17, 7, 18, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 967385288, 'comment_body': ""logits isn't a variable anymore. preds?"", 'comment_created': datetime.datetime(2022, 9, 9, 19, 5, 21, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '85f8afb1afbdffe0531534a0f8229563294ae8e8', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2182177c0098eb00b2698f73169b17535ca257d3', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f630bb6db446c0b26bdacad5e3821bd2da657188', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '866043b7cfd0ce4a37e07d93e02e7c2d33b5cca4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7656693abc657077d08a05f7ea2d93374921f844', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '05a3ceb790bf20ac734f70ec64e5ea91acbe3dbe', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a26c666c66f67d09d50d93b522238de623c998a1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1047108238,Text Generation Functions: Add Benchmark Script,"Resolves https://github.com/keras-team/keras-nlp/issues/335

Tested it here: https://colab.research.google.com/drive/1eaYRSGYY6qu6KhfUJHNVd22TUol_WkPK?usp=sharing",True,342,https://api.github.com/repos/keras-team/keras-nlp/pulls/342,https://github.com/keras-team/keras-nlp/pull/342,closed,259,1,4,30,5,15,0,0,[],2022-09-06 06:16:34+00:00,2022-09-14 23:58:16+00:00,754902.0,"8 days, 17:41:42","[{'comment_id': 964109250, 'comment_body': 'Can we pull this whole thing straight into the python script? Or as a separate python script if we thing the separation is important?\r\n\r\nI think we would like to avoid json like this generally if possible. We made that call when making the bert config file https://github.com/keras-team/keras-nlp/blob/master/examples/bert/bert_config.py, but I think the same reasoning applied here.', 'comment_created': datetime.datetime(2022, 9, 6, 19, 48, 49, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964110438, 'comment_body': ""Let's move this whole thing into `keras_nlp/benchmarks` maybe. I think having this live next to `integration_tests` makes sense.\r\n\r\nCould we also avoid the nested directory? Just a `text_generation.py` file straight in `benchmarks`? "", 'comment_created': datetime.datetime(2022, 9, 6, 19, 50, 23, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964253640, 'comment_body': 'Moved it to a separate Python script! Not putting it in the same script because the config is kinda long.', 'comment_created': datetime.datetime(2022, 9, 6, 23, 27, 34, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 965203332, 'comment_body': ""Should we just move these straight into the main file? There's a lot of hardcoding one from the other. If we moved these in, we could get rid of `SUPPORTED_TEXT_GEN_METHODS`, `EXECUTION_METHODS` and the validation we are doing there.\r\n\r\nI also think for people that want to run the benchmarks with modifications, having a single file to edit might be nice."", 'comment_created': datetime.datetime(2022, 9, 7, 19, 34, 12, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965205401, 'comment_body': ""could make this slightly more concise with the following style, as we don't need the intermediate layers\r\n\r\n```\r\nx = keras_nlp.layers.TransformerDecoder(\r\n    num_heads=num_heads,\r\n    intermediate_dim=ff_dim,\r\n)(x)\r\n```"", 'comment_created': datetime.datetime(2022, 9, 7, 19, 37, 4, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965206941, 'comment_body': 'maybe we should just call this `generate_text`?', 'comment_created': datetime.datetime(2022, 9, 7, 19, 39, 16, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965207774, 'comment_body': 'What is this `2` here? is that the prompt length? Maybe just the line above write `prompt_length = 2` for readability.', 'comment_created': datetime.datetime(2022, 9, 7, 19, 40, 25, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965388392, 'comment_body': 'The idea was to have a separate function for eager mode. Is that worth adding to this script? Or does testing in graph mode suffice?', 'comment_created': datetime.datetime(2022, 9, 7, 23, 51, 58, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 965397026, 'comment_body': 'Make the same x = Embeddding(...)(x) change here?', 'comment_created': datetime.datetime(2022, 9, 8, 0, 14, 39, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 970073637, 'comment_body': ""These metrics are subject to change, I would prefer delete them. @mattdangerw what's your opinion on this?"", 'comment_created': datetime.datetime(2022, 9, 13, 20, 54, 11, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 970075495, 'comment_body': ""Since you are writing results to a file, let's add logging at the end: \r\n```\r\nwriting benchmark results to {path}\r\n```\r\nso users are notified that a new file is generated. "", 'comment_created': datetime.datetime(2022, 9, 13, 20, 56, 40, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 970317985, 'comment_body': ""Waiting for Matt's opinion on this"", 'comment_created': datetime.datetime(2022, 9, 14, 5, 17, 5, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 970364756, 'comment_body': ""Oh sorry missed this. I think it helps to have the numbers to track!\r\n\r\nIn a lot of ways this benchmark seems like a pilot. If we notice these numbers get stale, we can always delete them later. I don't see a cost to adding."", 'comment_created': datetime.datetime(2022, 9, 14, 6, 27, 23, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 970368744, 'comment_body': ""Cool! I'm thinking of making a table instead of pasting the raw output. And, will mention the GPU version, etc."", 'comment_created': datetime.datetime(2022, 9, 14, 6, 30, 46, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 971396055, 'comment_body': 'These are great results btw! Should we open up a bug for top-p being slower? Or do we already know why?', 'comment_created': datetime.datetime(2022, 9, 14, 23, 57, 59, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '9ffb362799fceab85e5485837e66a2e48e8a12e2', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '636c354ce3bd2c04850c7dcab1cc53111f33fe78', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7a8744574eeba28f0c2ee3df9cf905ae383d74d1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ca177f2487f572d02de0941302b53bf7981863a7', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b84aba2d410bdf8bbe7373bace8fce1fed3a08fd', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9ef679fd8bae44b9622719aaee2f8017e56d603a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f6845f81a1cf8c6d4db3879ecdfe3931b471e4ad', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '40fbf16c1cdc512cebcbfc21a18dec4f026a2770', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ba77782d3842ed9af1b6ed76a3a2e21e62d7f4c8', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9f571bddf3cd8a9f760f2ce7315c9445bbdbcf19', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7a0b87bf6c21bd89764792c38ff8db0ea4d64c13', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '20799830b217f37868a1c5f72890efa2c8f73ede', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'eebb4b3f49d93c9c752c277550320b5de41b48ec', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '18b25f7b63b05d9dda8abc2fc8b35dc07734c2e4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ef9c81aacc0a7aea0047f1659e0743840ad86248', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e8d0f9da4dda1eaf5e851938e993587198ccb68c', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a56304f959f8c67688bb090efa1513a2920e824e', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cc5aac4bfe33aefdc5739596bcc48bee341e21d5', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1c3905a6649f75a3510c43fff336713164cf19e4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ab516349e3b98299f667f928696c1448b3471e6f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '524e1e66d5509668f426f923a471af38e53e00d8', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '92f4db990c58d957285a37f17318d70f8caec38d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7f04e3064ef924aaf2832f7178e6931335fd71c8', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bfae90b6fcf4c4eeed8cc870673562b71bc6956f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6e05ccc6b5c0073a7f8dea196b1e56c04ead4114', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e811cfe61b30b626e5392eb5447ebae4ac537b7b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1068476ea6abf64146aedfb1af658e11113b2d34', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '760d6b6471a9c31fc03e03f226ed8cc92545312f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a06f4b442ff0de24a97d738a0e354764deb243aa', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '47599db8760848e1d26200c85069551f263389bc', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
949395216,English-to-Spanish Example [KerasNLP],"Partially resolves https://github.com/keras-team/keras-nlp/issues/190

@mattdangerw, here is the PR for the NMT example. Please have a look. Thanks!

As per the instructions in README, I will add the generated files after approval.",True,890,https://api.github.com/repos/keras-team/keras-io/pulls/890,https://github.com/keras-team/keras-io/pull/890,closed,2106,0,3,28,9,70,0,0,[],2022-05-27 14:57:51+00:00,2022-07-07 20:20:46+00:00,3561775.0,"41 days, 5:22:55","[{'comment_id': 884154224, 'comment_body': 'This is not a great UX... can we include it in KerasNLP?', 'comment_created': datetime.datetime(2022, 5, 28, 16, 33, 48, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 884154295, 'comment_body': 'Use backticks for code keywords, e.g. `WordPieceTokenizer`', 'comment_created': datetime.datetime(2022, 5, 28, 16, 34, 59, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 884154313, 'comment_body': 'The class needs to be further introduced to the reader', 'comment_created': datetime.datetime(2022, 5, 28, 16, 35, 16, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 884154339, 'comment_body': 'This needs to be explained', 'comment_created': datetime.datetime(2022, 5, 28, 16, 35, 32, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 884154396, 'comment_body': 'Gotta credit the original example. You\'re using a lot of text copy from the original. It\'s also a useful reference for readers to see where they can see the ""everything from scratch"" workflow.', 'comment_created': datetime.datetime(2022, 5, 28, 16, 36, 29, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 884154477, 'comment_body': 'Again, use backticks for code keywords everywhere.', 'comment_created': datetime.datetime(2022, 5, 28, 16, 37, 55, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 886047981, 'comment_body': 'Actually, here, I am not referring to any code keyword. I am referring to WordPiece Tokenizer, and not the class `WordPieceTokenizer`.', 'comment_created': datetime.datetime(2022, 5, 31, 19, 22, 8, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 886048469, 'comment_body': 'Same comment as before.', 'comment_created': datetime.datetime(2022, 5, 31, 19, 22, 31, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 886049553, 'comment_body': 'Hmmm, not sure. @mattdangerw, thoughts?', 'comment_created': datetime.datetime(2022, 5, 31, 19, 23, 26, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 886106289, 'comment_body': ""I'm increasingly of the mind that we should wrap this tf text tool and the SentencePiece trainer (from SentencePiece pypi) in a universal trainer interface. It will really improve the UX here.\r\n\r\nBut I don't think we need to block this example on having it. IMO we should ship this using the tf text tool, and when we have a trainer class in KerasNLP replace the invocation here."", 'comment_created': datetime.datetime(2022, 5, 31, 20, 33, 54, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 886108022, 'comment_body': 'I would either say the\r\n\r\nThe WordPiece tokenization algorithm is a subword...\r\n\r\nor\r\n\r\n`WordPieceTokenizer` is a subword tokenizer;...', 'comment_created': datetime.datetime(2022, 5, 31, 20, 36, 17, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887274342, 'comment_body': 'for all of these, list the full simple `keras_nlp.tokenizers.WordPieceTokenizer`, then we can actually turn these into hyperlinks when we generate docs.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 33, 37, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887276978, 'comment_body': 'I would put this towards this top.\r\n\r\nThis example will show an encoder-decoder Transformer for English-to-Spanish using KerasNLP components. It is based on the [English-to-Spanish translation example](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/) by [fchollet](https://twitter.com/fchollet). The original example is more low-level and implements layers from scratch, this example uses KerasNLP to show some more advanced approaches, like sub-word tokenization and using metrics to estimate translation quality.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 37, 16, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887278492, 'comment_body': '@fchollet what should we do about dependencies like this for KerasNLP examples. I think we will need two (rouge-score and sentencepiece) as things go forward. Can we just add these to requirements.txt?', 'comment_created': datetime.datetime(2022, 6, 1, 20, 39, 19, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887279474, 'comment_body': 'This comment is `# This should be at least 30 for convergence` is there a reason this example needs less?', 'comment_created': datetime.datetime(2022, 6, 1, 20, 40, 41, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887280026, 'comment_body': 'newline after header.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 41, 18, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887281405, 'comment_body': 'here as well, list this as with the full symbol name for hyperlinks. see \r\nhttps://keras.io/guides/keras_nlp/transformer_pretraining/\r\nas an example.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 42, 19, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887282415, 'comment_body': 'TensorFlow Text makes it very simple to train WordPiece on a corpus, as described in this [guide](link location)', 'comment_created': datetime.datetime(2022, 6, 1, 20, 43, 29, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887289990, 'comment_body': 'I think you should either refer to things in plain english with no caps...\r\n\r\n""We also need a positional embedding layer"" or a with backticks. Every class we use should have at least once instance where we refer to it with the entire symbol path, so readers can get a hyperlink to browse documentation for that symbol.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 52, 28, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887291596, 'comment_body': 'this one feels a little aggressive, maybe go with the one from the original guide\r\n\r\nTom has never heard Mary sing.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 54, 42, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887291939, 'comment_body': 'I think you should expand this a bit, what is a bigram match?', 'comment_created': datetime.datetime(2022, 6, 1, 20, 55, 10, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887293777, 'comment_body': '""Earlier"" is unclear, maybe...\r\n\r\nEarlier, we would have had to define these classes. But now, all these layers can be used off-the-shelf from KerasNLP!\r\n\r\n->\r\n\r\nWith KerasNLP, we don\'t need to write these layers ourselves, we can use them straight from the library!', 'comment_created': datetime.datetime(2022, 6, 1, 20, 57, 44, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887295390, 'comment_body': 'I think we can remove this note now that we are showing better metrics at the bottom.', 'comment_created': datetime.datetime(2022, 6, 1, 20, 59, 56, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887300884, 'comment_body': 'let\'s just write this as `{""lower_case"": True}` and ditch the comment above.', 'comment_created': datetime.datetime(2022, 6, 1, 21, 4, 16, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887301127, 'comment_body': 'we can remove this right?', 'comment_created': datetime.datetime(2022, 6, 1, 21, 4, 27, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887309188, 'comment_body': 'One bigger comment for this guide and the other guide. Instead of doing string manipulation like this, which is fragile and cumbersome, let\'s add the start and end tokens after tokenization.\r\n\r\nIf you do the `format_dataset` call before batching, this should be pretty simple. Just concat `eng_tokenize.token_to_id(""[START]"")`, `eng_tokenizer(eng)`, and `eng_tokenize.token_to_id(""[END]"")`', 'comment_created': datetime.datetime(2022, 6, 1, 21, 15, 18, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 889002160, 'comment_body': '@mattdangerw, two questions:\r\n1. Don\'t we also want to show the tokenizer being called on a batched dataset? Won\'t users mostly use the tokenizer on a batch rather than a sample?\r\n2. We can\'t concatenate `eng_tokenize.token_to_id(""[START]"")`, `eng_tokenizer(eng)`, and `eng_tokenize.token_to_id(""[END]"")` directly. Rather, we\'d have to insert the `[END]` token right before the first occurrence of `[PAD]` (if padding tokens are present). If padding tokens are not present, we can just add `[END]` at the end. Right?', 'comment_created': datetime.datetime(2022, 6, 3, 14, 32, 19, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 889004165, 'comment_body': ""Modified it to this:\r\n\r\n```\r\nWe'll use accuracy as a quick way to monitor training progress on the validation data.\r\nNote that machine translation typically uses BLEU scores as well as other metrics,\r\nrather than accuracy. However, in order to use metrics like ROUGE, BLEU, etc. we\r\nwill have decode the probabilities and generate the text. Text generation is\r\ncomputationally expensive, and performing this during training is not recommended.\r\n```"", 'comment_created': datetime.datetime(2022, 6, 3, 14, 34, 34, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 889011726, 'comment_body': ""Hehe, sure. Actually, I chose random input sentences from the test set, I've changed it now."", 'comment_created': datetime.datetime(2022, 6, 3, 14, 43, 6, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 889016551, 'comment_body': ""Sorry about this! I was training the model on Colab and it was taking a while...so, I trained it for 20 epochs and obtained decent results. So, I changed this to 20. I'll put it back to 30 :P"", 'comment_created': datetime.datetime(2022, 6, 3, 14, 48, 33, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 889017273, 'comment_body': ""I'll also put it to train for 30 epochs so that I can report the correct ROUGE score."", 'comment_created': datetime.datetime(2022, 6, 3, 14, 49, 23, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 889701770, 'comment_body': ""@mattdangerw, looks like the model overfits after 10 epochs. Let's keep the number of epochs as 10, then.\r\n\r\n```\r\n__________________________________________________________________________________________________\r\nEpoch 1/30\r\n1302/1302 [==============================] - 118s 83ms/step - loss: 1.0494 - accuracy: 0.4150 - val_loss: 0.8318 - val_accuracy: 0.5127\r\nEpoch 2/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.7940 - accuracy: 0.5508 - val_loss: 0.6952 - val_accuracy: 0.5853\r\nEpoch 3/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.6803 - accuracy: 0.6098 - val_loss: 0.6260 - val_accuracy: 0.6276\r\nEpoch 4/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.6136 - accuracy: 0.6425 - val_loss: 0.5969 - val_accuracy: 0.6429\r\nEpoch 5/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.5767 - accuracy: 0.6621 - val_loss: 0.5845 - val_accuracy: 0.6526\r\nEpoch 6/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.5497 - accuracy: 0.6774 - val_loss: 0.5666 - val_accuracy: 0.6631\r\nEpoch 7/30\r\n1302/1302 [==============================] - 107s 83ms/step - loss: 0.5286 - accuracy: 0.6900 - val_loss: 0.5592 - val_accuracy: 0.6664\r\nEpoch 8/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.5114 - accuracy: 0.6999 - val_loss: 0.5542 - val_accuracy: 0.6700\r\nEpoch 9/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.4969 - accuracy: 0.7088 - val_loss: 0.5535 - val_accuracy: 0.6726\r\nEpoch 10/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.4845 - accuracy: 0.7161 - val_loss: 0.5527 - val_accuracy: 0.6716\r\nEpoch 11/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.4734 - accuracy: 0.7227 - val_loss: 0.5481 - val_accuracy: 0.6752\r\nEpoch 12/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.4636 - accuracy: 0.7282 - val_loss: 0.5451 - val_accuracy: 0.6787\r\nEpoch 13/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.4551 - accuracy: 0.7335 - val_loss: 0.5454 - val_accuracy: 0.6776\r\nEpoch 14/30\r\n1302/1302 [==============================] - 107s 83ms/step - loss: 0.4471 - accuracy: 0.7382 - val_loss: 0.5449 - val_accuracy: 0.6792\r\nEpoch 15/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.4401 - accuracy: 0.7427 - val_loss: 0.5480 - val_accuracy: 0.6759\r\nEpoch 16/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.4334 - accuracy: 0.7461 - val_loss: 0.5472 - val_accuracy: 0.6842\r\nEpoch 17/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.4264 - accuracy: 0.7507 - val_loss: 0.5445 - val_accuracy: 0.6829\r\nEpoch 18/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.4199 - accuracy: 0.7543 - val_loss: 0.5482 - val_accuracy: 0.6842\r\nEpoch 19/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.4139 - accuracy: 0.7580 - val_loss: 0.5511 - val_accuracy: 0.6845\r\nEpoch 20/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.4084 - accuracy: 0.7610 - val_loss: 0.5578 - val_accuracy: 0.6819\r\nEpoch 21/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.4027 - accuracy: 0.7644 - val_loss: 0.5572 - val_accuracy: 0.6849\r\nEpoch 22/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.3965 - accuracy: 0.7683 - val_loss: 0.5593 - val_accuracy: 0.6844\r\nEpoch 23/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.3921 - accuracy: 0.7707 - val_loss: 0.5575 - val_accuracy: 0.6867\r\nEpoch 24/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.3873 - accuracy: 0.7737 - val_loss: 0.5641 - val_accuracy: 0.6806\r\nEpoch 25/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.3826 - accuracy: 0.7764 - val_loss: 0.5630 - val_accuracy: 0.6892\r\nEpoch 26/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.3778 - accuracy: 0.7793 - val_loss: 0.5646 - val_accuracy: 0.6872\r\nEpoch 27/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.3738 - accuracy: 0.7815 - val_loss: 0.5697 - val_accuracy: 0.6820\r\nEpoch 28/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.3707 - accuracy: 0.7834 - val_loss: 0.5637 - val_accuracy: 0.6890\r\nEpoch 29/30\r\n1302/1302 [==============================] - 108s 83ms/step - loss: 0.3658 - accuracy: 0.7862 - val_loss: 0.5713 - val_accuracy: 0.6886\r\nEpoch 30/30\r\n1302/1302 [==============================] - 107s 82ms/step - loss: 0.3622 - accuracy: 0.7882 - val_loss: 0.5778 - val_accuracy: 0.6830\r\n```"", 'comment_created': datetime.datetime(2022, 6, 5, 14, 58, 51, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 890174994, 'comment_body': '>>> from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\r\n\r\n>> This is is not great UX \r\n\r\n> we should wrap this tf text tool and the SentencePiece trainer (from SentencePiece pypi) in a universal trainer interface.\r\n>\r\n> But I don\'t think we need to block this example on having it.\r\n\r\n+1. \r\n\r\nI rigged that up (for [this tutorial](https://www.tensorflow.org/text/guide/subwords_tokenizer)) back when there was _no_ public interface for learning a wordpiece vocab. I tried, but failed, to convince TFText team that this should probably be put in the `.adapt` method of a `WordpieceTokenizer` class. Their reasons, IIRC, were mainly ""you shouldn\'t use `.adapt` for any real problem. You\'ll want to use an apache-beam pipeline"".\r\n\r\nI find that one of the good arguments for keeping the vocabulary learner close to the tokenizer is just to be sure you\'re using the same normalization and splitting in both.', 'comment_created': datetime.datetime(2022, 6, 6, 13, 56, 6, tzinfo=datetime.timezone.utc), 'commenter': 'MarkDaoust', 'type': 'User'}, {'comment_id': 891527009, 'comment_body': 'add newline after heading', 'comment_created': datetime.datetime(2022, 6, 7, 17, 29, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891529722, 'comment_body': 'remove extra newline', 'comment_created': datetime.datetime(2022, 6, 7, 17, 32, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891531916, 'comment_body': 'a bunch of extra spaces at the start of lines for this paragraph?', 'comment_created': datetime.datetime(2022, 6, 7, 17, 35, 15, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891533823, 'comment_body': 'I wouldn\'t say ""(word tokenizers have the issue of many OOV tokens)"". That\'s not really accurate. I would more saw word tokenizers need very large vocabularies for good coverage of input words.', 'comment_created': datetime.datetime(2022, 6, 7, 17, 37, 16, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891534876, 'comment_body': ""Let's remove this, the blog is more about the speedup of a variation of word piece that explaining the approach. The paragraph above is enough intro."", 'comment_created': datetime.datetime(2022, 6, 7, 17, 38, 29, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891535935, 'comment_body': 'put quotes inside the backticks so it is clear these are strings', 'comment_created': datetime.datetime(2022, 6, 7, 17, 39, 46, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891537650, 'comment_body': 'why are you overriding the split pattern here? by default you get things like splitting ""hello!"" into ""hello"", ""!"" which is usually what you want.', 'comment_created': datetime.datetime(2022, 6, 7, 17, 41, 43, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891538252, 'comment_body': 'quote around strings in the backticks', 'comment_created': datetime.datetime(2022, 6, 7, 17, 42, 27, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891541322, 'comment_body': 'generally you want to call dataset.map with num_parallel_calls=tf.data.AUTOTUNE', 'comment_created': datetime.datetime(2022, 6, 7, 17, 45, 52, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891543441, 'comment_body': 'nit: maybe remove this sentence? At this point I think the users will get that KerasNLP is providing a lot of layers, and we can focus on the details. This feels too ""sales-y""', 'comment_created': datetime.datetime(2022, 6, 7, 17, 47, 54, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891559803, 'comment_body': 'I would call this method `preprocesss` or `preprocess_batch`', 'comment_created': datetime.datetime(2022, 6, 7, 18, 5, 32, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891560613, 'comment_body': 'Could you replace this line with `tf.fill((batch_size, 1), start_token)`, and ditto for end_token? Might be more readable', 'comment_created': datetime.datetime(2022, 6, 7, 18, 6, 32, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891563220, 'comment_body': 'Might be more readable to remove `normalize_length` and show the full shape explicitly...\r\n\r\n```\r\n# Truncate or pad text to constant length.\r\neng = eng.to_tensor(shape=(batch_size, MAX_SEQUENCE_LENGTH))\r\nspa = spa.to_tensor(shape=(batch_size, MAX_SEQUENCE_LENGTH + 1))\r\n```', 'comment_created': datetime.datetime(2022, 6, 7, 18, 9, 48, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891567780, 'comment_body': 'should we be stripping the [START] and [END] as well? or is that in the reference too?', 'comment_created': datetime.datetime(2022, 6, 7, 18, 15, 16, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891569995, 'comment_body': 'Is there a way we could put this in more context? maybe we could show rouge-1 and rouge-2 to show that the word overlap is higher than the bigram overlap?', 'comment_created': datetime.datetime(2022, 6, 7, 18, 17, 45, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 891571088, 'comment_body': ""just curious, does this go up if you train for 20 epochs? i'm not sure if overfitting on next word predictions would necessarily mean rouge goes down for the entire sequence"", 'comment_created': datetime.datetime(2022, 6, 7, 18, 19, 5, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 892601325, 'comment_body': ""Actually, I've made a mistake by adding space at the beginning of lines. I've removed those now."", 'comment_created': datetime.datetime(2022, 6, 8, 16, 32, 10, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 892622861, 'comment_body': 'Thanks!', 'comment_created': datetime.datetime(2022, 6, 8, 16, 48, 22, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 892625237, 'comment_body': ""@mattdangerw, won't this break if the last batch does not have `batch_size` number of samples? The last batch will have `total_no_of_samples % batch_size` samples, right?"", 'comment_created': datetime.datetime(2022, 6, 8, 16, 49, 49, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 892627694, 'comment_body': 'Interesting. Thanks!', 'comment_created': datetime.datetime(2022, 6, 8, 16, 51, 28, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 892628486, 'comment_body': ':P', 'comment_created': datetime.datetime(2022, 6, 8, 16, 52, 1, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 894318088, 'comment_body': '@mattdangerw, this is another reason why I opened this issue: https://github.com/keras-team/keras-nlp/issues/209. We need to protect the reserved tokens. Otherwise, this is what happens:\r\n\r\n```\r\n>>> spa_tokenizer(""[START] eres demasiado muy muy joven. [END]"")\r\n<tf.Tensor: shape=(12,), dtype=int32, numpy=\r\narray([  1,   1,   1, 184, 187,  97,  97, 356,  15,   1,   1,   1],\r\n      dtype=int32)>\r\n```\r\n\r\nEarlier, I was adding the special tokens to the string and then tokenizing. Now that we are tokenizing first and then adding the special tokens, I can remove `split_pattern="" ""`. But I do think that we should add a `special_tokens` arg in our tokenizers.', 'comment_created': datetime.datetime(2022, 6, 10, 9, 5, 33, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 894349460, 'comment_body': 'Nice catch. Forgot to do this.', 'comment_created': datetime.datetime(2022, 6, 10, 9, 42, 12, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 894431198, 'comment_body': ""10 epochs:\r\n```\r\nROUGE-1 Score:  {'rouge-n_precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.575053>, 'rouge-n_recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.5421237>, 'rouge-n_f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.5522118>}\r\nROUGE-2 Score:  {'rouge-n_precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.40087304>, 'rouge-n_recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.37767088>, 'rouge-n_f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.38413975>}\r\n```\r\n\r\n20 epochs:\r\n```\r\nROUGE-1 Score:  {'rouge-n_precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.4181>, 'rouge-n_recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.3744375>, 'rouge-n_f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.39165395>}\r\nROUGE-2 Score:  {'rouge-n_precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.19518518>, 'rouge-n_recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.18228835>, 'rouge-n_f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.18680958>}\r\n```"", 'comment_created': datetime.datetime(2022, 6, 10, 11, 36, 10, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 899354074, 'comment_body': ""All links should be on a single line (otherwise they ddon't render)"", 'comment_created': datetime.datetime(2022, 6, 16, 17, 27, 48, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 899355733, 'comment_body': 'Please rephrase to sound more professional-casual, e.g. ""Don\'t worry if you aren\'t familiar with KerasNLP. This tutorial will start with the basics. Let\'s dive in.""', 'comment_created': datetime.datetime(2022, 6, 16, 17, 29, 57, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 899356149, 'comment_body': ""The above code is fine. We don't need to update requirements.txt"", 'comment_created': datetime.datetime(2022, 6, 16, 17, 30, 30, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 899356396, 'comment_body': ""What's the current status here?"", 'comment_created': datetime.datetime(2022, 6, 16, 17, 30, 53, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 899357801, 'comment_body': ""Section titles shouldn't capitalize all words (just the first one)"", 'comment_created': datetime.datetime(2022, 6, 16, 17, 32, 26, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 899396817, 'comment_body': 'I think the plan is to add a wrapper for this in KerasNLP (not sure), but we plan to roll with the current code for now, and update this example later once it is a part of KerasNLP. @mattdangerw, please correct me if I am wrong.', 'comment_created': datetime.datetime(2022, 6, 16, 18, 17, 41, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 899398340, 'comment_body': 'Is there a reason for this? Curious. Because generally, we follow ""Title Case"" for headings, right?', 'comment_created': datetime.datetime(2022, 6, 16, 18, 19, 34, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 899514662, 'comment_body': 'https://developers.google.com/style/capitalization#capitalization-in-titles-and-headings', 'comment_created': datetime.datetime(2022, 6, 16, 20, 55, 49, tzinfo=datetime.timezone.utc), 'commenter': 'MarkDaoust', 'type': 'User'}, {'comment_id': 900344517, 'comment_body': ""Yep! I think that's the plan right now!"", 'comment_created': datetime.datetime(2022, 6, 17, 17, 0, 7, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 900345561, 'comment_body': ""Let's use the StartEndPacker here."", 'comment_created': datetime.datetime(2022, 6, 17, 17, 0, 49, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 900813600, 'comment_body': 'Interesting. Thanks!', 'comment_created': datetime.datetime(2022, 6, 18, 10, 50, 23, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 902036767, 'comment_body': 'Add line break before list', 'comment_created': datetime.datetime(2022, 6, 20, 22, 50, 51, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 910528000, 'comment_body': 'We can now replace this with pip install -q keras-nlp', 'comment_created': datetime.datetime(2022, 6, 30, 1, 24, 40, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '2f3541952286dc938820b0e397c5dce65a794326', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cb5dd07997375f83ef5fc98ecef5bd1d477eeab7', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6d0f6c2f2970fc7181ed6d319882717644b09215', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b0ad24f687e87bebb7ecb428b910702e392569f7', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a657308857075a073853cf0b559d944929d6618e', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '48274b1cc3d2a057f069eb1a22a8fbc6d6ba11f7', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd789ae5fc4b9862ad1939b26ddabc52c72741d65', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5f98b989810acdba0995474a4ef5887e6e8aff98', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c9c742cc7cf9bd045acfdc1d7cd475c2ee3536a4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4049052b24c0dbd610581ef1944f6a0ff9de2cab', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9c9b058d9e6f1cab9252e8e77249fd947de2b651', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9c48e2a6bccac8d84dc711b1ba2c3960f141b1f2', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2e75e3f13b68a49309a7e4df401e91481eca1520', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3433af182014eac9bc453c7d0ab387f8b12b70b6', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '85073e49f6b4ba308f84a22452f8b9b2c516f55e', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9eff55799d0551a6fa11fc002d33d3e96b033085', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4c76a5e566a23bc555d8cf45015de2ff201fddba', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ad2ce3c1b98aa6651a87e54fc38dc7a82beab7b4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c28e7e57562d9953d4261a5012e5b26587962b99', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ce1246b66c0068f9cd2818dfb7435577ae72ead8', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ba9b0f1c92d7b5aef8ab55f4a79a9b1c52191ced', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '501fb4cbe62e5b5c434393f512a452ef833640a1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd6df099fd5a68d0e80547e3566418b9590fc00e1', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9a5a5365d2e4d587b5a6b82a8908fa5fc0a47b78', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8ece94d657b12379b42b184e24742073bdf2025a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'caecf10ba0ba8a79b7290aa5e7f28f694d2f92cb', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ff1a621f1776416d089adcbcabdbb2920d24850e', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a21fad67cd8eedc9f833ebd23419533a3b720a19', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
954734089,Text Classification with FNet [KerasNLP],"Resolves https://github.com/keras-team/keras-nlp/issues/213

Dataset: IMDb
Compared results with Transformer model",True,898,https://api.github.com/repos/keras-team/keras-io/pulls/898,https://github.com/keras-team/keras-io/pull/898,closed,1682,0,3,26,5,36,0,0,[],2022-06-01 19:48:01+00:00,2022-06-29 22:13:47+00:00,2427946.0,"28 days, 2:25:46","[{'comment_id': 887303179, 'comment_body': ""Can't we lowercase inside the tokenizer? Why do that here?"", 'comment_created': datetime.datetime(2022, 6, 1, 21, 6, 38, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887305362, 'comment_body': 'TensorFlow Text makes it very simple to train WordPiece on a corpus, as described in this [guide](link location)', 'comment_created': datetime.datetime(2022, 6, 1, 21, 9, 39, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887306350, 'comment_body': 'Same comments as other guide... `bert_tokenizer_params={""lower_case""=True}` remove `Arguments for `text.BertTokenizer` comment, remove `learn_params`.\r\n', 'comment_created': datetime.datetime(2022, 6, 1, 21, 10, 55, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887309925, 'comment_body': ""I don't believe you need [START] and [END], and you aren't using them. Please remove."", 'comment_created': datetime.datetime(2022, 6, 1, 21, 16, 28, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887310405, 'comment_body': 'element = train_ds.take(1).get_single_element()', 'comment_created': datetime.datetime(2022, 6, 1, 21, 17, 12, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887310670, 'comment_body': 'full paths to these symbols generally, for hyperlinking', 'comment_created': datetime.datetime(2022, 6, 1, 21, 17, 34, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887313101, 'comment_body': 'We generally try to show downloading and using source file directly. It is more flexible when copying and updating a guide to a new dataset. You can see how to download sst directly in our current guide for KerasNLP', 'comment_created': datetime.datetime(2022, 6, 1, 21, 21, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887317403, 'comment_body': ""I would generally tighten up this section. Examples shouldn't have a ton of offhand comments, we should focus on what is show in this guide. This reads a little too much like a blog currently.\r\n\r\nRoughly, we should just say...\r\n\r\n - BERT, RoBERTa, etc have shown the effectiveness of using transformers to compute a rich embedding for input text.\r\n - However transformers are expensive, an ongoing question is how to lower the compute requirements.\r\n - In this guide we will focus on FNet, which replace the expensive attention mechanism with a Fourier transform.\r\n - We will show how this can speed up training, without significantly degrading performance."", 'comment_created': datetime.datetime(2022, 6, 1, 21, 28, 23, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887318338, 'comment_body': 'Why is our speedup so much less pronounced? Are we including compilation time in the total time? If we grow the model would the speedup become clearer?', 'comment_created': datetime.datetime(2022, 6, 1, 21, 29, 59, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887319439, 'comment_body': 'I would state this point a little more clearly--We can see that FNet significantly speeds up our run time, with only a small sacrifice in overall accuracy.', 'comment_created': datetime.datetime(2022, 6, 1, 21, 31, 15, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887419672, 'comment_body': 'Why are we using nightly?', 'comment_created': datetime.datetime(2022, 6, 2, 1, 19, 13, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887419992, 'comment_body': 'All of these are hypers?', 'comment_created': datetime.datetime(2022, 6, 2, 1, 20, 4, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887420974, 'comment_body': ""I feel it's okay to use tfds here, which already does the splitting. The hassle I see from data loading is usually not how to switch between tfds and other sources, but how to find sources."", 'comment_created': datetime.datetime(2022, 6, 2, 1, 22, 56, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887421259, 'comment_body': ""One more thing - let's be more concise in the description, if we choose to use tfds, just say load SST-2 from Tensorflow Datasets (TFDS)."", 'comment_created': datetime.datetime(2022, 6, 2, 1, 23, 39, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887422720, 'comment_body': 'vocabulary is not the ""input"" to tokenizer. We could say ""configure the tokenizer with vocabulary trained above.""', 'comment_created': datetime.datetime(2022, 6, 2, 1, 27, 40, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887429015, 'comment_body': 'This was a request from @fchollet when I was doing my guide, so maybe we should discuss with him? Personally not particularly opinionated. ', 'comment_created': datetime.datetime(2022, 6, 2, 1, 44, 57, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887533854, 'comment_body': ""Ah okay, let's bring this up in the team chat. I feel since tfds is part of TF ecosystem and still being maintained, we should try using their product. "", 'comment_created': datetime.datetime(2022, 6, 2, 3, 47, 17, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 894455283, 'comment_body': 'Right. Changes made ðŸ‘ðŸ¼ . Sorry for the rather verbose introduction! :P', 'comment_created': datetime.datetime(2022, 6, 10, 12, 11, 22, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 894633129, 'comment_body': ""Hey,  the SST-2 dataset has very short sequences. I tried it with the IMDb dataset (which has longer sequences) and I'm getting a noticeable speed-up :D"", 'comment_created': datetime.datetime(2022, 6, 10, 15, 8, 56, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 894634013, 'comment_body': ""I used nightly because it has the `huggingface:sst` dataset. I've removed it now because I am using the IMDb dataset."", 'comment_created': datetime.datetime(2022, 6, 10, 15, 9, 55, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 894634399, 'comment_body': ""Removed TFDS. We don't need it for IMDb :)"", 'comment_created': datetime.datetime(2022, 6, 10, 15, 10, 20, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 894634775, 'comment_body': 'It lowercases the special tokens as well, which I wanted to avoid.', 'comment_created': datetime.datetime(2022, 6, 10, 15, 10, 46, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 896301191, 'comment_body': '2022...?', 'comment_created': datetime.datetime(2022, 6, 14, 1, 47, 53, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 896304378, 'comment_body': 'One question - here you are chaining three `ls` commands, do all of these prints get shown?', 'comment_created': datetime.datetime(2022, 6, 14, 1, 56, 30, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 896307482, 'comment_body': 'a layer that maps every token in input sequence to a vector.', 'comment_created': datetime.datetime(2022, 6, 14, 2, 4, 31, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 896310700, 'comment_body': ""Ah, man. Looks like I'm still mentally stuck in 2021 ðŸ˜† . Changed!"", 'comment_created': datetime.datetime(2022, 6, 14, 2, 12, 37, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 896311275, 'comment_body': ""Hmmm, I'll have to check this by generating the `.ipynb` file. Ideally, it should print all. I'll generate the `.iypnb` file and let you know."", 'comment_created': datetime.datetime(2022, 6, 14, 2, 14, 20, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 896394973, 'comment_body': 'None of this will get outputted in the rendered example the way things work on keras.io. If you want to do that you would need to actually os.listdir or something', 'comment_created': datetime.datetime(2022, 6, 14, 5, 43, 17, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 897459929, 'comment_body': 'Ah, I see. Will change.', 'comment_created': datetime.datetime(2022, 6, 15, 2, 9, 43, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 899366485, 'comment_body': ""Don't do this, instead use `keras.utils.set_random_seed()`"", 'comment_created': datetime.datetime(2022, 6, 16, 17, 42, 36, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 899366622, 'comment_body': 'Same as in the other example -- this should be hidden away', 'comment_created': datetime.datetime(2022, 6, 16, 17, 42, 47, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 899367141, 'comment_body': ""Use `from tensorflow import keras` so you don't have to use `tf.keras` everywhere"", 'comment_created': datetime.datetime(2022, 6, 16, 17, 43, 27, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 899367281, 'comment_body': 'Add line break above', 'comment_created': datetime.datetime(2022, 6, 16, 17, 43, 39, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 899367502, 'comment_body': 'Add line break before list', 'comment_created': datetime.datetime(2022, 6, 16, 17, 43, 55, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 899367740, 'comment_body': 'Only capitalize the first word in a section title', 'comment_created': datetime.datetime(2022, 6, 16, 17, 44, 12, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 902044624, 'comment_body': 'Does this get rendered as a nested list when you generate the website?', 'comment_created': datetime.datetime(2022, 6, 20, 23, 16, 2, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}]","[{'commit_sha': 'd106d8cd994e1f55aa21edaaa68f9520b58ff8b2', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bf668372f28cbc9a97b17abb63d3e928c924eb89', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bef06eda76f48c7e624bff6b0f2853a2c54ddd96', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5320271790dffb1ccb079f87dd941b68ebda7a88', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b460836688c869c20bc64286c2599ecec0d54ce2', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8be2947c7f6b5a7392cb39a1b7e0848f725b4f57', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'eb437ea06fe6f873d87156b7e1206f1d25a596f9', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ba2b01c255e867db91efdec244f24c6d0c9bb623', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dfbaf9b2f96917e67f44edf2c7cb06ebff72bbad', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1beaef37ef293c31b9fe1cd904f95e22af2987c7', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '426a23c1aee37ec1efc149eb564929185c34cb4a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fcd15add49848925df538e78d60e7bf4c9b1e912', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a6fbceaecdca1c150ca456aad8615a90cba7d5ec', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '61fff5b8545efb8ec0ffaa119ffa25ad30936d37', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '653efec051cf5ad149f93795a218fdd7c27d1afb', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4143538ffc7311d4b2debe45c9a3c26973de81fb', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bf9304401fc50cb406329bd95818ee35f44ab5b4', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5cb9cdfabcaaeec4b70141815a7d4b8d45509f9e', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '46708ca828469a282943bb41c665025b9b516896', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7284c3f53dc84213f12d973d23d4fb0532774739', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0e5f1f3216a00239cf59a39ce74fa4eb9f486652', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '60c0c8d179fc9743f47acc05ebf253a458e62d60', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dd53d3928f3cb33f979edaa0f45a73ad67938726', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '43d48dc7945dfec45c9014534e2f491669906f39', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3775e4ffb3c58550e1fa0538164308a9deae8048', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '42b8362166e36cc3be7c2818446f57926b0aaaee', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
984102271,Update the FNet Classification Example [KerasNLP],Updated the results. More accurate now.,True,953,https://api.github.com/repos/keras-team/keras-io/pulls/953,https://github.com/keras-team/keras-io/pull/953,closed,42,42,3,1,0,0,0,0,[],2022-06-30 17:16:47+00:00,2022-07-01 17:28:10+00:00,87083.0,"1 day, 0:11:23",[],"[{'commit_sha': 'ee2a502afc9df15faee1f5944d23f3ed99131eea', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1025525894,Update English-to-Spanish NMT Example [KerasNLP],"Resolves https://github.com/keras-team/keras-nlp/issues/259 and https://github.com/keras-team/keras-nlp/issues/190
- BLEU
- Use KerasNLP's WordPiece Tokeniser",False,1024,https://api.github.com/repos/keras-team/keras-io/pulls/1024,https://github.com/keras-team/keras-io/pull/1024,closed,171,142,3,5,0,4,0,0,[],2022-08-13 06:27:59+00:00,2023-07-09 12:19:44+00:00,28533105.0,"330 days, 5:51:45","[{'comment_id': 950518874, 'comment_body': 'This should really be avoided -- we should install the latest version from pip.', 'comment_created': datetime.datetime(2022, 8, 19, 19, 53, 40, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 950519178, 'comment_body': 'N-grams', 'comment_created': datetime.datetime(2022, 8, 19, 19, 54, 12, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 950664507, 'comment_body': ""@fchollet, the reason for doing this is that certain APIs used in this example are not yet a part of any KerasNLP release. That's why I am using a particular commit for installation. Should we wait for the next KerasNLP release?\r\n\r\nCC: @mattdangerw, @chenmoneygithub"", 'comment_created': datetime.datetime(2022, 8, 20, 7, 47, 13, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 950740381, 'comment_body': ""I'd suggest so, yes."", 'comment_created': datetime.datetime(2022, 8, 20, 20, 55, 55, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}]","[{'commit_sha': '4a6e2d1d00968e237dbde7436e9a1151e7d0c59a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b4ddbdbdb96de05f526e162fedbec7acafb4d603', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4b8e71c952325ab0f1474d93e10821b91f0a2c18', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4ad7f39f78b2d47743a4a35fd578e6514d327b8d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '71fad9867d4c0968b99b1f8a8bd3728aa0e5d124', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
1025545937,Update Text CLS with FNet Example with KerasNLP's WP Trainer,,False,1025,https://api.github.com/repos/keras-team/keras-io/pulls/1025,https://github.com/keras-team/keras-io/pull/1025,closed,115,144,3,3,0,1,0,0,[],2022-08-13 08:14:24+00:00,2023-07-09 12:20:00+00:00,28526736.0,"330 days, 4:05:36","[{'comment_id': 950520911, 'comment_body': 'Same comment -- the problem with a line like this is that it will be out of date extremely quickly.', 'comment_created': datetime.datetime(2022, 8, 19, 19, 57, 9, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}]","[{'commit_sha': 'af860a47352ca51970a79669b9d806522d1c4439', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5d71d9cdfa18d6b55019fcb24537df2cc7cf2860', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd64a26a5afceaf22cfcd477968bf410f8990aafc', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
964545773,Add StartEndPacker layer,Resolves #220 ,True,221,https://api.github.com/repos/keras-team/keras-nlp/pulls/221,https://github.com/keras-team/keras-nlp/pull/221,closed,318,4,4,10,2,13,0,0,[],2022-06-10 20:44:29+00:00,2022-06-16 01:13:40+00:00,448151.0,"5 days, 4:29:11","[{'comment_id': 896263085, 'comment_body': 'Might be good to add a short intro paragraph.\r\n\r\nThis layer is useful when tokenizing inputs for tasks like translation, where each sequence should be marked with a start and end marker. It should be called after tokenization. The layer will first trim inputs to fit, then add start/end tokens, and finally pad if necessary to `sequence_length`.', 'comment_created': datetime.datetime(2022, 6, 14, 0, 2, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 896277589, 'comment_body': 'Before this should probably do the same\r\n\r\n```\r\nif not isinstance(inputs, tf.Tensor) or isinstance(inputs, tf.RaggedTensor):\r\n    inputs = tf.conver_to_tensor(inputs)\r\n```\r\n\r\nThat will allow us to support things like numpy or list inputs. Useful for demos.', 'comment_created': datetime.datetime(2022, 6, 14, 0, 42, 32, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 896278685, 'comment_body': 'I think it would be more helpful to say....\r\n\r\nInput must be either dense and rank 1 or ragged and rank 2. Received dense input with rank={...}', 'comment_created': datetime.datetime(2022, 6, 14, 0, 45, 39, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 896278727, 'comment_body': 'Similar edit here', 'comment_created': datetime.datetime(2022, 6, 14, 0, 45, 47, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 896278950, 'comment_body': 'add a config test', 'comment_created': datetime.datetime(2022, 6, 14, 0, 46, 24, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 896296865, 'comment_body': 'Please include the error message (just prefix works) to ensure we are capturing the right error.', 'comment_created': datetime.datetime(2022, 6, 14, 1, 36, 27, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 896296957, 'comment_body': 'Same here', 'comment_created': datetime.datetime(2022, 6, 14, 1, 36, 39, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 896297712, 'comment_body': 'Right. Thanks! Pretty much copied it verbatim :)', 'comment_created': datetime.datetime(2022, 6, 14, 1, 38, 46, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 896299150, 'comment_body': 'ðŸ‘ðŸ¼ . I\'ve removed the `else` block below:\r\n\r\n```\r\n        else:\r\n            raise ValueError(\r\n                ""Input must be of type `tf.Tensor` or `tf.RaggedTensor`, ""\r\n                f""but got {type(inputs)}""\r\n            )\r\n```', 'comment_created': datetime.datetime(2022, 6, 14, 1, 42, 27, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 896308736, 'comment_body': ""Done! I've reported the whole error message. Hope that's okay?"", 'comment_created': datetime.datetime(2022, 6, 14, 2, 7, 37, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 897256391, 'comment_body': 'Actually one more issue here. We need to support the case where the static shape of the batch size is `None`. I think you will need to call `tf.shape(inputs)[0]` to get the dynamic batch size.\r\n\r\nWe should also add another unit test, using tf.data.map, and calling batch() on the dataset before applying the layer. That should catch this bug.', 'comment_created': datetime.datetime(2022, 6, 14, 19, 44, 35, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 897529992, 'comment_body': 'One minor thing - I would rename this to `input_is_dense` to match `input_is_ragged`.', 'comment_created': datetime.datetime(2022, 6, 15, 4, 47, 7, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 897544188, 'comment_body': 'Done!', 'comment_created': datetime.datetime(2022, 6, 15, 5, 18, 1, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}]","[{'commit_sha': 'e0905d211f487569db152198e92f58cbdc393735', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '19e9d37a53f1550f58dc9268fadc58c5e4400df3', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e0c5074e9e8c53fe7f75c3b13a4bda4df2bc8a3b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6fb2777132abc0ba0d08f187f12247b393439cee', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5a38a9df198f40d72dad804baf08fbfcef7449fe', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '50638b2cb477ae84b21efc7d22948a35f4c44060', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '53b73942ccfba9994a934c729afe8a45e5fab9a7', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8b854bfa99d54ca1bde1f7d1e7fab41d14c3590b', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e9559593183a65af452090ff3f8da54848dcb8f0', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b768b5b99cb48f2e255fefe9b383f7fe3cd3162d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96
890396766,Add Perplexity Metric,"Resolves #63.

Notebook: https://colab.research.google.com/drive/1XV1h5aeiy5IlHoQFjDTJ45hRC8wMSf16?usp=sharing
HF Reference: https://github.com/huggingface/transformers/blob/main/examples/research_projects/codeparrot/scripts/validation_loss.py#L56-L69",True,68,https://api.github.com/repos/keras-team/keras-nlp/pulls/68,https://github.com/keras-team/keras-nlp/pull/68,closed,552,0,4,25,11,55,0,0,[],2022-03-26 14:54:13+00:00,2022-04-13 01:16:34+00:00,1506141.0,"17 days, 10:22:21","[{'comment_id': 839067479, 'comment_body': 'Should this be `tf.equal(y_true, self.pad_token_id)`?', 'comment_created': datetime.datetime(2022, 3, 31, 0, 24, 42, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839067480, 'comment_body': 'Should this be `tf.equal(y_true, self.pad_token_id)`?', 'comment_created': datetime.datetime(2022, 3, 31, 0, 24, 42, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839067652, 'comment_body': 'bsz is vague, please just use full name batch_size', 'comment_created': datetime.datetime(2022, 3, 31, 0, 25, 7, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839067653, 'comment_body': 'bsz is vague, please just use full name batch_size', 'comment_created': datetime.datetime(2022, 3, 31, 0, 25, 7, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839072148, 'comment_body': 'Curious - why are you doing these reshaping? I think `SparseCategoricalCrossentropy` can handle the original shape? \r\n\r\n```\r\ny_true = tf.constant([[1, 1, 0], [0, 2, 1]])\r\ny_pred = tf.random.uniform(shape=[2, 3, 3])\r\n\r\nentropy = tf.keras.losses.SparseCategoricalCrossentropy(\r\n            from_logits=True, reduction=""sum""\r\n        )\r\nentropy(y_true, y_pred)\r\n```\r\nthe above code can run, am I missing something?\r\n\r\n', 'comment_created': datetime.datetime(2022, 3, 31, 0, 36, 13, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839072152, 'comment_body': 'Curious - why are you doing these reshaping? I think `SparseCategoricalCrossentropy` can handle the original shape? \r\n\r\n```\r\ny_true = tf.constant([[1, 1, 0], [0, 2, 1]])\r\ny_pred = tf.random.uniform(shape=[2, 3, 3])\r\n\r\nentropy = tf.keras.losses.SparseCategoricalCrossentropy(\r\n            from_logits=True, reduction=""sum""\r\n        )\r\nentropy(y_true, y_pred)\r\n```\r\nthe above code can run, am I missing something?\r\n\r\n', 'comment_created': datetime.datetime(2022, 3, 31, 0, 36, 13, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839144284, 'comment_body': 'Ah, yes. Fixed ðŸ‘ðŸ¼ ', 'comment_created': datetime.datetime(2022, 3, 31, 3, 47, 38, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 839146622, 'comment_body': ""Yeah, you are right. Actually, the reason why I do reshaping is here: https://github.com/huggingface/transformers/blob/main/examples/research_projects/codeparrot/scripts/validation_loss.py#L56-L69 and https://github.com/huggingface/transformers/blob/v4.17.0/src/transformers/models/bert/modeling_bert.py#L1363.\r\n\r\n\r\nEssentially, in PyTorch, when I compute the Cross Entropy loss and set the reduction method to `mean`, it computes the mean over the non-masked tokens. So, it sums the values of the non-masked tokens and **divides by the number of non-masked tokens**.\r\n\r\nHowever, when I tried out some experiments with `tf.keras.losses.SparseCategoricalCrossentropy` and set the reduction method to `mean`, it sums over the non-masked tokens but **divides by number of ALL tokens** (basically, the first dimension). So, initially, I'd set the reduction method to `mean`, but changed it to `sum`, and handled the denominator in subsequent lines. \r\n\r\nNow that we have set the reduction to `sum`, I can remove the lines where I do reshaping. Thanks for pointing this out!"", 'comment_created': datetime.datetime(2022, 3, 31, 3, 54, 51, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 839844205, 'comment_body': 'You need to add an import of metrics from the init file one directory up, otherwise the imports will not work on the exported package.', 'comment_created': datetime.datetime(2022, 3, 31, 17, 11, 40, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 839864142, 'comment_body': 'We should add a lot of returns here. Blank line after the one liner, blank line after paragraph, blank line before Args: and Examples:', 'comment_created': datetime.datetime(2022, 3, 31, 17, 35, 58, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 839867416, 'comment_body': 'there is a dtype property on metric, so this can be accessed as self.dtype\r\n\r\n(many other instances)', 'comment_created': datetime.datetime(2022, 3, 31, 17, 39, 59, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 839927988, 'comment_body': 'What\'s this ""fixed-size windows""?', 'comment_created': datetime.datetime(2022, 3, 31, 18, 57, 39, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839933966, 'comment_body': 'Prefer using this: \r\n```\r\ntarget = tf.random.uniform(shape=[2, 5],  maxval=10, dtype=tf.int32)\r\n```\r\n\r\nSince after the numpy stuff graduates from experimental namespace, this code would break. \r\n\r\nAdditionally, we may want to format this example section like this: https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/tokenizers/word_piece_tokenizer.py#L123, which is more clear on the output, and more testable. ', 'comment_created': datetime.datetime(2022, 3, 31, 19, 5, 49, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839934397, 'comment_body': 'Please use python3 style:\r\n```\r\nsuper().__init__(name=name, dtype=dtype, **kwargs)\r\n```', 'comment_created': datetime.datetime(2022, 3, 31, 19, 6, 26, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839934861, 'comment_body': 'Shall we move this default to the argument? ', 'comment_created': datetime.datetime(2022, 3, 31, 19, 7, 10, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839935293, 'comment_body': 'This should be a private member, since it is not directly passed from constructor argument.', 'comment_created': datetime.datetime(2022, 3, 31, 19, 7, 49, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839935512, 'comment_body': 'should be private.', 'comment_created': datetime.datetime(2022, 3, 31, 19, 8, 7, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839935776, 'comment_body': 'Same here, should be private.', 'comment_created': datetime.datetime(2022, 3, 31, 19, 8, 29, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839939241, 'comment_body': 'Curious: why are you using this classmethod `setUpClass` instead of `setUp(self)`?\r\n\r\nAlso I would prefer setting the value of x, y in separate test cases - the disadvantage is we are copying code around, but the advantage is that the test case is more clear on its purposes and data.', 'comment_created': datetime.datetime(2022, 3, 31, 19, 13, 20, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839942817, 'comment_body': 'Instead of using random y_pred and put a magic number here, I would prefer to arbitrarily set `y_pred` to some fixed number, and compute the expected value manually.', 'comment_created': datetime.datetime(2022, 3, 31, 19, 18, 20, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 839982062, 'comment_body': 'Reference: https://huggingface.co/docs/transformers/perplexity\r\nDiscussion with @mattdangerw: https://github.com/keras-team/keras-nlp/issues/38#issuecomment-1069802876', 'comment_created': datetime.datetime(2022, 3, 31, 20, 14, 36, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 839982171, 'comment_body': 'Sure, will change ðŸ‘ðŸ¼ ', 'comment_created': datetime.datetime(2022, 3, 31, 20, 14, 46, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 839982476, 'comment_body': ""You could do the core keras doc style here to show output...\r\nhttps://github.com/keras-team/keras/blob/master/keras/metrics/metrics.py#L74\r\n\r\nWe don't have checks for this now, but we hope to add those soon. Output would make these more readable.\r\n\r\nAlso consider breaking these up with headers outside the code block, e.g.\r\n\r\nCall the metric directly:\r\nsome code\r\n\r\nSet padding id:\r\nsome code"", 'comment_created': datetime.datetime(2022, 3, 31, 20, 15, 15, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 839983396, 'comment_body': ""Do you mean moving it to `kwargs`?\r\n\r\nI'm planning to handle it this way: https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/tokenizers/word_piece_tokenizer.py#L177-L186."", 'comment_created': datetime.datetime(2022, 3, 31, 20, 16, 35, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 839984729, 'comment_body': ""`setUp()` runs before every unit test, whereas, `setUpClass()` runs only once before all the UTs.\r\n\r\nFor some reason, even though I've set the random seed, `setUp()` gives different outputs for every test. That's why I used `setUpClass()`."", 'comment_created': datetime.datetime(2022, 3, 31, 20, 18, 28, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 839985455, 'comment_body': 'Ah, so do you mean that we should do something like this?\r\n\r\n```\r\ny_pred = tf.constant([[...], [...]], dtype=...)\r\n``` \r\n\r\nAs in, for every UT, we should set y_pred explicitly?', 'comment_created': datetime.datetime(2022, 3, 31, 20, 19, 30, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 839986156, 'comment_body': ""Sure, I'll add x and y to every UT."", 'comment_created': datetime.datetime(2022, 3, 31, 20, 20, 27, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 840008063, 'comment_body': 'Also, I think it would be good to use the the metric version here if possible https://www.tensorflow.org/api_docs/python/tf/keras/metrics/SparseCategoricalCrossentropy, and remove `_loss` from your variable names. We are dealing with a metric here, not a loss.', 'comment_created': datetime.datetime(2022, 3, 31, 20, 52, 39, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840008246, 'comment_body': 'Sg!', 'comment_created': datetime.datetime(2022, 3, 31, 20, 52, 58, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 840009571, 'comment_body': 'oh this comment goes into the awkward position, I mean:\r\n\r\n```\r\nif dtype is None:\r\n  self._dtype = tf.float32\r\n```', 'comment_created': datetime.datetime(2022, 3, 31, 20, 54, 55, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 840010212, 'comment_body': 'I think the default is handled on the base class already. Can we remove this check entirely? And keep the error if `not self.dtype.is_floating`?', 'comment_created': datetime.datetime(2022, 3, 31, 20, 55, 50, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 840010956, 'comment_body': 'Yes, otherwise the test case is a bit opaque because we do not know what the input is like.', 'comment_created': datetime.datetime(2022, 3, 31, 20, 57, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 840992429, 'comment_body': 'Yeah, sure. I think you are talking about this: https://github.com/keras-team/keras/blob/master/keras/metrics/base_metric.py#L122-L123.', 'comment_created': datetime.datetime(2022, 4, 2, 1, 58, 33, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 840995783, 'comment_body': '@mattdangerw, I gave this a look and I don\'t think we can use `tf.keras.metrics.SparseCategoricalCrossentropy()`, since it doesn\'t have the `reduction` arg; it just takes the mean of all the values. We don\'t want the mean because we want to handle masked tokens later.\r\n\r\nA sample of what I mean (pun on mean unintended xD):\r\n\r\n```\r\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\r\nmetric = tf.keras.metrics.SparseCategoricalCrossentropy(from_logits=True)\r\n\r\n\r\ntarget = tf.random.uniform(shape=[2, 5],  maxval=10, dtype=tf.int32, seed=42)\r\nlogits = tf.random.uniform(shape=(2, 5, 10), seed=42)\r\n\r\n\r\nprint(""Print Element-wise Loss: "", loss(target, logits))\r\nprint(""Metric Value: "", metric(target, logits))\r\n```\r\n\r\nOutput:\r\n```\r\nPrint Element-wise Loss:  tf.Tensor(\r\n[[2.6863036 2.5335345 2.2001379 2.7056365 1.8480766]\r\n [2.319472  2.1234791 2.0424395 2.079166  2.5738573]], shape=(2, 5), dtype=float32)\r\nMetric Value:  tf.Tensor(2.3112102, shape=(), dtype=float32)\r\n```', 'comment_created': datetime.datetime(2022, 4, 2, 2, 31, 43, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 840997658, 'comment_body': 'I have used this condition:\r\n\r\n```\r\n if not tf.as_dtype(self.dtype.is_floating)\r\n```\r\nsince the parent class sets it to a string (`""float32""`) if `dtype == None`.', 'comment_created': datetime.datetime(2022, 4, 2, 2, 50, 25, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 842051623, 'comment_body': 'I checked out the source code of tf.keras.metrics.SparseCategoricalCrossentropy, and it is dong WEIGHTED_MEAN reduction (https://github.com/keras-team/keras/blob/d8fcb9d4d4dad45080ecfdd575483653028f8eda/keras/metrics.py#L583), which should automatically set the divisor as the sum over masks, could you help verify it with your unit test? Thanks!\r\n', 'comment_created': datetime.datetime(2022, 4, 4, 19, 0, 51, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 844292284, 'comment_body': 'Sure. Will try this out! Thanks!', 'comment_created': datetime.datetime(2022, 4, 6, 18, 58, 57, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 844297231, 'comment_body': '@chenmoneygithub, this particular UT failed:\r\n\r\n```\r\n____________________________________________________ PerplexityTest.test_two_inputs_from_logits ____________________________________________________ \r\n\r\nself = <keras_nlp.metrics.perplexity_test.PerplexityTest testMethod=test_two_inputs_from_logits>\r\n\r\n    def test_two_inputs_from_logits(self):\r\n        perplexity = Perplexity(from_logits=True, pad_token_id=0)\r\n\r\n        y_true_1 = tf.constant([[1, 3, 0], [2, 1, 3]])\r\n        y_pred_1 = tf.constant(\r\n            [\r\n                [\r\n                    [1.034, 4.797, 2.82, 1.154],\r\n                    [2.258, 1.591, 1.811, 1.852],\r\n                    [3.216, 1.037, 0.3662, 2.7],\r\n                ],\r\n                [\r\n                    [1.363, 1.726, 1.898, 2.582],\r\n                    [1.163, 1.943, 1.761, 1.497],\r\n                    [2.766, 1.453, 2.61, 2.805],\r\n                ],\r\n            ]\r\n        )\r\n\r\n        perplexity_val = perplexity(y_true_1, y_pred_1)\r\n        self.assertAlmostEqual(perplexity_val, 2.8788896)\r\n\r\n        y_true_2 = tf.constant([[2, 0, 0], [1, 2, 3]])\r\n        y_pred_2 = tf.constant(\r\n            [\r\n                [\r\n                    [2.887, 0.885, 2.973, 2.582],\r\n                    [0.3838, 2.629, 1.91, 1.802],\r\n                    [0.2578, 1.081, 1.125, 2.773],\r\n                ],\r\n                [\r\n                    [1.623, 2.784, 0.2109, 2.66],\r\n                    [2.395, 2.01, 0.252, 1.828],\r\n                    [0.4482, 2.629, 0.9697, 0.998],\r\n                ],\r\n            ]\r\n        )\r\n        perplexity_val = perplexity(y_true_2, y_pred_2)\r\n>       self.assertEqual(perplexity_val, 3.9998498)\r\nE       AssertionError: <tf.Tensor: shape=(), dtype=float32, numpy=3.3319612> != 3.9998498\r\n\r\nkeras_nlp\\metrics\\perplexity_test.py:132: AssertionError\r\n```\r\n', 'comment_created': datetime.datetime(2022, 4, 6, 19, 3, 30, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 844305036, 'comment_body': ""I did a quick analysis on Colab. Apparently, when `sample_weight` is provided, the outputs of `keras.losses.SparseCategoricalCrossentropy` and `keras.metrics.SparseCategoricalCrossentropy` don't match. Have a look at this: https://colab.research.google.com/drive/1Jh44hylKVmmdqR1Z3B_redGzr1ZJdI3i?usp=sharing.\r\nIs it some precision thingy which is messing up the values?\r\n\r\nLet me know what the correct course of action is. Thanks!"", 'comment_created': datetime.datetime(2022, 4, 6, 19, 12, 57, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 845398675, 'comment_body': 'This is pretty odd, I guess we can stick to Loss function and open an issue for future investigation. ', 'comment_created': datetime.datetime(2022, 4, 7, 17, 28, 14, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 845403606, 'comment_body': 'Coolio :). Thanks! ðŸ‘ðŸ¼ ', 'comment_created': datetime.datetime(2022, 4, 7, 17, 34, 26, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 845538999, 'comment_body': 'Please avoid directly checking the private members, if we think it is something necessary to check, then we should consider exposing a public interface for it. \r\n\r\nfor this case, can we just check the perplexity value?', 'comment_created': datetime.datetime(2022, 4, 7, 20, 32, 21, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 845690441, 'comment_body': ""Ah, yes. I just wanted to show that cross_entropy_for_state_1 + cross_entropy_for_state_2 = cross_entropy_for_state_3. But yeah, checking for perplexity will suffice. Changes made! I've kept the check for number of samples though"", 'comment_created': datetime.datetime(2022, 4, 8, 2, 28, 2, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 846301384, 'comment_body': 'Thanks for fixing! Please also apply similar changes on other test cases, basically we never want to explicitly check private fields, because private fields are subject to change without notice. \r\n', 'comment_created': datetime.datetime(2022, 4, 8, 16, 31, 58, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 846323580, 'comment_body': '@chenmoneygithub, done! Thanks!', 'comment_created': datetime.datetime(2022, 4, 8, 17, 3, 10, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 846536493, 'comment_body': ""`name` argument should come last. It's a base class argument."", 'comment_created': datetime.datetime(2022, 4, 8, 23, 22, 12, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 846536590, 'comment_body': 'Same for dtype (which comes before name).', 'comment_created': datetime.datetime(2022, 4, 8, 23, 22, 30, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 846536777, 'comment_body': 'Nit: Spell ""crossentropy"" in a single word, for consistency. This applies to the weight name and also to Python variable names.', 'comment_created': datetime.datetime(2022, 4, 8, 23, 23, 20, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 846536971, 'comment_body': 'Please make the metric serializable by adding a `get_config()` method.', 'comment_created': datetime.datetime(2022, 4, 8, 23, 24, 7, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 846539703, 'comment_body': 'Also prefer ""mask_token_id"" over ""pad_token_id""', 'comment_created': datetime.datetime(2022, 4, 8, 23, 36, 11, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 846539730, 'comment_body': 'by -> for ?', 'comment_created': datetime.datetime(2022, 4, 8, 23, 36, 18, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 846539832, 'comment_body': 'This behavior is problematic; we should *combine* the masks, not drop one of them.', 'comment_created': datetime.datetime(2022, 4, 8, 23, 36, 45, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 846554615, 'comment_body': 'What do you think is the best way to combine the masks? Element-wise maximum or element-wise addition (if both are not None)? Or do you have something else in mind?', 'comment_created': datetime.datetime(2022, 4, 9, 1, 3, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}, {'comment_id': 847673902, 'comment_body': 'I think we can just multiply the masks together. If padding token is set, that will give a mask of 1s and 0s, which could be multiplied with sample_weight.\r\n\r\nPut one way... If a padding token has sample weight `0.5` it should be ignore. If a non padding token has sample weight 0.5 we should still weight it by its sample weight before summing it in.', 'comment_created': datetime.datetime(2022, 4, 11, 19, 41, 13, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847939988, 'comment_body': 'Great! Done ðŸ‘ðŸ¼ ', 'comment_created': datetime.datetime(2022, 4, 12, 4, 6, 9, tzinfo=datetime.timezone.utc), 'commenter': 'abheesht17', 'type': 'User'}]","[{'commit_sha': '937e68fc6c3339ee524690b52ae297d3b77630a9', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'de41f62bc798a33944eb83a336c494fa17c28b95', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '52257dd2461f84290fe51f7e203d499594d4951d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6c9438b726c3536648d664013d114c1d91643374', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '53f2e9ce0974e220576e924bf3749ef6b44af790', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4d68c5b0657ab790ba69da14c7fc27b8fc72598c', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ff8e341ebeb39fd24a265f684f03d6770cb4fc35', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a5fb375b488d84a3eade87e0f2395af8b22446d0', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fb4f6d3ed351cb80eee76a983f9fca7155fcd464', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd50842f32887cb4af540d195196f6c152d3f6850', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'be547becf7ac43726081edc4d7b383ec5825a62b', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ff1cfaa509d0633af1f27204a123fc18dfbd9a26', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5b19a7322a5be23dd0d1dc3117079abd6fbf65cc', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ae976d12f2dffba3820a064ad86531f62259000d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd2600901e22013c24254333c61f036019a16784d', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ad9a2a550b60594bd7eb314773d436498317b28a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '362db989726c6daea371f9aff966a084ec44e501', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'af39af8ed68c2290dc44493c3c4d7bdf2e824fc6', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '829b2368843d4c6ff7f1a1bf126d2e82474a09d0', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '18372f825bb741e9ec618bb66ab2fddf2622b179', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9cde5bf5f220249fd170e470d9f795d83cff4673', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '72421aca6bc5aef1dcec26029a39a16bbe1c2cfc', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7213bbbcbcd80e8331bf36d30dbf36fc1d859a77', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3c9e4620438fc9df3a9e89614677cec7172ed54f', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ee3793756e7bf519dd7a73cee8a6d81d9a4ecf3a', 'committer_username': 'abheesht17', 'committer_name': 'Abheesht', 'committer_email': 'sharmabhee@gmail.com', 'commit_date': datetime.datetime(2017, 8, 26, 0, 55, 7, tzinfo=datetime.timezone.utc)}]",Abheesht,31360468,sharmabhee@gmail.com,User,,31,,126,96

Project_ID,Name,Full_name,Language,Forks,Stars,Watchers,contributors,commits,issues,branches,PRs_count,contributor pullrequests
267715375,keras-nlp,keras-team/keras-nlp,Python,223,751,30,80,1026,173,24,19,"[{'id': 1586027271, 'number': 1293, 'closed': None, 'created': datetime.datetime(2023, 11, 3, 12, 50, 8, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 723, 'deletions': 1, 'state': 'open'}, {'id': 1577328720, 'number': 1289, 'closed': datetime.datetime(2023, 10, 28, 0, 56, 24, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 10, 28, 0, 41, 2, tzinfo=datetime.timezone.utc), 'time_taken': 922.0, 'time_delta': '0:15:22', 'additions': 23, 'deletions': 3, 'state': 'closed'}, {'id': 1577058445, 'number': 1287, 'closed': datetime.datetime(2023, 11, 3, 3, 18, 19, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 10, 27, 18, 46, 39, tzinfo=datetime.timezone.utc), 'time_taken': 549100.0, 'time_delta': '6 days, 8:31:40', 'additions': 347, 'deletions': 24, 'state': 'closed'}, {'id': 1441225859, 'number': 1164, 'closed': datetime.datetime(2023, 7, 27, 1, 52, 20, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 19, 15, 7, 47, tzinfo=datetime.timezone.utc), 'time_taken': 643473.0, 'time_delta': '7 days, 10:44:33', 'additions': 9, 'deletions': 21, 'state': 'closed'}, {'id': 1434890173, 'number': 1155, 'closed': datetime.datetime(2023, 7, 17, 20, 36, 58, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 14, 14, 37, 33, tzinfo=datetime.timezone.utc), 'time_taken': 280765.0, 'time_delta': '3 days, 5:59:25', 'additions': 52, 'deletions': 63, 'state': 'closed'}, {'id': 1432410561, 'number': 1150, 'closed': datetime.datetime(2023, 7, 13, 17, 50, 8, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 13, 5, 49, 34, tzinfo=datetime.timezone.utc), 'time_taken': 43234.0, 'time_delta': '12:00:34', 'additions': 6, 'deletions': 1, 'state': 'closed'}, {'id': 1426172670, 'number': 1121, 'closed': datetime.datetime(2023, 7, 14, 17, 8, 39, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 9, 4, 39, 5, tzinfo=datetime.timezone.utc), 'time_taken': 476974.0, 'time_delta': '5 days, 12:29:34', 'additions': 5, 'deletions': 2, 'state': 'closed'}, {'id': 1421921181, 'number': 1114, 'closed': datetime.datetime(2023, 10, 25, 11, 41, 29, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 5, 22, 47, 33, tzinfo=datetime.timezone.utc), 'time_taken': 9636836.0, 'time_delta': '111 days, 12:53:56', 'additions': 719, 'deletions': 0, 'state': 'closed'}, {'id': 1404344485, 'number': 1089, 'closed': datetime.datetime(2023, 6, 30, 0, 37, 23, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 6, 23, 1, 31, 58, tzinfo=datetime.timezone.utc), 'time_taken': 601525.0, 'time_delta': '6 days, 23:05:25', 'additions': 1320, 'deletions': 14, 'state': 'closed'}, {'id': 1376732358, 'number': 1060, 'closed': datetime.datetime(2023, 6, 8, 2, 36, 37, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 6, 3, 7, 41, 39, tzinfo=datetime.timezone.utc), 'time_taken': 413698.0, 'time_delta': '4 days, 18:54:58', 'additions': 37, 'deletions': 0, 'state': 'closed'}, {'id': 1375289653, 'number': 1058, 'closed': datetime.datetime(2023, 6, 2, 23, 57, 9, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 6, 2, 8, 4, 19, tzinfo=datetime.timezone.utc), 'time_taken': 57170.0, 'time_delta': '15:52:50', 'additions': 21, 'deletions': 3, 'state': 'closed'}, {'id': 1353281890, 'number': 1046, 'closed': datetime.datetime(2023, 5, 17, 18, 45, 24, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 5, 17, 0, 17, 21, tzinfo=datetime.timezone.utc), 'time_taken': 66483.0, 'time_delta': '18:28:03', 'additions': 123, 'deletions': 180, 'state': 'closed'}, {'id': 1353219854, 'number': 1045, 'closed': datetime.datetime(2023, 5, 17, 18, 45, 10, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 5, 16, 22, 41, 49, tzinfo=datetime.timezone.utc), 'time_taken': 72201.0, 'time_delta': '20:03:21', 'additions': 101, 'deletions': 57, 'state': 'closed'}, {'id': 1312730346, 'number': 984, 'closed': datetime.datetime(2023, 4, 14, 20, 25, 57, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 4, 13, 15, 37, 19, tzinfo=datetime.timezone.utc), 'time_taken': 103718.0, 'time_delta': '1 day, 4:48:38', 'additions': 40, 'deletions': 20, 'state': 'closed'}, {'id': 1307549160, 'number': 978, 'closed': None, 'created': datetime.datetime(2023, 4, 10, 13, 49, 25, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 161, 'deletions': 7, 'state': 'open'}, {'id': 1306413493, 'number': 974, 'closed': datetime.datetime(2023, 5, 17, 5, 46, 35, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 4, 8, 6, 0, 24, tzinfo=datetime.timezone.utc), 'time_taken': 3368771.0, 'time_delta': '38 days, 23:46:11', 'additions': 1280, 'deletions': 126, 'state': 'closed'}, {'id': 1305132519, 'number': 969, 'closed': datetime.datetime(2023, 4, 7, 19, 32, 52, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 4, 6, 18, 33, 43, tzinfo=datetime.timezone.utc), 'time_taken': 89949.0, 'time_delta': '1 day, 0:59:09', 'additions': 111, 'deletions': 46, 'state': 'closed'}, {'id': 1304776038, 'number': 968, 'closed': datetime.datetime(2023, 4, 12, 18, 51, 2, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 4, 6, 13, 40, 1, tzinfo=datetime.timezone.utc), 'time_taken': 537061.0, 'time_delta': '6 days, 5:11:01', 'additions': 13, 'deletions': 1, 'state': 'closed'}, {'id': 1304767654, 'number': 967, 'closed': datetime.datetime(2023, 4, 6, 13, 37, 39, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 4, 6, 13, 34, 5, tzinfo=datetime.timezone.utc), 'time_taken': 214.0, 'time_delta': '0:03:34', 'additions': 6, 'deletions': 0, 'state': 'closed'}, {'id': 1299118993, 'number': 953, 'closed': datetime.datetime(2023, 8, 12, 15, 56, 53, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 4, 2, 14, 34, tzinfo=datetime.timezone.utc), 'time_taken': 11409773.0, 'time_delta': '132 days, 1:22:53', 'additions': 11, 'deletions': 5, 'state': 'closed'}, {'id': 1279821295, 'number': 861, 'closed': datetime.datetime(2023, 3, 17, 16, 50, 29, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 3, 17, 7, 59, tzinfo=datetime.timezone.utc), 'time_taken': 31889.0, 'time_delta': '8:51:29', 'additions': 2, 'deletions': 2, 'state': 'closed'}, {'id': 1278003862, 'number': 856, 'closed': datetime.datetime(2023, 4, 4, 20, 18, 12, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 3, 16, 6, 46, 14, tzinfo=datetime.timezone.utc), 'time_taken': 1690318.0, 'time_delta': '19 days, 13:31:58', 'additions': 842, 'deletions': 0, 'state': 'closed'}, {'id': 1274463372, 'number': 847, 'closed': datetime.datetime(2023, 5, 16, 19, 27, 18, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 3, 14, 6, 32, 54, tzinfo=datetime.timezone.utc), 'time_taken': 5489664.0, 'time_delta': '63 days, 12:54:24', 'additions': 643, 'deletions': 12, 'state': 'closed'}, {'id': 1271934186, 'number': 832, 'closed': datetime.datetime(2023, 3, 11, 3, 19, 55, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 3, 11, 3, 18, 59, tzinfo=datetime.timezone.utc), 'time_taken': 56.0, 'time_delta': '0:00:56', 'additions': 13, 'deletions': 4, 'state': 'closed'}, {'id': 1271647647, 'number': 825, 'closed': datetime.datetime(2023, 3, 10, 19, 46, 4, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 3, 10, 19, 42, 20, tzinfo=datetime.timezone.utc), 'time_taken': 224.0, 'time_delta': '0:03:44', 'additions': 16, 'deletions': 9, 'state': 'closed'}, {'id': 1270677816, 'number': 820, 'closed': datetime.datetime(2023, 3, 28, 18, 34, 19, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 3, 10, 6, 10, 28, tzinfo=datetime.timezone.utc), 'time_taken': 1599831.0, 'time_delta': '18 days, 12:23:51', 'additions': 240, 'deletions': 0, 'state': 'closed'}, {'id': 1260936837, 'number': 801, 'closed': datetime.datetime(2023, 3, 10, 18, 0, 2, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 3, 2, 19, 8, 13, tzinfo=datetime.timezone.utc), 'time_taken': 687109.0, 'time_delta': '7 days, 22:51:49', 'additions': 557, 'deletions': 1, 'state': 'closed'}, {'id': 1245905957, 'number': 759, 'closed': datetime.datetime(2023, 2, 24, 21, 41, 46, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 2, 18, 3, 9, 25, tzinfo=datetime.timezone.utc), 'time_taken': 585141.0, 'time_delta': '6 days, 18:32:21', 'additions': 61, 'deletions': 4, 'state': 'closed'}, {'id': 1236845423, 'number': 739, 'closed': datetime.datetime(2023, 2, 15, 18, 40, 56, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 2, 10, 15, 10, 51, tzinfo=datetime.timezone.utc), 'time_taken': 444605.0, 'time_delta': '5 days, 3:30:05', 'additions': 531, 'deletions': 0, 'state': 'closed'}, {'id': 1236699125, 'number': 737, 'closed': datetime.datetime(2023, 2, 15, 18, 40, 1, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 2, 10, 13, 31, tzinfo=datetime.timezone.utc), 'time_taken': 450541.0, 'time_delta': '5 days, 5:09:01', 'additions': 216, 'deletions': 0, 'state': 'closed'}, {'id': 1236684444, 'number': 736, 'closed': datetime.datetime(2023, 2, 15, 18, 38, 43, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 2, 10, 13, 19, 5, tzinfo=datetime.timezone.utc), 'time_taken': 451178.0, 'time_delta': '5 days, 5:19:38', 'additions': 319, 'deletions': 0, 'state': 'closed'}, {'id': 1226560774, 'number': 716, 'closed': datetime.datetime(2023, 2, 2, 20, 0, 39, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 2, 2, 13, 5, 57, tzinfo=datetime.timezone.utc), 'time_taken': 24882.0, 'time_delta': '6:54:42', 'additions': 4, 'deletions': 5, 'state': 'closed'}, {'id': 1211492151, 'number': 685, 'closed': datetime.datetime(2023, 2, 2, 21, 5, 49, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 20, 12, 46, 44, tzinfo=datetime.timezone.utc), 'time_taken': 1153145.0, 'time_delta': '13 days, 8:19:05', 'additions': 417, 'deletions': 0, 'state': 'closed'}, {'id': 1209984972, 'number': 683, 'closed': datetime.datetime(2023, 1, 19, 18, 45, 22, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 19, 12, 47, 43, tzinfo=datetime.timezone.utc), 'time_taken': 21459.0, 'time_delta': '5:57:39', 'additions': 2, 'deletions': 1, 'state': 'closed'}, {'id': 1201211327, 'number': 676, 'closed': datetime.datetime(2023, 1, 24, 19, 42, 12, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 18, 6, 19, 3, tzinfo=datetime.timezone.utc), 'time_taken': 566589.0, 'time_delta': '6 days, 13:23:09', 'additions': 44, 'deletions': 7, 'state': 'closed'}, {'id': 1200291082, 'number': 671, 'closed': datetime.datetime(2023, 1, 18, 18, 48, 26, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 17, 13, 39, 23, tzinfo=datetime.timezone.utc), 'time_taken': 104943.0, 'time_delta': '1 day, 5:09:03', 'additions': 588, 'deletions': 717, 'state': 'closed'}, {'id': 1198808319, 'number': 667, 'closed': datetime.datetime(2023, 1, 18, 18, 22, 18, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 16, 10, 46, 48, tzinfo=datetime.timezone.utc), 'time_taken': 200130.0, 'time_delta': '2 days, 7:35:30', 'additions': 1, 'deletions': 1, 'state': 'closed'}, {'id': 1198527350, 'number': 666, 'closed': datetime.datetime(2023, 1, 16, 15, 30, 2, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 16, 6, 49, 15, tzinfo=datetime.timezone.utc), 'time_taken': 31247.0, 'time_delta': '8:40:47', 'additions': 2, 'deletions': 2, 'state': 'closed'}, {'id': 1197525863, 'number': 661, 'closed': datetime.datetime(2023, 1, 18, 23, 55, 40, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 14, 11, 26, 55, tzinfo=datetime.timezone.utc), 'time_taken': 390525.0, 'time_delta': '4 days, 12:28:45', 'additions': 404, 'deletions': 0, 'state': 'closed'}, {'id': 1197500127, 'number': 659, 'closed': datetime.datetime(2023, 1, 18, 21, 7, 55, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 14, 8, 52, 6, tzinfo=datetime.timezone.utc), 'time_taken': 389749.0, 'time_delta': '4 days, 12:15:49', 'additions': 240, 'deletions': 2, 'state': 'closed'}, {'id': 1197258981, 'number': 655, 'closed': datetime.datetime(2023, 1, 18, 18, 38, 8, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 13, 20, 58, 55, tzinfo=datetime.timezone.utc), 'time_taken': 423553.0, 'time_delta': '4 days, 21:39:13', 'additions': 303, 'deletions': 2, 'state': 'closed'}, {'id': 1195980647, 'number': 651, 'closed': datetime.datetime(2023, 1, 12, 23, 32, 3, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 12, 22, 53, 34, tzinfo=datetime.timezone.utc), 'time_taken': 2309.0, 'time_delta': '0:38:29', 'additions': 1, 'deletions': 1, 'state': 'closed'}, {'id': 1192780143, 'number': 646, 'closed': datetime.datetime(2023, 1, 13, 21, 9, 14, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 10, 18, 50, 57, tzinfo=datetime.timezone.utc), 'time_taken': 267497.0, 'time_delta': '3 days, 2:18:17', 'additions': 534, 'deletions': 0, 'state': 'closed'}, {'id': 1189445085, 'number': 643, 'closed': datetime.datetime(2023, 1, 14, 5, 48, 40, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 7, 18, 29, 10, tzinfo=datetime.timezone.utc), 'time_taken': 559170.0, 'time_delta': '6 days, 11:19:30', 'additions': 322, 'deletions': 12, 'state': 'closed'}, {'id': 1188042705, 'number': 640, 'closed': datetime.datetime(2023, 1, 17, 19, 57, 58, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 6, 11, 46, 7, tzinfo=datetime.timezone.utc), 'time_taken': 979911.0, 'time_delta': '11 days, 8:11:51', 'additions': 303, 'deletions': 24, 'state': 'closed'}, {'id': 1186780027, 'number': 638, 'closed': datetime.datetime(2023, 1, 9, 21, 48, 11, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 5, 16, 48, 22, tzinfo=datetime.timezone.utc), 'time_taken': 363589.0, 'time_delta': '4 days, 4:59:49', 'additions': 270, 'deletions': 505, 'state': 'closed'}, {'id': 1184231267, 'number': 633, 'closed': datetime.datetime(2023, 1, 5, 0, 16, 13, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 1, 4, 3, 30, 48, tzinfo=datetime.timezone.utc), 'time_taken': 74725.0, 'time_delta': '20:45:25', 'additions': 326, 'deletions': 0, 'state': 'closed'}, {'id': 1180648850, 'number': 627, 'closed': datetime.datetime(2023, 1, 5, 0, 50, 13, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 12, 30, 2, 33, 17, tzinfo=datetime.timezone.utc), 'time_taken': 512216.0, 'time_delta': '5 days, 22:16:56', 'additions': 574, 'deletions': 0, 'state': 'closed'}, {'id': 1180451990, 'number': 622, 'closed': datetime.datetime(2023, 1, 10, 20, 55, 7, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 12, 29, 18, 35, 45, tzinfo=datetime.timezone.utc), 'time_taken': 1045162.0, 'time_delta': '12 days, 2:19:22', 'additions': 387, 'deletions': 0, 'state': 'closed'}, {'id': 1175000039, 'number': 604, 'closed': datetime.datetime(2022, 12, 22, 17, 53, 30, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 12, 22, 12, 15, 4, tzinfo=datetime.timezone.utc), 'time_taken': 20306.0, 'time_delta': '5:38:26', 'additions': 24, 'deletions': 16, 'state': 'closed'}, {'id': 1174074627, 'number': 595, 'closed': datetime.datetime(2022, 12, 21, 18, 41, 41, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 12, 21, 18, 18, 19, tzinfo=datetime.timezone.utc), 'time_taken': 1402.0, 'time_delta': '0:23:22', 'additions': 8, 'deletions': 4, 'state': 'closed'}, {'id': 1174055827, 'number': 594, 'closed': datetime.datetime(2022, 12, 22, 20, 21, 4, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 12, 21, 18, 0, 48, tzinfo=datetime.timezone.utc), 'time_taken': 94816.0, 'time_delta': '1 day, 2:20:16', 'additions': 901, 'deletions': 13, 'state': 'closed'}, {'id': 1168882542, 'number': 589, 'closed': datetime.datetime(2022, 12, 20, 21, 39, 53, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 12, 16, 19, 48, 29, tzinfo=datetime.timezone.utc), 'time_taken': 352284.0, 'time_delta': '4 days, 1:51:24', 'additions': 535, 'deletions': 0, 'state': 'closed'}, {'id': 1157865915, 'number': 565, 'closed': datetime.datetime(2022, 12, 11, 0, 54, 2, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 12, 10, 17, 9, 23, tzinfo=datetime.timezone.utc), 'time_taken': 27879.0, 'time_delta': '7:44:39', 'additions': 13, 'deletions': 13, 'state': 'closed'}, {'id': 1154785898, 'number': 559, 'closed': datetime.datetime(2022, 12, 9, 21, 48, 16, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 12, 8, 20, 52, 1, tzinfo=datetime.timezone.utc), 'time_taken': 89775.0, 'time_delta': '1 day, 0:56:15', 'additions': 5, 'deletions': 20, 'state': 'closed'}, {'id': 1142551155, 'number': 516, 'closed': datetime.datetime(2022, 12, 4, 0, 46, 43, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 12, 1, 23, 22, 57, tzinfo=datetime.timezone.utc), 'time_taken': 177826.0, 'time_delta': '2 days, 1:23:46', 'additions': 54, 'deletions': 38, 'state': 'closed'}, {'id': 1141892070, 'number': 506, 'closed': datetime.datetime(2022, 12, 3, 2, 53, 35, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 12, 1, 13, 52, 12, tzinfo=datetime.timezone.utc), 'time_taken': 133283.0, 'time_delta': '1 day, 13:01:23', 'additions': 888, 'deletions': 15, 'state': 'closed'}, {'id': 1127685625, 'number': 482, 'closed': datetime.datetime(2022, 12, 1, 20, 16, 49, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 11, 18, 14, 31, 8, tzinfo=datetime.timezone.utc), 'time_taken': 1143941.0, 'time_delta': '13 days, 5:45:41', 'additions': 869, 'deletions': 14, 'state': 'closed'}, {'id': 1124883535, 'number': 479, 'closed': datetime.datetime(2022, 12, 1, 23, 15, 35, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 11, 16, 18, 6, 40, tzinfo=datetime.timezone.utc), 'time_taken': 1314535.0, 'time_delta': '15 days, 5:08:55', 'additions': 869, 'deletions': 17, 'state': 'closed'}, {'id': 1120262002, 'number': 472, 'closed': datetime.datetime(2022, 11, 30, 18, 12, 32, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 11, 13, 8, 21, 20, tzinfo=datetime.timezone.utc), 'time_taken': 1504272.0, 'time_delta': '17 days, 9:51:12', 'additions': 632, 'deletions': 8, 'state': 'closed'}, {'id': 1120187680, 'number': 471, 'closed': datetime.datetime(2022, 11, 15, 17, 59, 12, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 11, 13, 3, 49, 6, tzinfo=datetime.timezone.utc), 'time_taken': 223806.0, 'time_delta': '2 days, 14:10:06', 'additions': 278, 'deletions': 146, 'state': 'closed'}, {'id': 1120052513, 'number': 470, 'closed': datetime.datetime(2022, 11, 15, 18, 2, 24, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 11, 12, 18, 32, 47, tzinfo=datetime.timezone.utc), 'time_taken': 257377.0, 'time_delta': '2 days, 23:29:37', 'additions': 208, 'deletions': 0, 'state': 'closed'}, {'id': 1120043729, 'number': 469, 'closed': datetime.datetime(2022, 11, 15, 17, 53, 55, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 11, 12, 18, 3, 26, tzinfo=datetime.timezone.utc), 'time_taken': 258629.0, 'time_delta': '2 days, 23:50:29', 'additions': 274, 'deletions': 66, 'state': 'closed'}, {'id': 1120029021, 'number': 468, 'closed': datetime.datetime(2022, 11, 15, 17, 53, 34, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 11, 12, 17, 10, 30, tzinfo=datetime.timezone.utc), 'time_taken': 261784.0, 'time_delta': '3 days, 0:43:04', 'additions': 207, 'deletions': 51, 'state': 'closed'}, {'id': 1104062795, 'number': 438, 'closed': datetime.datetime(2022, 10, 31, 19, 26, 30, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 10, 29, 17, 17, 10, tzinfo=datetime.timezone.utc), 'time_taken': 180560.0, 'time_delta': '2 days, 2:09:20', 'additions': 33, 'deletions': 18, 'state': 'closed'}, {'id': 1103918758, 'number': 437, 'closed': datetime.datetime(2022, 11, 4, 22, 52, 47, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 10, 29, 10, 18, 31, tzinfo=datetime.timezone.utc), 'time_taken': 563656.0, 'time_delta': '6 days, 12:34:16', 'additions': 200, 'deletions': 0, 'state': 'closed'}, {'id': 1103754394, 'number': 435, 'closed': datetime.datetime(2022, 12, 20, 19, 41, 33, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 10, 29, 1, 1, 40, tzinfo=datetime.timezone.utc), 'time_taken': 4559993.0, 'time_delta': '52 days, 18:39:53', 'additions': 1014, 'deletions': 0, 'state': 'closed'}, {'id': 1102013885, 'number': 428, 'closed': datetime.datetime(2022, 11, 7, 20, 40, 48, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 10, 27, 17, 13, 53, tzinfo=datetime.timezone.utc), 'time_taken': 962815.0, 'time_delta': '11 days, 3:26:55', 'additions': 206, 'deletions': 26, 'state': 'closed'}, {'id': 1101984254, 'number': 427, 'closed': datetime.datetime(2022, 10, 27, 17, 12, 44, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 10, 27, 16, 53, 36, tzinfo=datetime.timezone.utc), 'time_taken': 1148.0, 'time_delta': '0:19:08', 'additions': 383, 'deletions': 381, 'state': 'closed'}, {'id': 1098383711, 'number': 422, 'closed': datetime.datetime(2022, 10, 27, 19, 54, 55, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 10, 25, 5, 9, 9, tzinfo=datetime.timezone.utc), 'time_taken': 225946.0, 'time_delta': '2 days, 14:45:46', 'additions': 197, 'deletions': 0, 'state': 'closed'}, {'id': 1096336046, 'number': 419, 'closed': datetime.datetime(2022, 10, 27, 20, 26, 19, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 10, 22, 20, 39, 50, tzinfo=datetime.timezone.utc), 'time_taken': 431189.0, 'time_delta': '4 days, 23:46:29', 'additions': 332, 'deletions': 1, 'state': 'closed'}, {'id': 1096196901, 'number': 418, 'closed': datetime.datetime(2022, 10, 24, 22, 22, 46, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 10, 22, 14, 40, 51, tzinfo=datetime.timezone.utc), 'time_taken': 200515.0, 'time_delta': '2 days, 7:41:55', 'additions': 44, 'deletions': 151, 'state': 'closed'}, {'id': 1096143195, 'number': 417, 'closed': datetime.datetime(2022, 10, 24, 23, 34, 12, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 10, 22, 12, 42, 41, tzinfo=datetime.timezone.utc), 'time_taken': 211891.0, 'time_delta': '2 days, 10:51:31', 'additions': 183, 'deletions': 354, 'state': 'closed'}, {'id': 1072151598, 'number': 382, 'closed': datetime.datetime(2022, 10, 11, 0, 32, 6, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 30, 4, 24, 47, tzinfo=datetime.timezone.utc), 'time_taken': 936439.0, 'time_delta': '10 days, 20:07:19', 'additions': 570, 'deletions': 0, 'state': 'closed'}, {'id': 1062288994, 'number': 373, 'closed': datetime.datetime(2022, 9, 21, 16, 39, 53, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 21, 3, 16, 43, tzinfo=datetime.timezone.utc), 'time_taken': 48190.0, 'time_delta': '13:23:10', 'additions': 7, 'deletions': 6, 'state': 'closed'}, {'id': 1062271916, 'number': 372, 'closed': datetime.datetime(2022, 10, 7, 0, 21, 45, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 21, 2, 41, 53, tzinfo=datetime.timezone.utc), 'time_taken': 1373992.0, 'time_delta': '15 days, 21:39:52', 'additions': 670, 'deletions': 0, 'state': 'closed'}, {'id': 1059372606, 'number': 366, 'closed': datetime.datetime(2022, 11, 13, 10, 45, 14, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 17, 17, 49, 36, tzinfo=datetime.timezone.utc), 'time_taken': 4899338.0, 'time_delta': '56 days, 16:55:38', 'additions': 13930, 'deletions': 55, 'state': 'closed'}, {'id': 1057585496, 'number': 360, 'closed': datetime.datetime(2022, 9, 21, 16, 39, 8, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 15, 15, 46, 12, tzinfo=datetime.timezone.utc), 'time_taken': 521576.0, 'time_delta': '6 days, 0:52:56', 'additions': 31, 'deletions': 44, 'state': 'closed'}, {'id': 1056724179, 'number': 359, 'closed': datetime.datetime(2022, 9, 16, 19, 21, 13, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 15, 0, 58, 28, tzinfo=datetime.timezone.utc), 'time_taken': 152565.0, 'time_delta': '1 day, 18:22:45', 'additions': 50, 'deletions': 50, 'state': 'closed'}, {'id': 1054581775, 'number': 354, 'closed': datetime.datetime(2022, 9, 16, 19, 17, 55, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 13, 11, 45, 59, tzinfo=datetime.timezone.utc), 'time_taken': 286316.0, 'time_delta': '3 days, 7:31:56', 'additions': 383, 'deletions': 0, 'state': 'closed'}, {'id': 1049689870, 'number': 350, 'closed': datetime.datetime(2022, 9, 9, 18, 53, 15, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 8, 7, 50, 57, tzinfo=datetime.timezone.utc), 'time_taken': 126138.0, 'time_delta': '1 day, 11:02:18', 'additions': 85, 'deletions': 35, 'state': 'closed'}, {'id': 1047108238, 'number': 342, 'closed': datetime.datetime(2022, 9, 14, 23, 58, 16, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 6, 6, 16, 34, tzinfo=datetime.timezone.utc), 'time_taken': 754902.0, 'time_delta': '8 days, 17:41:42', 'additions': 259, 'deletions': 1, 'state': 'closed'}, {'id': 1046620705, 'number': 341, 'closed': datetime.datetime(2022, 9, 13, 19, 21, 26, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 5, 15, 46, 57, tzinfo=datetime.timezone.utc), 'time_taken': 704069.0, 'time_delta': '8 days, 3:34:29', 'additions': 171, 'deletions': 81, 'state': 'closed'}, {'id': 1045273951, 'number': 338, 'closed': datetime.datetime(2022, 9, 9, 23, 43, 22, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 2, 20, 52, 43, tzinfo=datetime.timezone.utc), 'time_taken': 615039.0, 'time_delta': '7 days, 2:50:39', 'additions': 2672, 'deletions': 2, 'state': 'closed'}, {'id': 1042377413, 'number': 331, 'closed': datetime.datetime(2022, 9, 8, 18, 6, 37, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 31, 13, 12, 8, tzinfo=datetime.timezone.utc), 'time_taken': 708869.0, 'time_delta': '8 days, 4:54:29', 'additions': 2554, 'deletions': 50, 'state': 'closed'}, {'id': 1042294723, 'number': 330, 'closed': datetime.datetime(2022, 8, 31, 21, 3, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 31, 11, 52, 11, tzinfo=datetime.timezone.utc), 'time_taken': 33049.0, 'time_delta': '9:10:49', 'additions': 86, 'deletions': 12, 'state': 'closed'}, {'id': 1038676331, 'number': 320, 'closed': datetime.datetime(2022, 8, 31, 0, 28, 54, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 27, 17, 31, 57, tzinfo=datetime.timezone.utc), 'time_taken': 284217.0, 'time_delta': '3 days, 6:56:57', 'additions': 994, 'deletions': 0, 'state': 'closed'}, {'id': 1038637660, 'number': 319, 'closed': datetime.datetime(2022, 9, 1, 19, 44, 12, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 27, 13, 44, 25, tzinfo=datetime.timezone.utc), 'time_taken': 453587.0, 'time_delta': '5 days, 5:59:47', 'additions': 2026, 'deletions': 16, 'state': 'closed'}, {'id': 1038572431, 'number': 318, 'closed': datetime.datetime(2022, 8, 29, 20, 24, 49, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 27, 6, 43, 35, tzinfo=datetime.timezone.utc), 'time_taken': 222074.0, 'time_delta': '2 days, 13:41:14', 'additions': 97, 'deletions': 7, 'state': 'closed'}, {'id': 1031715141, 'number': 300, 'closed': datetime.datetime(2022, 9, 1, 19, 35, 7, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 20, 6, 59, 21, tzinfo=datetime.timezone.utc), 'time_taken': 1082146.0, 'time_delta': '12 days, 12:35:46', 'additions': 250, 'deletions': 195, 'state': 'closed'}, {'id': 995799943, 'number': 271, 'closed': datetime.datetime(2022, 8, 16, 19, 15, 24, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 7, 13, 18, 10, 53, tzinfo=datetime.timezone.utc), 'time_taken': 2941471.0, 'time_delta': '34 days, 1:04:31', 'additions': 421, 'deletions': 72, 'state': 'closed'}, {'id': 971446220, 'number': 231, 'closed': datetime.datetime(2022, 7, 1, 19, 21, 29, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 6, 19, 9, 42, 22, tzinfo=datetime.timezone.utc), 'time_taken': 1071547.0, 'time_delta': '12 days, 9:39:07', 'additions': 529, 'deletions': 4, 'state': 'closed'}, {'id': 965195260, 'number': 222, 'closed': datetime.datetime(2022, 7, 11, 23, 35, 17, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 6, 13, 2, 46, 55, tzinfo=datetime.timezone.utc), 'time_taken': 2494102.0, 'time_delta': '28 days, 20:48:22', 'additions': 697, 'deletions': 7, 'state': 'closed'}, {'id': 964545773, 'number': 221, 'closed': datetime.datetime(2022, 6, 16, 1, 13, 40, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 6, 10, 20, 44, 29, tzinfo=datetime.timezone.utc), 'time_taken': 448151.0, 'time_delta': '5 days, 4:29:11', 'additions': 318, 'deletions': 4, 'state': 'closed'}, {'id': 948100471, 'number': 208, 'closed': datetime.datetime(2022, 5, 27, 19, 11, 46, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 5, 26, 10, 15, 3, tzinfo=datetime.timezone.utc), 'time_taken': 118603.0, 'time_delta': '1 day, 8:56:43', 'additions': 1, 'deletions': 1, 'state': 'closed'}, {'id': 944643864, 'number': 199, 'closed': datetime.datetime(2022, 5, 24, 17, 8, 4, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 5, 23, 15, 59, 10, tzinfo=datetime.timezone.utc), 'time_taken': 90534.0, 'time_delta': '1 day, 1:08:54', 'additions': 8, 'deletions': 4, 'state': 'closed'}, {'id': 911208416, 'number': 122, 'closed': datetime.datetime(2022, 6, 17, 17, 4, 14, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 4, 16, 5, 47, 29, tzinfo=datetime.timezone.utc), 'time_taken': 5397405.0, 'time_delta': '62 days, 11:16:45', 'additions': 1000, 'deletions': 0, 'state': 'closed'}, {'id': 896186721, 'number': 80, 'closed': datetime.datetime(2022, 4, 7, 17, 59, 32, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 3, 31, 17, 51, 24, tzinfo=datetime.timezone.utc), 'time_taken': 605288.0, 'time_delta': '7 days, 0:08:08', 'additions': 523, 'deletions': 0, 'state': 'closed'}, {'id': 890925552, 'number': 69, 'closed': datetime.datetime(2022, 4, 16, 5, 48, 8, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 3, 27, 7, 6, 24, tzinfo=datetime.timezone.utc), 'time_taken': 1723304.0, 'time_delta': '19 days, 22:41:44', 'additions': 241, 'deletions': 0, 'state': 'closed'}, {'id': 890396766, 'number': 68, 'closed': datetime.datetime(2022, 4, 13, 1, 16, 34, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 3, 26, 14, 54, 13, tzinfo=datetime.timezone.utc), 'time_taken': 1506141.0, 'time_delta': '17 days, 10:22:21', 'additions': 552, 'deletions': 0, 'state': 'closed'}, {'id': 886358931, 'number': 57, 'closed': datetime.datetime(2022, 3, 22, 19, 1, 30, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 3, 22, 18, 15, 6, tzinfo=datetime.timezone.utc), 'time_taken': 2784.0, 'time_delta': '0:46:24', 'additions': 3, 'deletions': 0, 'state': 'closed'}, {'id': 881550654, 'number': 44, 'closed': datetime.datetime(2022, 3, 16, 21, 55, 51, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 3, 16, 16, 14, 19, tzinfo=datetime.timezone.utc), 'time_taken': 20492.0, 'time_delta': '5:41:32', 'additions': 1, 'deletions': 1, 'state': 'closed'}, {'id': 880186431, 'number': 43, 'closed': datetime.datetime(2022, 3, 28, 17, 46, 55, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 3, 15, 13, 30, 55, tzinfo=datetime.timezone.utc), 'time_taken': 1138560.0, 'time_delta': '13 days, 4:16:00', 'additions': 310, 'deletions': 0, 'state': 'closed'}, {'id': 879339769, 'number': 42, 'closed': datetime.datetime(2022, 3, 14, 19, 52, 59, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 3, 14, 17, 36, 51, tzinfo=datetime.timezone.utc), 'time_taken': 8168.0, 'time_delta': '2:16:08', 'additions': 3, 'deletions': 3, 'state': 'closed'}]"
261861733,keras-io,keras-team/keras-io,Jupyter Notebook,2023,2723,60,335,1850,125,5,30,"[{'id': 1450162736, 'number': 1454, 'closed': datetime.datetime(2023, 9, 16, 1, 43, 52, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 26, 12, 19, 10, tzinfo=datetime.timezone.utc), 'time_taken': 4454682.0, 'time_delta': '51 days, 13:24:42', 'additions': 33, 'deletions': 18, 'state': 'closed'}, {'id': 1431102277, 'number': 1432, 'closed': datetime.datetime(2023, 8, 1, 17, 4, 29, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 12, 11, 28, 26, tzinfo=datetime.timezone.utc), 'time_taken': 1748163.0, 'time_delta': '20 days, 5:36:03', 'additions': 155, 'deletions': 62, 'state': 'closed'}, {'id': 1426041223, 'number': 1422, 'closed': datetime.datetime(2023, 8, 2, 19, 0, 11, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 7, 8, 19, 9, 30, tzinfo=datetime.timezone.utc), 'time_taken': 2159441.0, 'time_delta': '24 days, 23:50:41', 'additions': 1124, 'deletions': 0, 'state': 'closed'}, {'id': 1367606503, 'number': 1388, 'closed': datetime.datetime(2023, 6, 8, 21, 45, 1, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2023, 5, 28, 0, 43, 13, tzinfo=datetime.timezone.utc), 'time_taken': 1026108.0, 'time_delta': '11 days, 21:01:48', 'additions': 2371, 'deletions': 0, 'state': 'closed'}, {'id': 1025545937, 'number': 1025, 'closed': datetime.datetime(2023, 7, 9, 12, 20, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 13, 8, 14, 24, tzinfo=datetime.timezone.utc), 'time_taken': 28526736.0, 'time_delta': '330 days, 4:05:36', 'additions': 115, 'deletions': 144, 'state': 'closed'}, {'id': 1025525894, 'number': 1024, 'closed': datetime.datetime(2023, 7, 9, 12, 19, 44, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 13, 6, 27, 59, tzinfo=datetime.timezone.utc), 'time_taken': 28533105.0, 'time_delta': '330 days, 5:51:45', 'additions': 171, 'deletions': 142, 'state': 'closed'}, {'id': 984102271, 'number': 953, 'closed': datetime.datetime(2022, 7, 1, 17, 28, 10, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 6, 30, 17, 16, 47, tzinfo=datetime.timezone.utc), 'time_taken': 87083.0, 'time_delta': '1 day, 0:11:23', 'additions': 42, 'deletions': 42, 'state': 'closed'}, {'id': 954734089, 'number': 898, 'closed': datetime.datetime(2022, 6, 29, 22, 13, 47, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 6, 1, 19, 48, 1, tzinfo=datetime.timezone.utc), 'time_taken': 2427946.0, 'time_delta': '28 days, 2:25:46', 'additions': 1682, 'deletions': 0, 'state': 'closed'}, {'id': 949395216, 'number': 890, 'closed': datetime.datetime(2022, 7, 7, 20, 20, 46, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 5, 27, 14, 57, 51, tzinfo=datetime.timezone.utc), 'time_taken': 3561775.0, 'time_delta': '41 days, 5:22:55', 'additions': 2106, 'deletions': 0, 'state': 'closed'}]"
