pr_id,pr_title,pr_body,is_merged,pr_number,pr_url,pr_html_url,pr_state,additions,deletions,pr_changed_files,pr_commits_count,pr_comments_count,pr_review_comments_count,pr_labels_count,pr_assignees_count,pr_labels,pr_created_at,pr_closed_at,time_taken,time_delta,pr_review_comments,pr_commits,contributor,contributor_id,contributor_email,contributor_type,contributions,contributor_public_repos,contributor_private_repos,contributor_followings,contributor_followers
951677691,Random Deletion Layer,"Fixes #152 
Hey @mattdangerw @chenmoneygithub 
I've added the layer and some tests, I'll add more tests in the meanwhile as the code is reviewed!
You can find a demo [here](https://colab.research.google.com/gist/aflah02/6a94101652fe3be49acbc907359165ec/randomdeletiondemo.ipynb)
Once major obstacle is adding docstrings as they are also tested and that causes conflicts since the layer randomly deletes values the docstring output may or may not be what the layer outputs hence I could only give one example where probability is 1 and all the words are the same. Any workarounds for this?
Also I'm facing issues with datasets as `.numpy()` doesn't work in graph_mode. Simply trying to iterate and copy the tensor also fails in graph_mode, any workarounds for this?
I have a workaround which I can do while running it but I can't seem to figure out any changes to make to the code which would fix this - 
```
tf.data.experimental.enable_debug_mode()
augmenter = RandomDeletion(
    probability = 0.5,
    max_deletions = 3,
)
inputs = [""Never gonna give you up"", ""Never gonna let you down"", ""Never gonna run around and desert you""]
ds = tf.data.Dataset.from_tensor_slices(inputs)
ds = ds.batch(3).map(lambda y: tf.py_function(
                          (lambda x: augmenter(x)),
                          inp=[y], Tout=tf.string
                      ))
ds.take(1).get_single_element()
```
Output - 
```
<tf.Tensor: shape=(3,), dtype=string, numpy=array([b'Never you', b'let you', b'Never gonna run and'], dtype=object)>
```",True,214,https://api.github.com/repos/keras-team/keras-nlp/pulls/214,https://github.com/keras-team/keras-nlp/pull/214,closed,493,0,3,41,3,72,0,0,[],2022-05-31 08:40:09+00:00,2022-07-27 15:26:40+00:00,4949191.0,"57 days, 6:46:31","[{'comment_id': 887189003, 'comment_body': 'period at end of sentence.\r\n\r\nAlso we should probably have a little more a description in a separate paragraph, that describes the flow of computation. E.g. split words, delete words, reforms words.', 'comment_created': datetime.datetime(2022, 6, 1, 18, 43, 32, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887190121, 'comment_body': 'I think before we ship this we should decide if we want to leave room for a separate character deletion layer, or if we would want to do that as attributes on this layer.\r\n\r\nIf a character deletion layer would be separate, we should probably call this ""RandomCharacterDeletion"" or something.', 'comment_created': datetime.datetime(2022, 6, 1, 18, 44, 56, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887195378, 'comment_body': ""I think we can leave the return annotation off. Let's stick to optional annotations for simple types only."", 'comment_created': datetime.datetime(2022, 6, 1, 18, 51, 38, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887196204, 'comment_body': 'Could we use `tf.random.set_seed(X)` to make this deterministic? Then you might be able to show some more interesting cases of different words, not just ""dog""', 'comment_created': datetime.datetime(2022, 6, 1, 18, 52, 38, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887196710, 'comment_body': 'Remove return type annotation', 'comment_created': datetime.datetime(2022, 6, 1, 18, 53, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887203339, 'comment_body': ""I think we can clean this whole for loop up, though will need to think about how exactly a bit more.\r\n\r\nOne place for inspiration is probably the op code for tf text's [RandomItemSelector](https://github.com/tensorflow/text/blob/v2.9.0/tensorflow_text/python/ops/item_selector_ops.py#L225). Which is also selecting a number of items based on a probability with a max cap.\r\n\r\nI'm a little skeptical that this would function trace, you are doing a lot of looping and calling `.numpy()`, the later definitely won't work in a compiled context."", 'comment_created': datetime.datetime(2022, 6, 1, 19, 1, 27, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887205413, 'comment_body': 'We need to add more test coverage here. Particularly\r\n\r\n - Calling the layer on an unbatched dataset using `tf.data.Dataset.map`.\r\n - Calling the layer on an batched dataset using `tf.data.Dataset.map`.\r\n - Compiling the layer into a simple keras model that takes a string input and outputs a string with deletions.\r\n\r\nThis should help us uncover where we need to improve the call graph code in the main class.', 'comment_created': datetime.datetime(2022, 6, 1, 19, 4, 17, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 887400578, 'comment_body': 'Agree here, we should consider the scalability.\r\n\r\nMy question is to make a character-level deletion layer, how much change would be required? ', 'comment_created': datetime.datetime(2022, 6, 2, 0, 29, 3, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887401534, 'comment_body': 'Let\'s be consistent with the capital case after "":"", also we need to note the type in the arg comment.', 'comment_created': datetime.datetime(2022, 6, 2, 0, 31, 50, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887402027, 'comment_body': 'basically we should be consistent with the style. Either type annotation everywhere or just no annotation. ', 'comment_created': datetime.datetime(2022, 6, 2, 0, 33, 23, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887403659, 'comment_body': 'of => or', 'comment_created': datetime.datetime(2022, 6, 2, 0, 38, 2, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887403741, 'comment_body': 'period at end.', 'comment_created': datetime.datetime(2022, 6, 2, 0, 38, 17, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887405145, 'comment_body': 'the format looks a bit strange, is that formatted by black?', 'comment_created': datetime.datetime(2022, 6, 2, 0, 42, 8, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887406547, 'comment_body': ""Reading about RandomItemSelector, I think briefly what it does is to:\r\n1. Calculate how many to select, let's call it N.\r\n2. shuffle the list/array/tensor's index array.\r\n3. Pick the first N elements from index array, then use `tf.gather` to get the actual selected elements."", 'comment_created': datetime.datetime(2022, 6, 2, 0, 46, 32, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 887591146, 'comment_body': ""@chenmoneygithub I did discuss it with Matt [here](https://github.com/keras-team/keras-nlp/issues/152#issuecomment-1144037892) after his initial comment. We're now thinking of having them as 2 separate layers but would love to hear your thoughts on this!"", 'comment_created': datetime.datetime(2022, 6, 2, 6, 11, 15, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 887593155, 'comment_body': 'Thanks for sharing this @mattdangerw and for the concise summary @chenmoneygithub this seems like a much more smoother way to do it, will incorporate this idea in my code. Just to clarify do I need to cite this code file in the code as (inspired by) or something of that sort?', 'comment_created': datetime.datetime(2022, 6, 2, 6, 14, 45, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 888196903, 'comment_body': 'I am fine with both! I raised the question because I want to check if possible to have a BaseClass and only do small customization on WordDelete and CharacaterDelete.', 'comment_created': datetime.datetime(2022, 6, 2, 17, 14, 5, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 888207162, 'comment_body': ""@chenmoneygithub That does seem like a more efficient design choice but not really sure about that. I'll get back if I find a good way for that"", 'comment_created': datetime.datetime(2022, 6, 2, 17, 26, 29, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 888222778, 'comment_body': '@chenmoneygithub Yup this was generated by black', 'comment_created': datetime.datetime(2022, 6, 2, 17, 45, 52, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 894041633, 'comment_body': ""I'm not sure we actually need to support this. The problem is we are only supporting it for one level of nested rank, not multiple.\r\n\r\nThere are three crucial shapes to support as input: `()`, `(batch_size,)`, and `(batch_size, 1)`. We should be able to support all of these without the outer `map_fn` in your code above.\r\n\r\nThis could it would actually be reasonable to error out on, and ask for a support shape size."", 'comment_created': datetime.datetime(2022, 6, 9, 23, 43, 1, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 894055330, 'comment_body': 'I\'m a little worried about how we are doing whitespace splitting here. There\'s two downsides with this approach.\r\n\r\n - It\'s destructive. Newlines, tabs, consecutive spaces will all get replaces with a single space.\r\n - It will group punctuation along with words and delete them both. So for an input ""Are you OK?"" It would delete ""OK?"" as a single token I think.\r\n\r\nDo we want that behavior? Maybe in the context of EDA normal usage it is fine.\r\n\r\nThere might be an option to do something fancier here. Use `tf_text.regex_split` to split on whitespace and punctuation, but still keep the split characters. Use `tf.strings.regex_full_match` to exclude whitespace and punctation from deletion. Something like...\r\n\r\n```\r\nragged_words = tf_text.regex_split(inputs, WHITESPACE_AND_PUNCTUATION_REGEX, WHITESPACE_AND_PUNCTUATION_REGEX)\r\n\r\npositions_flat = tf.range(tf.size(ragged_words.flat_values))\r\npositions = ragged_words.with_flat_values(positions_flat)\r\nseperator_positions = tf.strings.regex_full_match(ragged_words, WHITESPACE_AND_PUNCTUATION_REGEX)\r\npositions = tf.ragged.boolean_mask(positions_flat, separator_positions == False)\r\n```', 'comment_created': datetime.datetime(2022, 6, 10, 0, 8, 27, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 894314321, 'comment_body': ""That Makes Sense, I'll throw an error for the same"", 'comment_created': datetime.datetime(2022, 6, 10, 9, 1, 3, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 894785530, 'comment_body': 'Thanks @mattdangerw working on this rn, just to confirm in the last line it should be \r\n`tf.ragged.boolean_mask(positions, separator_positions == False)` instead right?', 'comment_created': datetime.datetime(2022, 6, 10, 18, 10, 57, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 894790428, 'comment_body': '@mattdangerw This method also does this weird thing where it preserves the whitespaces for the deleted texts too, as you can see in this [demo](https://colab.research.google.com/gist/aflah02/e9672690129b5193851883115a21784d/demodeletionwithregex.ipynb) the whitespaces in front of ""like"" and ""to"" are also preserved. This does satisfy the criteria of preserving spaces albeit in a very weird way is this the intended result?', 'comment_created': datetime.datetime(2022, 6, 10, 18, 18, 28, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 901020343, 'comment_body': ""This sentence is too long, let's refactor it a bit. One possible way is to break down by steps: 1) Split the words ... 2) randomly select ... 3) mask out... 4) join "", 'comment_created': datetime.datetime(2022, 6, 18, 22, 2, 20, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 901020723, 'comment_body': 'Nit: argument type, and period at the end. \r\n\r\nAlso maybe call this `selection_rate` to be consistent with [mlm_mask_generator](https://github.com/keras-team/keras-nlp/blob/master/keras_nlp/layers/mlm_mask_generator.py#L34)', 'comment_created': datetime.datetime(2022, 6, 18, 22, 4, 6, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 901020825, 'comment_body': 'The output becomes byte string. I am wondering is it still a valid input to our tokenizers? ', 'comment_created': datetime.datetime(2022, 6, 18, 22, 5, 25, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 901020862, 'comment_body': 'Nit: Augment => augment.', 'comment_created': datetime.datetime(2022, 6, 18, 22, 6, 11, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 901020977, 'comment_body': ""All examples above have string inputs and string outputs, but here we allow integer inputs as well (we default to tf.int32). If integer inputs are expected, let's add one example to showcase, otherwise let's modify this type check part. "", 'comment_created': datetime.datetime(2022, 6, 18, 22, 8, 39, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 901021400, 'comment_body': ""Do we allow integer tensors? If not, let's error out. "", 'comment_created': datetime.datetime(2022, 6, 18, 22, 14, 14, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 901021484, 'comment_body': 'I am a little confused about this - if the input is tokenized input, such as ```[[""Input"", ""must"", ""be"", ""string""], [""I"", ""don\'t"", ""know"", ""what"", ""I"", ""am"", ""writing""]]```, then we will reject the input?', 'comment_created': datetime.datetime(2022, 6, 18, 22, 16, 9, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 901022484, 'comment_body': 'For batched input, is this `positions` still having the batch dim? It looks not to me because you are taking the `flat_values`. But I might be wrong here.\r\n\r\n', 'comment_created': datetime.datetime(2022, 6, 18, 22, 32, 31, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 901022755, 'comment_body': ""I am a little concerning about the current implementation - the current logic of picking up indices to delete looks complex to me, and you are using tf.map_fn to handle each element separately instead of doing a batch processing, which could get slow. \r\n\r\nRethinking about how we can do the index pick up, can we do the following? Let's assume we have already done the split, so the input is a 2D raggedTensor. \r\n1) calculate how many words to pick up for each sequence, let's call it num_row_i for ith row.\r\n2) For each row, pick up 1 - num_row_i indices.\r\n3) for the selected indices in 2), we do a tf.gather to gather them into a new tensor, which are the words we want to keep.\r\n4) reduce_join the output of 3) to reconstruct the string for each row.\r\n\r\nIn this way, we can keep the batch dimension along with the computation. But I am not 100% sure if this works, WDYT?\r\n\r\n"", 'comment_created': datetime.datetime(2022, 6, 18, 22, 37, 35, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 903144397, 'comment_body': '@chenmoneygithub I think the confusion you are having is whether we allow input that is pre-split or if splitting happens on the layer.\r\n\r\nIf we split inside the layer, the input you are describing is invalid. And I think it is totally valid to support only shapes `(bs, 1)`, `(bs)` and `()`.\r\n\r\nIf we split outside the layer, the input shapes we should support will change. Essentially we should support ragged with shape `(bs, None)` or dense with shape `(None)`.', 'comment_created': datetime.datetime(2022, 6, 21, 23, 20, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 903156538, 'comment_body': ""Let's chat about this tomorrow! Overall I think we need to make sure this can trace, but don't actually need to care about performance that much. If you are running an augmentation, it's because you have very little data. You don't actually care about speed.\r\n\r\nI think the reason this is so complex (here and in tf.text), is that there is no good way to uniformly sample from a pool of candidate indices with a max cap without using a `map` call. It's actually the `shuffle` call that we can't run without `map`."", 'comment_created': datetime.datetime(2022, 6, 21, 23, 49, 42, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 914335352, 'comment_body': 'no spaces around the equal signs, we should follow our usual style here.', 'comment_created': datetime.datetime(2022, 7, 6, 1, 33, 3, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 914335604, 'comment_body': ""we should be able to not set this right? leave None as a default, and if it's not set there's no hard limit to the number of deletions?"", 'comment_created': datetime.datetime(2022, 7, 6, 1, 33, 45, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 914335786, 'comment_body': 'I think you can show these examples without max_deletions, just to keep things simple', 'comment_created': datetime.datetime(2022, 7, 6, 1, 34, 13, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 914338584, 'comment_body': ""I don't know why we would accept a single string now that we are expecting inputs to be pre-split.\r\n\r\nWe should basically be document and handling our inputs the same as the packer layers now. See\r\nhttps://github.com/keras-team/keras-nlp/blob/master/keras_nlp/layers/start_end_packer.py#L29\r\nhttps://github.com/keras-team/keras-nlp/blob/master/keras_nlp/layers/start_end_packer.py#L105-L120"", 'comment_created': datetime.datetime(2022, 7, 6, 1, 41, 17, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 914338938, 'comment_body': ""Let's discuss, we may want to add some wort of skip_fn or skip_list argument before we ship this."", 'comment_created': datetime.datetime(2022, 7, 6, 1, 42, 9, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 914340337, 'comment_body': 'Still need to do this!', 'comment_created': datetime.datetime(2022, 7, 6, 1, 45, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 914341018, 'comment_body': 'just use this from the keras import, `keras.backend.RandomGenerator`', 'comment_created': datetime.datetime(2022, 7, 6, 1, 47, 48, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 914342272, 'comment_body': ""can't we do `self._random_generator.make_seed_for_stateless_op()`?"", 'comment_created': datetime.datetime(2022, 7, 6, 1, 49, 33, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 914342613, 'comment_body': 'after we make the minor randomness changes suggested below, can we do `tf.keras.utils.set_random_seed(30)`? That would make this a little more readable with a single seed line.', 'comment_created': datetime.datetime(2022, 7, 6, 1, 50, 26, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 914461233, 'comment_body': 'Done', 'comment_created': datetime.datetime(2022, 7, 6, 6, 31, 12, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 914468107, 'comment_body': ""@mattdangerw I don't think we can. It returns a None right now unless I set the rng_type but as per the latest tf release it is not available currently and was added after the latest release"", 'comment_created': datetime.datetime(2022, 7, 6, 6, 41, 44, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 914478289, 'comment_body': ""There is some weird behaviour here, if I do this I get `AttributeError: module 'keras.api._v2.keras.backend' has no attribute 'RandomGenerator'` not sure why \r\n"", 'comment_created': datetime.datetime(2022, 7, 6, 6, 56, 8, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 914478515, 'comment_body': 'Done!', 'comment_created': datetime.datetime(2022, 7, 6, 6, 56, 24, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 915032768, 'comment_body': 'Makes Sense Changed!', 'comment_created': datetime.datetime(2022, 7, 6, 16, 21, 7, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 915055207, 'comment_body': 'RandomGenerator is not exposed as a public API: https://github.com/keras-team/keras/blob/v2.9.0/keras/backend.py#L1823\r\n\r\nThere is actually an API in TF: tf.random.Generator (https://www.tensorflow.org/api_docs/python/tf/random/Generator), can we use that one?', 'comment_created': datetime.datetime(2022, 7, 6, 16, 45, 33, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 925805311, 'comment_body': 'I think both skip_fn and py_skip_fn should be in the config', 'comment_created': datetime.datetime(2022, 7, 20, 16, 12, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925807862, 'comment_body': 'we should make sure we are testing both skip_fn and py_skip_fn with dataset.map (this will test tracing)', 'comment_created': datetime.datetime(2022, 7, 20, 16, 14, 50, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925808552, 'comment_body': 'periods after example titles', 'comment_created': datetime.datetime(2022, 7, 20, 16, 15, 31, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925820085, 'comment_body': 'maybe switch this to a character level example with tf.strings.regex_full_match(x, r""\\pP""). I think it would be more compelling to show an actual tf op here.', 'comment_created': datetime.datetime(2022, 7, 20, 16, 27, 13, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925833447, 'comment_body': 'maybe `skip_py_fn`? Then they all have the same prefix', 'comment_created': datetime.datetime(2022, 7, 20, 16, 41, 38, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925833830, 'comment_body': 'Add an error, only one of skip_list, skip_fn, skip_py_fn should be set?', 'comment_created': datetime.datetime(2022, 7, 20, 16, 42, 2, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925834344, 'comment_body': 'This must be a traceable function of tf operations.', 'comment_created': datetime.datetime(2022, 7, 20, 16, 42, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925835555, 'comment_body': 'This is a Python function, and not a TensorFlow function. -> Unlike `skip_fn`, this can be any python function that operates on strings, and does not need to use tf operations.', 'comment_created': datetime.datetime(2022, 7, 20, 16, 43, 55, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925837100, 'comment_body': 'Maybe something simpler that makes it obvious you are not working with tensors? `return len(word) < 4` for example.', 'comment_created': datetime.datetime(2022, 7, 20, 16, 45, 41, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925840766, 'comment_body': 'periods after sentences', 'comment_created': datetime.datetime(2022, 7, 20, 16, 49, 48, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925938250, 'comment_body': '@mattdangerw This check already exists - https://github.com/aflah02/keras-nlp/blob/20df2b2ef260895031876fa6a07cbe11accdb707/keras_nlp/layers/random_deletion.py#L141', 'comment_created': datetime.datetime(2022, 7, 20, 18, 31, 33, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 927829349, 'comment_body': 'backticks around arg names', 'comment_created': datetime.datetime(2022, 7, 22, 16, 44, 39, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 927830069, 'comment_body': ""let's be explicit here about the full list of dtypes we support, maybe copy this error message\r\n\r\nhttps://github.com/keras-team/keras-nlp/blob/master/keras_nlp/tokenizers/sentence_piece_tokenizer.py#L105"", 'comment_created': datetime.datetime(2022, 7, 22, 16, 45, 44, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 927861353, 'comment_body': 'We need to be conditional on type here I think. We need to support the case where we are operating on tokenized, integer id inputs.\r\n\r\nPlease add a test case for this as well.', 'comment_created': datetime.datetime(2022, 7, 22, 17, 30, 37, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 927862251, 'comment_body': 'why do we need this line?', 'comment_created': datetime.datetime(2022, 7, 22, 17, 31, 51, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 929115678, 'comment_body': '@mattdangerw Not too sure but I kept it because I think the original Item Selector also has this and it was also there in the Colab you had shared', 'comment_created': datetime.datetime(2022, 7, 25, 17, 11, 14, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 929383444, 'comment_body': 'word_counts -> token_counts', 'comment_created': datetime.datetime(2022, 7, 25, 23, 13, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 929391810, 'comment_body': 'I think we can simplify this a bit. We should also rename word -> token. Something like...\r\n\r\n```\r\nstring_fn = lambda x: self.skip_py_fn(x.numpy().decode(""utf-8""))\r\nint_fn = lambda x: self.skip_py_fn(x.numpy())\r\npy_fn = string_fn if ragged.dtype == tf.string else int_fn\r\n\r\nreturn tf.map_fn(\r\n    lambda x: tf.py_function(py_fn, [x], tf.bool),\r\n    inputs.flat_values,\r\n    dtype=tf.bool,\r\n)\r\n```', 'comment_created': datetime.datetime(2022, 7, 25, 23, 34, 19, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 929392567, 'comment_body': ""we aren't testing anything about this, leave it out?"", 'comment_created': datetime.datetime(2022, 7, 25, 23, 36, 10, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 929397837, 'comment_body': 'This looks like we are just testing roughly the same things as the dataset test below. Can we just move the skip_list test into the dataset test below and delete this test?', 'comment_created': datetime.datetime(2022, 7, 25, 23, 49, 22, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 929503445, 'comment_body': '@mattdangerw The dataset tests already have skip options being tested so should I remove this altogether? ', 'comment_created': datetime.datetime(2022, 7, 26, 4, 1, 46, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 929504386, 'comment_body': '@mattdangerw This seems easier however it fails the style rules, the formatter gives this error - `E731 do not assign a lambda expression, use a def`. I guess this will persist till we keep assigning lambdas. Alternatively this works:\r\n\r\n```\r\n            def string_fn(token):\r\n                return self.skip_py_fn(token.numpy().decode(""utf-8""))\r\n            def int_fn(token):\r\n                return self.skip_py_fn(token.numpy())\r\n            py_fn = string_fn if inputs.dtype == tf.string else int_fn\r\n```', 'comment_created': datetime.datetime(2022, 7, 26, 4, 4, 25, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}]","[{'commit_sha': 'bf3219fe097029725b2fb33df39610de7ddf02a2', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '66c0de76bc6e2c41a29166ff13f1d78f9d7f63d7', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd2508dc368febc67961c8255a74525c58b1ef6ad', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9e2e243972bfcbdddcf3b8277c51ebc14dc2f916', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b48dc5a779809b9034b2b916a5ea6d929840e44b', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '02ce27dacd0b88201a7aa6cadfe42a78786eb376', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f137256f13ff053c8bec5462491e21ef5d66469b', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c7dcb8a3ed9f7f74f8c93e75fd998f3ebb8e20b4', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '515a1d38626fd777bde719811794d7e796f3a019', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6acfb6214a0043f2983505f3bdfda6e03c3b4b5c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '68fcae0d17e106ff57e02b4db34d3e363b05f038', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '058d572c71ee78fba6dc4ea0cc384c469d7aee5c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7a57339afaf68ce4ed20fbb843702c4f66219add', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9cf9a2a5355d63ca3164eb25a1f2982af002205f', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ce10e2d2ba9b951cb3108e622b8e6cfefe5dea22', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a1ab88efd72dbc09f695151e963eb18ed1e69dc2', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '17e2365a4fb40b63f86750110c0a2bf6929d3e5e', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b5cfe45a1316e04a8c22a92114f2b3e31c8342ad', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '839a770f5694dbe5fe5cefddaf0d406c9bc83dbf', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ec2d4ed6ea8a8bfed41361e5438190e46ab40a62', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dbdd690dd624e7a0ce4b5e343296c19b9b999877', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fd856ada43d911d3924abb39719694c06da2149f', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '292353b4d3a1c9fec0cbaf5d72b791f6eddd1f20', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9c447dc14d94887d4ed667c65e29be1a311ee560', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '35cf31a796b7ff47c86f1ede6bd494c7886d93f0', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b26dd24970c9d71115571268b8c3e8cd5d910ffa', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '983bfb36a8d350c6ec5acc2c36c8bda01bdfc1d1', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4813f1ff7ce12645b5a30bbc6ad30f42b8ec23d4', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '86bafbefff702a8201f3802c91f75b9073321d7b', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '46eda298ff908ed7c9a444ca9bb56bbe5139a241', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '906614a9f03b6a9b7c5e1acee860bb5aa80e0d0c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '20df2b2ef260895031876fa6a07cbe11accdb707', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0c8fdc5984b2876cf2c6462f84c9d4e7846b1bfc', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3d87d14303aa3e86ea61d2103dd5e5b9c4048874', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5ee888a4bee07aa4e99d1e5e5c4f52320e2cd127', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a77880eba45107da6f2a9b1cdd83aaf4dacc6f37', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9c1904cced4e31078b6bf30568c53c6fd657341b', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b87394d8d2cb3782536a2618e5cd9502f5f177d8', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '81ba7bfe1859a80da6a1e6aefae7b39156426c70', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0da28dc30e25f005d7774ebe111a2cd6fcdad4d6', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd91a1bbfb0b409560ac4c533d5d0644b26519ab3', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
966757546,Random Swap Layer,"Fixes #223 
@mattdangerw @chenmoneygithub I believe the PR is ready for review!",True,224,https://api.github.com/repos/keras-team/keras-nlp/pulls/224,https://github.com/keras-team/keras-nlp/pull/224,closed,482,0,3,27,0,31,0,0,[],2022-06-14 10:41:21+00:00,2022-09-01 22:11:37+00:00,6867016.0,"79 days, 11:30:16","[{'comment_id': 925839776, 'comment_body': 'I think this should be RandomSwap not RandomSwaps (that will align with RandomDeletion).', 'comment_created': datetime.datetime(2022, 7, 20, 16, 48, 43, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925846698, 'comment_body': 'Make sure to do the same amount of testing, e.g. dataset.map for this as with random deletion', 'comment_created': datetime.datetime(2022, 7, 20, 16, 56, 33, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925847440, 'comment_body': 'I think we should copy the whole list of args from RandomDeletion...\r\n\r\n        rate,\r\n        max_swaps=None,\r\n        skip_list=None,\r\n        skip_fn=None,\r\n        py_skip_fn=None,\r\n        seed=None,\r\n\r\nYou can still ""do this n times"" with rate=1, max_deletions=n. But that would also allow the number of swaps to be variable according to the length of the input.', 'comment_created': datetime.datetime(2022, 7, 20, 16, 57, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925848264, 'comment_body': 'camel case', 'comment_created': datetime.datetime(2022, 7, 20, 16, 58, 10, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925849946, 'comment_body': 'No need to document call here.', 'comment_created': datetime.datetime(2022, 7, 20, 17, 0, 3, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 925852141, 'comment_body': 'I think the flow of this function can look very similar to the random deletion layer up to the shuffle and trim part?', 'comment_created': datetime.datetime(2022, 7, 20, 17, 2, 34, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 941386238, 'comment_body': 'This still applies I think!', 'comment_created': datetime.datetime(2022, 8, 9, 14, 5, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 941387133, 'comment_body': 'I think this should have the same rate, max API as the deletion layer. ', 'comment_created': datetime.datetime(2022, 8, 9, 14, 6, 22, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 941388382, 'comment_body': 'Why do you have to run this in a loop? Should assertAllEqual work on batched tensors?', 'comment_created': datetime.datetime(2022, 8, 9, 14, 7, 26, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 948174383, 'comment_body': 'Why are we adding tf.function by default? This is unusual for a layer.\r\n\r\nGenerally tf functions if created when tracing an entire model or in a tf data map.', 'comment_created': datetime.datetime(2022, 8, 17, 16, 27, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 948188620, 'comment_body': ""I'm unsure we should do the skipping this way. Probably better to first filter down to a list of candidate positions (using skip_fn), then use rate/max to determine the number of swaps, then apply the swaps to the candidate positions.\r\n\r\nSimilar to how we do this for deletions."", 'comment_created': datetime.datetime(2022, 8, 17, 16, 41, 59, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 951763126, 'comment_body': 'Maybe ""The probability of a given token being chosen to be swapped with another random token.""', 'comment_created': datetime.datetime(2022, 8, 22, 18, 20, 38, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 951763626, 'comment_body': 'nit: Level -> level', 'comment_created': datetime.datetime(2022, 8, 22, 18, 21, 3, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 956667750, 'comment_body': 'Done!', 'comment_created': datetime.datetime(2022, 8, 28, 6, 4, 6, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 958823383, 'comment_body': 'augmentation -> augmentations', 'comment_created': datetime.datetime(2022, 8, 30, 18, 52, 28, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958825422, 'comment_body': 'maybe instead of ""pretokenized"" say...\r\n\r\n...pre-split into token level inputs. This allows control over the level of augmentation,\r\nyou can split by character for character level swaps, or by word for word level swaps.', 'comment_created': datetime.datetime(2022, 8, 30, 18, 55, 5, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958826465, 'comment_body': ""we don't have this check for the deletion layer, should we add it there?"", 'comment_created': datetime.datetime(2022, 8, 30, 18, 56, 21, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958827064, 'comment_body': 'f""`max_swaps` must be non-negative. Received `max_swaps={max_swaps}`.""', 'comment_created': datetime.datetime(2022, 8, 30, 18, 57, 2, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958827482, 'comment_body': 'remove line', 'comment_created': datetime.datetime(2022, 8, 30, 18, 57, 29, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958829000, 'comment_body': 'we are using a mix of ""dtype"" and tf.dtype in this PR, choose one and be consistent', 'comment_created': datetime.datetime(2022, 8, 30, 18, 58, 55, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958845654, 'comment_body': 'remove blank line', 'comment_created': datetime.datetime(2022, 8, 30, 19, 18, 5, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958848639, 'comment_body': 'could we wrap all these expected outputs in convert to tensor to avoid the bytestrings?', 'comment_created': datetime.datetime(2022, 8, 30, 19, 22, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958849132, 'comment_body': ""let's just write this test directly on integer inputs, we want to keep the unit tests isolated from each other where possible"", 'comment_created': datetime.datetime(2022, 8, 30, 19, 22, 38, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958849398, 'comment_body': 'remove line?', 'comment_created': datetime.datetime(2022, 8, 30, 19, 22, 58, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958853211, 'comment_body': 'what does this regex do? maybe drop a one line comment', 'comment_created': datetime.datetime(2022, 8, 30, 19, 27, 58, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958854682, 'comment_body': 'why batch 5 here? you only have two elements', 'comment_created': datetime.datetime(2022, 8, 30, 19, 29, 57, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 958867380, 'comment_body': ""It looks like you would like a positions tensor to be relative for each row right? `[[0, 1, 2], [0, 1], [0]]`\r\n\r\nIf I'm understanding correctly, let's try to find a way to avoiding needing to pass row lengths to map_fn. You should be able to compute the tensor you need as input outside the map_fn."", 'comment_created': datetime.datetime(2022, 8, 30, 19, 46, 50, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 960967622, 'comment_body': 'If I am understanding what you need positions for, I think this whole block could just be replace by something like...\r\n\r\n`positions = tf.ragged.range(inputs.row_lengths())`\r\n\r\nCould you try that out?', 'comment_created': datetime.datetime(2022, 9, 1, 18, 12, 46, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 961032110, 'comment_body': ""we shouldn't need this import, use exported ragged symbols"", 'comment_created': datetime.datetime(2022, 9, 1, 19, 34, 30, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 961082636, 'comment_body': 'I think we should actually use `positionts.row_lengths()` here, as we want to only ""make a coin flip"" for eligible tokens. We should make a similar change to `RandomDeletion`.', 'comment_created': datetime.datetime(2022, 9, 1, 20, 41, 19, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 961086274, 'comment_body': ""Rightt totally missed that. I'll carry forward the changes we make here to Random Deletion as well!"", 'comment_created': datetime.datetime(2022, 9, 1, 20, 46, 26, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}]","[{'commit_sha': '9546fd862cbbecad8cc2da0b30e5517526fded09', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fb7195d78a71ddb9a036c5988c7e30b41ac6e297', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '80c6f2de2d52f80c51cef7054c2ca48de9d737cb', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3059d05a1803479c91a3a934a1a895f9a6b05633', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a77dd70a9415660a5bb357d464b4a2195b1bd890', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a074bc0733309a03c297ba2016599591dc4da109', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fca0c5a1a11802e04a91c2a55331b19d81a3d78e', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f48d390dde96331e2b1cd6e7db5041511c2b16ee', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '92d93ce641f146a386bf2e6ac958fbf9315b1ab1', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '35a8402d563641c868cfd41b8d6e348a9052e18b', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '973d835fce07584070c1e0258a96fa1a2bd74570', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4872f09d0bf7685618792aa21889e760e3b49f8c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '31d6daae43c35886f7a03936ecc49571c71c73c3', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dbaeb40d3bad136bc599910b6807b53002aed9c5', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd479d1b06d76a009e96a99f5302ee5db52152347', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f7ef9c529c2cad3f5551bef9ab2a1e95a1ec20b4', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1b8f36ede06ae0dc46ef670067c8d4a26936dab3', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '78212c0470e503c91ce9eb0ae2a55e61a7998db6', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9a026894e445166c8b165ada6f0ed43f6f1a0d9d', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4fe37bc7d8ab956b937e6cf5cad0d585cabb5274', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'def2188fc1f347a59f7e72747818c6b5c8dce852', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0425f8ebd57033dd4a851f2735165252e6fab1da', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '384c2d493da1e47cb644159a22f801062696580f', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd6a8c961d1bd8b3c7b07bc8a4df47eb1cde3b0c0', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b97d14cba4a49b58a8e926ae78ccee95fb95abc5', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '973396447c880a24b0dfa5474d9e1cf8015ae9c9', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '11dfad8e7be7fa8a68524de292a2a6d15a369939', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
1002378015,Random Replacement,PR for Random Replacement Layer,False,274,https://api.github.com/repos/keras-team/keras-nlp/pulls/274,https://github.com/keras-team/keras-nlp/pull/274,open,681,0,3,21,1,7,0,1,[],2022-07-20 15:02:25+00:00,,0.0,,"[{'comment_id': 965295416, 'comment_body': 'under_score note camelCase for local variables', 'comment_created': datetime.datetime(2022, 9, 7, 21, 2, 49, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965296059, 'comment_body': 'nit: format this the same as the lower check', 'comment_created': datetime.datetime(2022, 9, 7, 21, 3, 39, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965433153, 'comment_body': 'this should not be needed, how could the binomial exceed the `positions.row_lengths()` given as input?', 'comment_created': datetime.datetime(2022, 9, 8, 1, 54, 47, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965433767, 'comment_body': 'maybe in the interest of readability pull this whole block into a private `_generate_skip_mask()` method.', 'comment_created': datetime.datetime(2022, 9, 8, 1, 56, 26, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965434462, 'comment_body': ""Careful to name this inferring the use case too much. Choose name like original_token, replacement_token, that don't assume a word or character level usage."", 'comment_created': datetime.datetime(2022, 9, 8, 1, 58, 12, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965434628, 'comment_body': 'Can we pull out a private method `_replace_token()` to make this more readable?', 'comment_created': datetime.datetime(2022, 9, 8, 1, 58, 38, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 965443691, 'comment_body': ""I'm skeptical we really need a `map_fn` with a loop inside it, with a scatter inside it. That sounds like it will be inefficient.\r\n\r\nIf we follow the flow we have in random deletion, we will have a complete `mask` containing only the indices we want to run a deletion on right? Then we could do something like run a `tf.map_fn` over the pair of `(inputs.flat_values, mask.flat_values)` and early return if mask == false, otherwise we lookup a replacement.\r\n\r\nThere might be other ways to do this, but overall we should:\r\n - avoid nested maps/loops\r\n - avoid scatters inside a map/loop"", 'comment_created': datetime.datetime(2022, 9, 8, 2, 22, 36, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '087a0cc07154dadf297f774e3faee32ebbbedf4d', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '84b5884c358913ea422f13a96bc33ba629b3befe', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3eb4b8d7c6f6aaeff54df02268fc3cb33f732e64', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c6d9b94d5a894052625d4854404ad902ad17a813', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'da7db2aba0d18c4dcd019cc8485bab4e4bddd899', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f0c2cf9d0a217475a8c9c6fdf044905bdbb2fdb4', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8ff3787a9c98931744489118f880c902ed467c98', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fb65aa5901f9f511e74447138a51349adfd8e206', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e0628417aa9755ef9130cb10575e75a444d888e9', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0db6c1d90b7838468e0500e9d3fcdf923c17a53c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fb2eea9cbaf80f01557ffc25905ba173260a9479', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '04fc189a9d492196faa7a820ff77ebe5cc761bc5', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8604c7ee19c7da5115154fa97d6b204b1ec25c8e', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6dce4930a27c7f512d620d225071222cfc9d61d8', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8e357f0f7e1b4ecf3555086b3040c15afb789736', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd3d35ff52208d3bee49f9cc91937a423587dbca1', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6da78b67fb334d13eb8ec287b6766e42a13bbb6d', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '082b46575da6eb214bb8a4876c5e6875a2a7cf4f', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f3562bb1cf48ed95a2a8ecdb26b2f8abb20cb0c3', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7850514d78dbce883bdfc2fc26bbc0257b8e08c4', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '91001ee925d604e9761cb5662e642246351b6f56', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
979628765,Random Insertion Layer,"Implementation of Random Insertion Layer, ToDo: Tests, Docstring, Error Handling
Creating the PR to discuss the API design and should we keep the insertion_list given it can be done through the function too",False,235,https://api.github.com/repos/keras-team/keras-nlp/pulls/235,https://github.com/keras-team/keras-nlp/pull/235,open,646,0,3,32,0,4,0,1,[],2022-06-27 06:22:27+00:00,,0.0,,"[{'comment_id': 937164570, 'comment_body': 'nit: period at the end ', 'comment_created': datetime.datetime(2022, 8, 3, 21, 45, 53, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 937164797, 'comment_body': 'max_insertions', 'comment_created': datetime.datetime(2022, 8, 3, 21, 46, 17, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 937166495, 'comment_body': 'use full name ""random number generator""', 'comment_created': datetime.datetime(2022, 8, 3, 21, 48, 58, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 937174804, 'comment_body': 'I am feeling confused about the usage here. This layer is called RandomInsertion, but here it gets a `replace_word` fn, which looks like a use case for RandomReplace? ', 'comment_created': datetime.datetime(2022, 8, 3, 22, 3, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}]","[{'commit_sha': 'e6eefd39b8c29bb5a0178a830f4024a4a3e2ccd1', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bdb84f18d263065a44bc3731369af40414848bda', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e7318f5b90443d6cca2540474811fbdb7ea55a0a', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '129649a52e9ff185a71b5a0b76edac3f70b3f883', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '25c059364760c8b2ced925b8acecdbfa970ebbf9', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '61b04817ac552a2e369947736ca152e7bed167e4', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4809f75657eaadca808efc76c537e9caf1849941', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '843b05ec30951fa81b4e2b9fb90da0e511f7c99d', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ce226583f45887af261315fd7df2d5c623a27bef', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7d5853fdb15ac941f469db9ca31b0db204a0cf77', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9b7d1fbb5b6ec900932b85904d557368ebab39f9', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f006ee21c3a975172791adc554be713736a751fe', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f063af28c1b33fba829a2582edffac98f02de54a', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '99cf7e2618f9497d0dad651e762af3aedd9f5bae', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '62c39065690841799d782da37b4d3f48defca29a', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1019e384303b3403f7f035ac28b082db06aff4d1', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e2d11b55745c2ab6c134b2811ed0f77a8142892c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9b1eff829a1116e174ea07efe8c62f1926be3a6f', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '17cfa0cb67b4244519a0d42daf22f73d24e51792', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b6c6d17eb954ab28521ae18d95639864dd663dbc', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7a630c3c74e396125aa29cce2d1b56a410f386b5', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'debe8f21200300e1eca796b3834dff43ea2fff5d', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e303ffd97d74a616cc1a22811fb4affeb4d1c4ac', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c1995d91428646beb51092616f1773351f8f34bd', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c11d1c353e28acdc07ae04fd0a6de2b554af3267', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f36a383cad13048b6e04bc2a4cddcd9f472090c7', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5d2b467962d832b68a0c5865314ad7ab08609e2f', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'dc1c8840ba3ec3206b10dab9b815074df1bf73ee', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2c310985655722b74572dc8ee3f6b120316247d9', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3a807675365f75f80a88591011d65d0615bafc33', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '80d7d00a8d7f5ee9676178d7e831e93acaeb293d', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2a8b37d6feac1a33b4b53317c871440185813a05', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
1020155132,Minor fixes to the Random Deletion Layer,"- Minor fix in docstring
- Added a missing test check ",True,286,https://api.github.com/repos/keras-team/keras-nlp/pulls/286,https://github.com/keras-team/keras-nlp/pull/286,closed,3,1,2,1,1,0,0,0,[],2022-08-08 11:16:02+00:00,2022-08-08 16:30:29+00:00,18867.0,5:14:27,[],"[{'commit_sha': '1344a00b495f3f431e57d844a9c63661d0f3f4fb', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
1045522705,Fixes for Random Deletion Layer,,True,339,https://api.github.com/repos/keras-team/keras-nlp/pulls/339,https://github.com/keras-team/keras-nlp/pull/339,closed,44,69,2,1,0,0,0,0,[],2022-09-03 16:55:28+00:00,2022-09-06 17:29:26+00:00,261238.0,"3 days, 0:33:58",[],"[{'commit_sha': '5d67eae480d546d47c205ef7fcd61129251602a6', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
1024208903,Fixes for WordPieceTrainer,Fixed to remove dependency between docstring tests and other tests for file removal,True,293,https://api.github.com/repos/keras-team/keras-nlp/pulls/293,https://github.com/keras-team/keras-nlp/pull/293,closed,32,24,2,4,1,6,0,0,[],2022-08-11 18:52:15+00:00,2022-08-11 19:55:09+00:00,3774.0,1:02:54,"[{'comment_id': 943831723, 'comment_body': ""remove this and the next line, since we don't actually have the print statement for the block examples"", 'comment_created': datetime.datetime(2022, 8, 11, 19, 4, 21, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943850651, 'comment_body': 'Done!', 'comment_created': datetime.datetime(2022, 8, 11, 19, 21, 44, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 943854204, 'comment_body': ""I think you remove the wrong two lines. Please remove\r\n\r\nvocab\r\n['[PAD]', '[CLS]', '[SEP]', '[UNK]', '[MASK]', 'a', 'b', 'm', 'p', 'r', 's', 't', '##at']"", 'comment_created': datetime.datetime(2022, 8, 11, 19, 26, 29, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943856062, 'comment_body': 'Oh right sorry about that ', 'comment_created': datetime.datetime(2022, 8, 11, 19, 29, 14, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 943857426, 'comment_body': 'for this and below, just use a local variable\r\n\r\nvocab_file = os.path.join(self.get_temp_dir(), ""test.txt"")\r\n\r\nthen reference that each place it is used', 'comment_created': datetime.datetime(2022, 8, 11, 19, 31, 4, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943860267, 'comment_body': ""do this for in each unit test if that's unclear, but the code will read a little better"", 'comment_created': datetime.datetime(2022, 8, 11, 19, 34, 46, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '918ab7aa2ebdecdef95b754940d8510eadc93ef1', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '03154e53579d0e4c4139cbf4a34cbe5b1c7c3c97', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '87cae899e356de01b0899d11326c65aa25abb801', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '32cbfd1ae685ddfb270a24064a0b81ba3ce446f8', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
1013415216,SentencePieceTrainer,"Fixes #249 
",True,281,https://api.github.com/repos/keras-team/keras-nlp/pulls/281,https://github.com/keras-team/keras-nlp/pull/281,closed,243,0,3,11,1,30,0,0,[],2022-08-01 06:44:34+00:00,2022-08-17 17:58:49+00:00,1422855.0,"16 days, 11:14:15","[{'comment_id': 935987980, 'comment_body': 'This comment \r\n```If provided it will be used as model_file proto_output_file which is passed to model_writer.```\r\nis unclear, what does it mean?\r\n', 'comment_created': datetime.datetime(2022, 8, 2, 20, 14, 58, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 935988756, 'comment_body': 'what is ""string proto_output_file to a SentencePiece proto file""? ', 'comment_created': datetime.datetime(2022, 8, 2, 20, 16, 1, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 935992313, 'comment_body': 'use the full path `keras_nlp.tokenizers.compute_sentencepiece_vocabulary`', 'comment_created': datetime.datetime(2022, 8, 2, 20, 20, 42, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 935993710, 'comment_body': 'use backtick: `proto_output_file`', 'comment_created': datetime.datetime(2022, 8, 2, 20, 21, 59, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 935994783, 'comment_body': 'no space between key and value: vocabulary_size=15', 'comment_created': datetime.datetime(2022, 8, 2, 20, 22, 55, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 935995981, 'comment_body': 'don\'t need to break the line, you can use\r\n```\r\n f""or `char`. Received: {model_type}.""\r\n```', 'comment_created': datetime.datetime(2022, 8, 2, 20, 24, 8, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 936037154, 'comment_body': 'Curious - must it be a numpy iterator? can it just be `iter(data)`?', 'comment_created': datetime.datetime(2022, 8, 2, 21, 20, 59, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 936037710, 'comment_body': 'WordPiece => SentencePiece', 'comment_created': datetime.datetime(2022, 8, 2, 21, 21, 51, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 936038489, 'comment_body': 'Let\'s print a warning, an example could be ""sentencepiece pip package is not found, please do `pip install sentencepiece` and rerun the script"".', 'comment_created': datetime.datetime(2022, 8, 2, 21, 23, 2, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 936039109, 'comment_body': 'Technically it is not a vocabulary for sentencepiece, but I am struggling to find a better name. Maybe `compute_sentencepiece_model_proto`? @mattdangerw Matt, opinions?', 'comment_created': datetime.datetime(2022, 8, 2, 21, 24, 4, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 936746030, 'comment_body': ""I'll rephrase that thanks for the catch!"", 'comment_created': datetime.datetime(2022, 8, 3, 14, 36, 26, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 936757998, 'comment_body': '@chenmoneygithub It causes an error as it feeds tensors instead of strings:\r\n<img width=""324"" alt=""image"" src=""https://user-images.githubusercontent.com/72096386/182637812-4ebf06dc-5f04-4892-ad80-45db8f6c1057.png"">\r\nThe error raised is: \r\n```\r\n[/usr/local/lib/python3.7/dist-packages/sentencepiece/__init__.py](https://localhost:8080/#) in _TrainFromMap4(args, iter)\r\n    399     @staticmethod\r\n    400     def _TrainFromMap4(args, iter):\r\n--> 401         return _sentencepiece.SentencePieceTrainer__TrainFromMap4(args, iter)\r\n    402 \r\n    403     @staticmethod\r\n\r\nRuntimeError: Internal: Not a string.\r\n```\r\nI reckon this is because it isn\'t designed to take tensor inputs', 'comment_created': datetime.datetime(2022, 8, 3, 14, 46, 37, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 936759871, 'comment_body': ""Ah it was a mistake I'll fix it hopefully should be better then!"", 'comment_created': datetime.datetime(2022, 8, 3, 14, 48, 13, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 940450132, 'comment_body': '`compute_sentence_piece_proto()` I think. Let\'s avoid the word ""model"" it means something else for Keras.', 'comment_created': datetime.datetime(2022, 8, 8, 16, 43, 9, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 940450481, 'comment_body': 'Actually we should not do that here, or this will print a warning for all users! Including people who are not touching sentencepiece.\r\n\r\nWe should just check `if spm is None` below in the body of the function, and raise an error with a nice message if that is true.', 'comment_created': datetime.datetime(2022, 8, 8, 16, 43, 37, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 940451848, 'comment_body': ""Don't add the # at the front, that might format this as a huge title. Just use a plain text sentence for the headers for examples."", 'comment_created': datetime.datetime(2022, 8, 8, 16, 45, 13, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 940452584, 'comment_body': 'Fix line length and remove space between lowercase =', 'comment_created': datetime.datetime(2022, 8, 8, 16, 46, 7, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 940456638, 'comment_body': ""Maybe just consolidate these two files ones into a single example with both input and output files, and format as a code block with triple backticks. That won't actually run in our testing, which might be a good thing. Having our docstring testing need file i/o could get annoying."", 'comment_created': datetime.datetime(2022, 8, 8, 16, 50, 41, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 940458523, 'comment_body': 'maybe so they know what you are printing out.\r\n\r\nReceived: type(data)={type(data)}', 'comment_created': datetime.datetime(2022, 8, 8, 16, 52, 51, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 940458902, 'comment_body': 'also should except a tuple probably', 'comment_created': datetime.datetime(2022, 8, 8, 16, 53, 17, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 940466129, 'comment_body': 'can we just fold this into one invocation? The args are almost the same\r\n\r\n```\r\nis_dataset = isinstance(data, tf.data.Dataset)\r\n\r\nspm.SentencePieceTrainer.train(\r\n    input=None if is_dataset else data,\r\n    sentence_iterator=data.as_numpy_iterator() if is_dataset else None,\r\n    ... all the same\r\n)\r\n```', 'comment_created': datetime.datetime(2022, 8, 8, 17, 1, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 940467397, 'comment_body': 'backticks around bytes as we are talking about the python literal bytes in this case.', 'comment_created': datetime.datetime(2022, 8, 8, 17, 2, 53, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 940467740, 'comment_body': 'or None if `proto_output_file` is provided.', 'comment_created': datetime.datetime(2022, 8, 8, 17, 3, 18, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 940472447, 'comment_body': 'Did we figure out the common mappings being used for models? Lets check t5, xlm-roberta, albert and see what they do.', 'comment_created': datetime.datetime(2022, 8, 8, 17, 9, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 940727937, 'comment_body': ""I think you should make sure to open all these files inside self.get_temp_dir()\r\n\r\nhttps://github.com/keras-team/keras-nlp/blob/master/keras_nlp/tokenizers/sentence_piece_tokenizer_test.py#L188\r\n\r\nMake sure there's no collisions for different tests reading writing the same files, and that things get cleaned up."", 'comment_created': datetime.datetime(2022, 8, 8, 22, 49, 20, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 942675436, 'comment_body': ""Ours seems to match others,  [T5](https://huggingface.co/docs/transformers/model_doc/t5#:~:text=decoder_start_token_id%20%2C%20which%20for%20T5%20is,used%20during%20T5's%20pre%2Dtraining.), [BERT and XLM-RoBERTa](https://www.kaggle.com/code/vbookshelf/basics-of-bert-and-xlm-roberta-pytorch) and [ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)"", 'comment_created': datetime.datetime(2022, 8, 10, 16, 39, 30, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 942717811, 'comment_body': 'Great! Thanks for checking.', 'comment_created': datetime.datetime(2022, 8, 10, 17, 27, 42, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943806285, 'comment_body': 'do we need a file.close for the else case? or a with scope for the file open?', 'comment_created': datetime.datetime(2022, 8, 11, 18, 32, 7, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 943829275, 'comment_body': '@mattdangerw A simple file.close() seems good to me!\r\nSo something like -\r\n```\r\n    if proto_output_file:\r\n        model_writer.close()\r\n    else:\r\n        return model_writer.getvalue()\r\n```\r\nright?', 'comment_created': datetime.datetime(2022, 8, 11, 19, 1, 18, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 943852908, 'comment_body': 'sgtm!', 'comment_created': datetime.datetime(2022, 8, 11, 19, 24, 42, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '370e8b56b8919ac09178f3fb1d2c408f7a45ad8d', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'bb2828b93959797af3a5369a878ce97efb12568e', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '577f9c082d58adbffe66f551151fbf0fd90665ad', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '83ae8bf3e705af9c2b78f79e5556c6dd7d7a115e', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '235eb5cf8e18e2aef3ea734fc06e31d9aed4bec7', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ac67358adea10c84745bf1f58b772188b49a3926', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '74f79aa4c3b7f3f289e0c5f1e58777625c39ab46', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'efbe389afdd11bbb49a0fe87100d7b0d43957706', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c9dd9d22c47364912d24dfc22a11dffe7dd45e39', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0c599f71ce139d5feb9725840443ca1a26ed8399', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e498a84e071bd8df55b3bffa8169764bde87c671', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
983054859,Fixed Bug with Unicode Tokenizer Vocab Size,As discussed renamed `self.vocabulary_size` to `self._vocabulary_size`,True,243,https://api.github.com/repos/keras-team/keras-nlp/pulls/243,https://github.com/keras-team/keras-nlp/pull/243,closed,5,5,1,1,0,0,0,0,[],2022-06-29 19:54:46+00:00,2022-06-29 19:59:52+00:00,306.0,0:05:06,[],"[{'commit_sha': '3e98a70d6b42b5f5d4123527e35885c5150ef651', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
926976950,Added a vocabulary_size argument to UnicodeCharacterTokenizer,"Fixes #155 
I've also added new tests and fixed old tests by modifying the config files with the new parameter",True,163,https://api.github.com/repos/keras-team/keras-nlp/pulls/163,https://github.com/keras-team/keras-nlp/pull/163,closed,77,0,2,8,1,4,0,0,[],2022-05-03 20:20:32+00:00,2022-05-05 06:27:50+00:00,122838.0,"1 day, 10:07:18","[{'comment_id': 864212801, 'comment_body': ""the OOV token -> the OOV value\r\n\r\n(I know I wrote that in description, but an id isn't actually a token, we should make sure we clear on our terminology)"", 'comment_created': datetime.datetime(2022, 5, 3, 20, 31, 18, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 864244146, 'comment_body': ""You could probably just move setting `vocabulary_size` up into the first example in this by setting it in `tokenizer` and `exp_config`. And ditch this extra case here.\r\n\r\nWe want to keep this test from exploding inside, and I don't think we should try to test every permutation here."", 'comment_created': datetime.datetime(2022, 5, 3, 20, 47, 11, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 864250393, 'comment_body': ""It might be nice to add an example for this? As a lot of real world use cases will probably want to add a cap.\r\n\r\nI'm not sure where to draw the cutoff for an example but here's a browser. https://codepoints.net/basic_multilingual_plane\r\n\r\nMaybe we just show passing `vocaublary_size=592` for support latin + latin extension characters."", 'comment_created': datetime.datetime(2022, 5, 3, 20, 49, 49, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 864470183, 'comment_body': 'Yeah this seems like a good addition!', 'comment_created': datetime.datetime(2022, 5, 4, 5, 37, 58, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}]","[{'commit_sha': '1451f43647123219ab4ac8cd6ad5d015c83fd168', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '25a87147755b9a97ce5b56cd53fb4c700d0c3e02', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9d62980601fa06bda8978b7a00165b48cb655497', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4e0ba7abf3df4494ebcbecc6420322f9a5eaeace', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '53714019b4dfb3208c2caac299d8a567bdf24d96', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8ce06b226b1700d6fbc99491707a0c783422eb26', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1b3113af442708c6b907787405133a8c48c9f2f9', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}, {'commit_sha': '35863ad11e5fcbd1e3261a31e754b757e6e32fb5', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
911218503,Adding Utility to Detokenize as list of Strings to Tokenizer Base Class,"In continuation to #119 which was closed due to git issues
Fixes #113 
Currently in progress",True,124,https://api.github.com/repos/keras-team/keras-nlp/pulls/124,https://github.com/keras-team/keras-nlp/pull/124,closed,81,0,3,13,12,20,0,0,[],2022-04-16 07:10:55+00:00,2022-05-03 17:21:19+00:00,1505424.0,"17 days, 10:10:24","[{'comment_id': 852382438, 'comment_body': 'Keep this private.', 'comment_created': datetime.datetime(2022, 4, 18, 20, 14, 41, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 852382668, 'comment_body': 'if isinstance(inputs, bytes):', 'comment_created': datetime.datetime(2022, 4, 18, 20, 15, 5, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 852384473, 'comment_body': ""Looks like there's a lot of repeated cases, this logic could be tightened up.\r\n\r\nWe should make by the time we hit this function we have either a nested list of byte strings, or a scalar byte string. Then you helper can just handle two cases, recursing on a list, or converting a scalar.\r\n\r\nBasically, the helpful should be only two cases, we can remove the map call below this line, and you shouldn't need to special case the scalar case here."", 'comment_created': datetime.datetime(2022, 4, 18, 20, 17, 56, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 852384914, 'comment_body': 'Empty line before args.', 'comment_created': datetime.datetime(2022, 4, 18, 20, 18, 40, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 852390758, 'comment_body': 'Detokenize, then convert the output tensor to nested lists of python strings.\r\n\r\nThis is a convenience method layered on top of `detokenize()`. This method\r\nwill call `detokenize()` and transform the output string tensors back to python\r\nstrings, by first converting output tensors to nested lists of elements, and then\r\nconverting each byte string to a python string.', 'comment_created': datetime.datetime(2022, 4, 18, 20, 28, 19, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 852491535, 'comment_body': ""it will be helpful to add a comment here what the inputs can be. Reading the code, you check in order:\r\n1) inputs is byte string\r\n2) inputs[0] is a byte string. \r\n3) Neither 1) or 2) applies, then keep recursive calls. \r\n\r\nHere we are making many assumptions, e.g., in 2) we assume inputs is always subscriptable, so for easier maintainence, let's add some explanation."", 'comment_created': datetime.datetime(2022, 4, 18, 23, 56, 48, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 852499373, 'comment_body': 'Thanks for the review! Will make these more descriptive!', 'comment_created': datetime.datetime(2022, 4, 19, 0, 20, 8, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 857103175, 'comment_body': ""@mattdangerw just to confirm what you've written down are changes for the docstring right?"", 'comment_created': datetime.datetime(2022, 4, 24, 10, 20, 21, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 857889463, 'comment_body': 'single underscore for private method. Prefer methods name to be verbs, e.g. `_decode_strings`.', 'comment_created': datetime.datetime(2022, 4, 25, 17, 59, 21, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 857889687, 'comment_body': 'return inputs.decode(""utf-8"")', 'comment_created': datetime.datetime(2022, 4, 25, 17, 59, 37, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 857890341, 'comment_body': ""Also, you don't need to take *args, **kwargs here. We don't expect this part of the layer to be subclassed with custom arguments. It's private to the base layer."", 'comment_created': datetime.datetime(2022, 4, 25, 18, 0, 29, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 857893452, 'comment_body': 'Just move the scalar value in here, it would be more readable\r\n\r\nif detokenized_input.shape.rank != 0:', 'comment_created': datetime.datetime(2022, 4, 25, 18, 4, 19, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 857893612, 'comment_body': 'return immediately instead of create local variable', 'comment_created': datetime.datetime(2022, 4, 25, 18, 4, 31, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 857894476, 'comment_body': 'return [self._decode_strings(x) for x in inputs]', 'comment_created': datetime.datetime(2022, 4, 25, 18, 5, 38, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 857896194, 'comment_body': 'Just make this a really simple class where tokenizer and detokenize just return the input directly, completely unmodified.\r\n\r\nThat will make it easer to read what is being tested below in the unit tests you are adding.', 'comment_created': datetime.datetime(2022, 4, 25, 18, 7, 58, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 857897738, 'comment_body': 'Here you could just pass `tf.ragged.constant([["""", ""samurai""]])`, and not call tokenize at all.', 'comment_created': datetime.datetime(2022, 4, 25, 18, 10, 3, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 857898081, 'comment_body': ""Same, here just pass a `tf.constant([...])`, don't bother with a sequence_length argument, and don't call tokenize in this test."", 'comment_created': datetime.datetime(2022, 4, 25, 18, 10, 47, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 857898704, 'comment_body': '`tf.constant("""")`, and don\'t call tokenize.', 'comment_created': datetime.datetime(2022, 4, 25, 18, 11, 35, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 857902531, 'comment_body': 'yes!', 'comment_created': datetime.datetime(2022, 4, 25, 18, 16, 26, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 857914655, 'comment_body': ""I'll move this inside the elif however I think we can't do away with the variable itself as we're doing a .numpy() call and after that we can't call detokenized_input.shape.rank"", 'comment_created': datetime.datetime(2022, 4, 25, 18, 32, 43, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}]","[{'commit_sha': '395a29600cc86fd3e8a2e97f8d3153e2f2952cd6', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e4fead56388147101852a6af87f425dedf295f8c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a7271ff8d546bb39a8c0ea93c3bd06588e836ffb', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4bf17440f8718fe5e927d777d8a8b512023975bc', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd6dbe9d459f6eae38852777a1be82bb83aa0a178', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'aa3f10d49e17b1af57d051b3979af3580cce0684', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5ce809195edaea8816b9015fd9fa3d91318f1739', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e1b6df8cae807cee816694f78030f6ffcad7cd41', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd615cb55fd930259dbba83583a96cc6236a96c56', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8d981211076024036eaff10f07e220296c7d757b', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f84b49dcc7d7ded8f9b9e4d825afdb70ba725768', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}, {'commit_sha': '511ccd684fe1aa89b71ca4083935fadcf98d5462', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9d6188f501e46b3ee56e9c4998b6c3ccea81b1d6', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
903651586,Adding a UnicodeCharacterTokenizer,"Fixes #78 
Sorry for the late PR
I've added the Tokenizer as well as added some tests.
Based on the discussion with @mattdangerw I've skipped custom padding tokens but since I had already implemented custom input/output encoding type I've retained that for now 
There's one issue which I'm facing right now w.r.t detokenization - 
When the input is something like: `""""` it gets tokenized to `[9600, 9601, 9602, 9603]` but during detokenization it's not clear how do I bring back the original format as currently I detokenize like [this](https://github.com/aflah02/keras-nlp/blob/8cd02d298ab174b17604e7195a30e54f4c8687c8/keras_nlp/tokenizers/unicode_character_tokenizer.py#L301) and this outputs `b""\xe2\x96\x80\xe2\x96\x81\xe2\x96\x82\xe2\x96\x83""`. One way to then decode this is to do a `.numpy()` and `.decode() `call but I'm not sure if this is the intended way",True,100,https://api.github.com/repos/keras-team/keras-nlp/pulls/100,https://github.com/keras-team/keras-nlp/pull/100,closed,594,2,4,42,8,26,0,0,[],2022-04-08 09:41:52+00:00,2022-04-16 01:49:52+00:00,662880.0,"7 days, 16:08:00","[{'comment_id': 847895225, 'comment_body': 'Do this before the unicode_encode call with `tf.ragged.boolean_mask(inputs, inputs != 0)`', 'comment_created': datetime.datetime(2022, 4, 12, 2, 46, 34, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847895348, 'comment_body': 'Unused? For all of these constants up here.', 'comment_created': datetime.datetime(2022, 4, 12, 2, 47, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847895690, 'comment_body': 'Remove this. Unused.', 'comment_created': datetime.datetime(2022, 4, 12, 2, 47, 51, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847896787, 'comment_body': 'For most of these example, we should show non-ascii input characters. Add in a word from another language, e.g.', 'comment_created': datetime.datetime(2022, 4, 12, 2, 50, 56, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847897051, 'comment_body': 'Why are input_encoding and output_encoding different capitalization and hyphenation? We should avoid this.', 'comment_created': datetime.datetime(2022, 4, 12, 2, 51, 42, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847897597, 'comment_body': 'We should catch this error during layer initialization; that would be more friendly. Here we can assume the check has already been run.', 'comment_created': datetime.datetime(2022, 4, 12, 2, 53, 12, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847897876, 'comment_body': 'Avoid comments like this where you just paraphrase what is obvious from code. Keep comments for when code needs extra explanation.', 'comment_created': datetime.datetime(2022, 4, 12, 2, 54, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847898218, 'comment_body': ""Try to make these examples a little more concise so that we don't get formatted into so many lines like this."", 'comment_created': datetime.datetime(2022, 4, 12, 2, 54, 50, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847898310, 'comment_body': 'Same here (shorten)', 'comment_created': datetime.datetime(2022, 4, 12, 2, 55, 6, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847898521, 'comment_body': 'remove newline', 'comment_created': datetime.datetime(2022, 4, 12, 2, 55, 35, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847899033, 'comment_body': 'Only uppercase the start of sentence.', 'comment_created': datetime.datetime(2022, 4, 12, 2, 57, 3, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 847966840, 'comment_body': ""Thanks for the review!\r\nI'm not entirely sure why do we even require this mask? Also since `regex_replace` works on strings I don't believe it can be done before the unicode_encode call. Am I missing something?"", 'comment_created': datetime.datetime(2022, 4, 12, 4, 53, 35, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 847966918, 'comment_body': 'Thanks missed those!', 'comment_created': datetime.datetime(2022, 4, 12, 4, 53, 48, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 848961272, 'comment_body': 'I know this is from ByteTokenizer, but we really don\'t need to say ""batch up"" here. ""batch"" is clearer', 'comment_created': datetime.datetime(2022, 4, 13, 0, 2, 39, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 848961510, 'comment_body': 'same, ""batch the dataset for dense outputs"". remove ""up""\r\n\r\nA couple more ""up""s below as well', 'comment_created': datetime.datetime(2022, 4, 13, 0, 3, 8, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 848962196, 'comment_body': 'The case does not agree with the actual default anymore. Also good to mention what values are accepted here.', 'comment_created': datetime.datetime(2022, 4, 13, 0, 4, 46, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 848962936, 'comment_body': ""nit: there's a lot of unnecessarily capitalized words, here and elsewhere\r\n\r\n```\r\n# Optionally lowercase the input text.\r\n```"", 'comment_created': datetime.datetime(2022, 4, 13, 0, 6, 44, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 848963444, 'comment_body': '`tf.ragged.boolean_mask(inputs, inputs != 0)` would replace this like entirely, by removing all the zeros from the input before unicode encoding, rather than after. I think it is more readable, as regexes are often hard to parse. See ByteTokenizer as an example.', 'comment_created': datetime.datetime(2022, 4, 13, 0, 8, 8, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 849822798, 'comment_body': ""I've fixed this however the docs don't have the default values for the input_encoding for [tf.strings.unicode_decode](https://www.tensorflow.org/api_docs/python/tf/strings/unicode_decode)"", 'comment_created': datetime.datetime(2022, 4, 13, 19, 28, 8, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 850883722, 'comment_body': 'I think some of these examples below are out of date. I see a lot examples with four inputs and three outputs.\r\n\r\nHow about we change all of these examples to `[""Book"", """", """"]`, so we are showing the same word in a few different languages (and make these example slightly shorter.', 'comment_created': datetime.datetime(2022, 4, 14, 23, 21, 7, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 850883819, 'comment_body': 'remove ""up""', 'comment_created': datetime.datetime(2022, 4, 14, 23, 21, 16, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 850883948, 'comment_body': 'remove ""up""', 'comment_created': datetime.datetime(2022, 4, 14, 23, 21, 28, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 850884909, 'comment_body': ""Can we still try making this example input a little shorter, so black doesn't format the output into 20+ lines?"", 'comment_created': datetime.datetime(2022, 4, 14, 23, 22, 57, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 850885018, 'comment_body': 'Same here', 'comment_created': datetime.datetime(2022, 4, 14, 23, 23, 7, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 851070513, 'comment_body': 'Sure that seems easier to analyze as well!', 'comment_created': datetime.datetime(2022, 4, 15, 5, 35, 33, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 851076531, 'comment_body': ""That's a good catch essentially since I was batching it in batches of size 3 this is present in many examples however in hindsight this does seem more tricky and less helpful for the user. \r\nI'll fix those and change to only these 3"", 'comment_created': datetime.datetime(2022, 4, 15, 5, 55, 35, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}]","[{'commit_sha': 'f04325cf5993b0cdb77fbcfed4c260e631892fa1', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '20db313023c8cba9a02b9936a22889d97c574820', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e0b6c444eb1725075b678c14375efe5cf9346eda', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f949f5aab69983971a099e261b40ae239fd8c910', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ac2bb89219063458a9550d8504a018bbb6903aab', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1d1a1a24445ea599068c332ebc332b2777914a08', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ef1b5b644276b8479074c064677f4188def0551c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '0de31535bc65b2185da12d8d4c278d1a584d8494', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '161e316da86e9ca24096896560287b893d54bf7b', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8054855f9914d067e77792baf174b0ddb67669bc', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd638260e848f0b7f97def00dd49864ad14d746a8', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5fad8ad5d030b7670b2e4a144b0dc514e3d3c551', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a6b095faba9bba437d304a9bd8bce0652c36597e', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c45de160b66b526c6323c91f79addda57d625af2', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '78f4da7457092b6a98931f90f4024904ef89af71', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '927fdc680ced6cf6774dfa1dea62c5bd96dc7725', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '68830d40564d15231b3feef27872ec361be07a8d', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7137c394cdb8b184780fd7fcefa67e87ede05d85', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '8cd02d298ab174b17604e7195a30e54f4c8687c8', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '11e5eed536f66b147fddfd05aeda069221c17f9a', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '09f5f30f3096c4a11bb527922a0f1f04358c60ed', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '91c06afb4778e0b8f96116c3057dcb025dc115d4', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '24fb3acb04aed2a20061ad07f48962037b1f29f5', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2ded9a7e40375ea113986f169dc2f07fdb053434', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '82ee48c804aeaa717e4b95f402f6a41ea3c3a07e', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '43c33c8d0a617dcc9cb7d0895b87016888fe07a8', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '073129482dedb2cc3b4926c93519dcb2b9c0edfc', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '4da87399f056fcd593d9dd48ab99448c2eaab7da', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '996fd2589145c21ba3fa8c77fbbe2f74eb73152a', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '44b01f762e9f03bc76b7fcba046ed7aaa4fa431e', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd3fe3206b6b9b86fbba5b7dbc532a400d1e1eb56', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'aaf945426c308d0918f68af65b5e0066c51b4f0e', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1046798f0a85295fc1cd5f1af2d5fa633842a239', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'fa8eeea6007dac771bbde2dea64c0a452acf2e03', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b47806a2ad59daf69d2188190edb4d6c56cfb7e2', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '9d1514fa6ffb9f295995d672e3dfb58ffc60748c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1ec59dfb6ab3f8376e8111da2c7773895eebcd1b', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ba76dcce6f6e1459fc2cf3911215e715eefd0f23', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ac55c10e0778b63eaf8369f5a2cfaf9ca4b61534', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '96cf0509eb6f89e2187510178623fa677ff80cbe', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a915c3d863853846e1936222f3105f109befa3ae', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}, {'commit_sha': '053375ddc8530924d8e7bbbbe5842c03edcd4c7a', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
902416010,Fixing rank 1 outputs for WordPieceTokenizer,"Fixes #49 
I've added a unit test as well based on the recommendations to test with batching tensors with `tf.data.experimental.dense_to_ragged_batch`
Sorry for the late PR I got winded up in some exams
",True,92,https://api.github.com/repos/keras-team/keras-nlp/pulls/92,https://github.com/keras-team/keras-nlp/pull/92,closed,23,0,2,13,1,4,0,0,[],2022-04-07 08:03:30+00:00,2022-04-07 20:20:25+00:00,44215.0,12:16:55,"[{'comment_id': 845406171, 'comment_body': 'I think we could just convert to tensor generally here, rather then converting only to check the rank. Add something like this first...\r\n\r\n```\r\n        if not isinstance(inputs, (tf.Tensor, tf.RaggedTensor)):\r\n            inputs = tf.convert_to_tensor(inputs)\r\n```\r\n\r\nThen you can just run `scalar_input = tf.convert_to_tensor(inputs).shape.rank == 0` assuming tensor.', 'comment_created': datetime.datetime(2022, 4, 7, 17, 37, 44, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 845410025, 'comment_body': ""Does the unit test below break without this? If it doesn't we might be able to remove.\r\n\r\nIf we do need it, we should probably update this set the shape to `[self._sequence_length]`. If sequence length is set, we do want to make sure the static shape is correct set to `[self._sequence_length]`."", 'comment_created': datetime.datetime(2022, 4, 7, 17, 42, 15, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 845421655, 'comment_body': 'Got it this is indeed a better way, thanks!', 'comment_created': datetime.datetime(2022, 4, 7, 17, 56, 20, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 845423426, 'comment_body': ""Yeah, it does fail without this check, I've made the changes to set shape appropriately now"", 'comment_created': datetime.datetime(2022, 4, 7, 17, 58, 17, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}]","[{'commit_sha': '854a7738ac671d19e129d49d23353f34b2bbb972', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '29749ca056b63fa3940816765e6c8a716accbdf9', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '81fa8fa3aa007194ea83300fdca943a3e5f17814', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1bee558195c915fdf718d9288c2482127087cef2', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '837d3c6b5691e8af1628f7c06fbd1db0ff71a34b', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '80eb54a104967543a0321cebb3b172aa7a543f4c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '59ed64440eab3e7220daffe413995582809f69b5', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f1d515ca483e96c69995550020df89d35efdd7fe', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2a7a15c60d8ce40f6c630b3284a78b9fedb46213', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3fb6dd57be33172b6b8e4e1c0816f96e83b74e54', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '97af8e7974c5e3f6b6003df51a483aa89aefc47b', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'ad78a12e09e8d63f7a6280303641a433e13a4fa5', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'f4f76be15fdd3aaf73ea38046721721a8a2956c0', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
1045541911,Migrating from Datasets to TFDS for GLUE Example,Resolves #333 ,True,340,https://api.github.com/repos/keras-team/keras-nlp/pulls/340,https://github.com/keras-team/keras-nlp/pull/340,closed,15,14,2,4,1,4,0,0,[],2022-09-03 19:01:12+00:00,2022-09-07 19:19:20+00:00,346688.0,"4 days, 0:18:08","[{'comment_id': 964081856, 'comment_body': 'You should be able to do this more succinctly!\r\n\r\n`train_ds, test_ds, val_ds = tfds.load(f""glue/{task_name}"", split=[""train"", ""test"", ""validation"")`', 'comment_created': datetime.datetime(2022, 9, 6, 19, 13, 23, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964098844, 'comment_body': 'This looks the same in all the cases, just move this to a named function above?', 'comment_created': datetime.datetime(2022, 9, 6, 19, 35, 46, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964099494, 'comment_body': ""Always remember to add `num_parallel_calls=tf.data.AUTOTUNE`, though in this case it probably doesn't affect much."", 'comment_created': datetime.datetime(2022, 9, 6, 19, 36, 23, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 964336019, 'comment_body': 'what about something like `def split_features(x):` which does the inner lambda, and then you call three times (for each dataset)\r\n\r\n`train_ds = train_ds.map(split_features, num_parallel_calls=tf.data.AUTOTUNE)`\r\n\r\nThat would be the most idiomatic', 'comment_created': datetime.datetime(2022, 9, 7, 2, 57, 45, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '4eaaee35c916e3447914efd167b63f9c954aa8ed', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'cb9d266fe43cba22bb95281da3f9fc5a38df8c80', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '1fa9b734121c7551a18372ee10864759b7f74891', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '051e8d082f8779cfd66c492af88eee739e784c20', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
984897941,Open Ended Text Generation Guide KerasNLP,"This guide compares byte and unicode tokenizers for text generation
@mattdangerw Raised a PR based on our discussion, it's open for review now!
Referring to the README, I will add the generated files after approval.
Reference: https://github.com/keras-team/keras-nlp/issues/191",False,956,https://api.github.com/repos/keras-team/keras-io/pulls/956,https://github.com/keras-team/keras-io/pull/956,closed,282,0,1,3,2,15,1,0,[{'name': 'stat:awaiting response from contributor'}],2022-07-01 12:14:28+00:00,2023-08-17 06:46:36+00:00,35577128.0,"411 days, 18:32:08","[{'comment_id': 916093835, 'comment_body': ""Please improve the text copy:\r\n\r\n- End every sentence with a period.\r\n- Only include sentences that actually convey useful information that the reader needs to know. Value your reader's time."", 'comment_created': datetime.datetime(2022, 7, 7, 16, 56, 53, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 916094175, 'comment_body': 'Avoid subjective statements such as ""This tutorial will be pretty useful"".', 'comment_created': datetime.datetime(2022, 7, 7, 16, 57, 18, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 916094617, 'comment_body': 'These are instances, not classes, they should be in snake case like `unicode_tokenizer`', 'comment_created': datetime.datetime(2022, 7, 7, 16, 57, 48, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 916095131, 'comment_body': 'Use `keras.layers`', 'comment_created': datetime.datetime(2022, 7, 7, 16, 58, 27, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 916095370, 'comment_body': 'Rather than LSTM prefer using the TransformerEncoder.', 'comment_created': datetime.datetime(2022, 7, 7, 16, 58, 43, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 916095703, 'comment_body': 'Use explicit variable names. E.g. `byte_input`', 'comment_created': datetime.datetime(2022, 7, 7, 16, 59, 8, tzinfo=datetime.timezone.utc), 'commenter': 'fchollet', 'type': 'User'}, {'comment_id': 916398772, 'comment_body': 'I think we can cut the intro paragraph, start at next paragraph', 'comment_created': datetime.datetime(2022, 7, 8, 1, 46, 37, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916398901, 'comment_body': 'period at end of sentence', 'comment_created': datetime.datetime(2022, 7, 8, 1, 46, 56, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916399608, 'comment_body': 'out -> our', 'comment_created': datetime.datetime(2022, 7, 8, 1, 48, 56, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916401302, 'comment_body': 'A common pattern is assigning `byte_input` followed by progressively reassigning `byte_output`. Then `byte_model = keras.Model(byte_input, byte_output)`', 'comment_created': datetime.datetime(2022, 7, 8, 1, 54, 10, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916402754, 'comment_body': ""Overall I think this example would be most compelling with a CJK dataset or some sort. That would really hammer home the tradeoff of a large embedding vs a much longer input sequence. But I don't know of a good data source we could swap in here.\r\n\r\nJust leaving this comment in case someone happens to know a dataset :)"", 'comment_created': datetime.datetime(2022, 7, 8, 1, 58, 15, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916407310, 'comment_body': 'The next couple code blocks could be simplified a bit. Maybe let\'s use a fixed shorter prompt, ""Rocinante"", or something like that, and avoid all the to_tensor shape manipulation. You can probably leave max_length as a constant directly in greed_search rather then passing it around. And if you are only starting with a scalar promp, you should be able to avoid all the [0] indexing you have.', 'comment_created': datetime.datetime(2022, 7, 8, 2, 11, 24, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916460163, 'comment_body': 'maybe just phrase this as\r\n\r\nLet\'s try tokenizing the the input string ""seora ma"".\r\n\r\nprint(codepoint_tokenizer(""seora ma))\r\nprint(byte_tokenizer(""seora ma))\r\n\r\nThis captures the tradeoff between the two tokenizer. The byte tokenizer will handle any text with only 256 output ids, at the cost of encoding to a longer sequence. The unicode tokenizer will produce shorted sequence, at the cost of a larger id space (and larger embeddings).', 'comment_created': datetime.datetime(2022, 7, 8, 4, 49, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 916460973, 'comment_body': 'We need to keep looking at where these erroneous characters are coming from `\x80`, maybe we need a longer dataset? Different hyperparameters? Maybe trying the transfomer will help?\r\n\r\nEspecially with greed search, we should at least be able to train to the point where we output a valid set of characters that are actually in the source text.', 'comment_created': datetime.datetime(2022, 7, 8, 4, 51, 24, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 918647886, 'comment_body': 'Got it!', 'comment_created': datetime.datetime(2022, 7, 12, 7, 39, 47, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}]","[{'commit_sha': 'c4f993553d45c676cf8f8d374025a6079b8f64bc', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd97a27e0a06f3d106cdff50dfa9142c2f5110d66', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b2df8a528722da6aa428407eb31f582a8ffcc143', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
889849530,Added Debug Info for Line Ending Issues,"Fixes #60 
I've added a couple of sentences in the contributing file to help users debug the issue",True,64,https://api.github.com/repos/keras-team/keras-nlp/pulls/64,https://github.com/keras-team/keras-nlp/pull/64,closed,6,0,1,3,2,1,0,0,[],2022-03-25 19:45:07+00:00,2022-03-28 18:42:24+00:00,255437.0,"2 days, 22:57:17","[{'comment_id': 835568690, 'comment_body': 'This looks good, but can you post the full error in a comment here or on this issue so we can take a look?', 'comment_created': datetime.datetime(2022, 3, 25, 19, 49, 43, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}]","[{'commit_sha': '545c078ad5df3e992d0e3fc06b04f05bb115fa05', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'a55083959b411c2842f6df890da0191645c7627c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '3b2ad94df30dc0d547e9398638aa29b8955391b2', 'committer_username': 'mattdangerw', 'committer_name': 'Matt Watson', 'committer_email': None, 'commit_date': datetime.datetime(2012, 1, 29, 22, 35, 17, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
926899570,Fixed Import Error,Fixes #159 ,True,161,https://api.github.com/repos/keras-team/keras-nlp/pulls/161,https://github.com/keras-team/keras-nlp/pull/161,closed,15,1,2,2,2,0,0,0,[],2022-05-03 18:56:22+00:00,2022-05-03 19:00:48+00:00,266.0,0:04:26,[],"[{'commit_sha': '4bf8a23742ebe20f821bb4fd9f5e1bda69acaf0c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '91c9e617f120a6113e359c444ba9fa4426dae28c', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
983119568,Fixed Import for top_p_search util,,True,245,https://api.github.com/repos/keras-team/keras-nlp/pulls/245,https://github.com/keras-team/keras-nlp/pull/245,closed,1,0,1,1,0,0,0,0,[],2022-06-29 21:12:31+00:00,2022-06-29 21:34:58+00:00,1347.0,0:22:27,[],"[{'commit_sha': 'f38738d50892b1eb6d2c978123b84600b72ec0ad', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219
883939607,Added Kernel and Bias Initializers to Encoder and Decoder,"Fixes - #48 
I've added the required parameters for the encoder and decoder and also added 2 new tests to check the config files generated. I've also added relevant docstrings in the encoder and decoder files",True,50,https://api.github.com/repos/keras-team/keras-nlp/pulls/50,https://github.com/keras-team/keras-nlp/pull/50,closed,105,10,4,18,5,23,0,1,[],2022-03-19 08:51:57+00:00,2022-03-24 00:12:30+00:00,400833.0,"4 days, 15:20:33","[{'comment_id': 831386127, 'comment_body': 'Generally we avoid using verbs in the docstring of args. Here kernel_initializer should have docstring ""The kernel initializer for the..."" instead of """"Sets the kernel initializer for the..."". Please change, thanks! \r\n\r\nAdditionally, please add period (\'.\') to the end of docstring, thanks!', 'comment_created': datetime.datetime(2022, 3, 21, 17, 47, 8, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 831387937, 'comment_body': 'A minor point: please keep `name` the last argument before **kwargs, thanks!', 'comment_created': datetime.datetime(2022, 3, 21, 17, 49, 3, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 831389175, 'comment_body': 'Please move this line after \r\n```        \r\nself.kernel_initializer = kernel_initializer\r\nself.bias_initializer = bias_initializer\r\n```\r\nwe want to keep attributes setting from constructor arguments together. ', 'comment_created': datetime.datetime(2022, 3, 21, 17, 50, 20, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 831391532, 'comment_body': 'Please use `keras.initializers` instead of `from keras import initializers`.\r\n\r\nSorry this is our bad, we should document is somewhere.', 'comment_created': datetime.datetime(2022, 3, 21, 17, 52, 59, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 831393126, 'comment_body': 'You can delete this decoder1, as this decoder2 covers decoder1. ', 'comment_created': datetime.datetime(2022, 3, 21, 17, 54, 51, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 831393874, 'comment_body': 'Same as decoder: remove ""Sets"" and add period to the end, thanks!', 'comment_created': datetime.datetime(2022, 3, 21, 17, 55, 41, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 831394031, 'comment_body': 'Same as decoder: change the order.', 'comment_created': datetime.datetime(2022, 3, 21, 17, 55, 53, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 831394391, 'comment_body': 'Delete encoder1.', 'comment_created': datetime.datetime(2022, 3, 21, 17, 56, 14, tzinfo=datetime.timezone.utc), 'commenter': 'chenmoneygithub', 'type': 'User'}, {'comment_id': 831397541, 'comment_body': ""These defaults should be 'glorot_uniform' and 'zero' I think. That would match what's on the underlying layers I'm pretty sure, while also communicating the default to users browsing code/docs."", 'comment_created': datetime.datetime(2022, 3, 21, 17, 59, 43, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 831400559, 'comment_body': 'I think you could actually call `kernel_initializer = keras.initializers.get(kernel_initializer)` here in init (same for bias). That would raise a friendly error message for a bad arg on layer creation, which I would find a lot more usable than only getting the error after calling the layer.', 'comment_created': datetime.datetime(2022, 3, 21, 18, 3, 15, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 831401750, 'comment_body': ""Did you run black on this? I'm pretty sure the formatter would push this to an argument a line."", 'comment_created': datetime.datetime(2022, 3, 21, 18, 4, 43, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 831402069, 'comment_body': 'here too, just use initializers from the keras import', 'comment_created': datetime.datetime(2022, 3, 21, 18, 5, 7, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 831405185, 'comment_body': 'We should probably actually do the same for activation while you are at it.', 'comment_created': datetime.datetime(2022, 3, 21, 18, 8, 52, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 831408619, 'comment_body': 'Once you make the fix above, you could test that an invalid initialzer name will throw. Should like something like...\r\n\r\n```\r\nself.assertRaises(ValueError):\r\n    transformer_decoder.TransformerDecoder(\r\n        intermediate_dim=4,\r\n        num_heads=2,\r\n        kernel_initializer=""invalid"",\r\n    )\r\n```', 'comment_created': datetime.datetime(2022, 3, 21, 18, 13, 10, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 831980671, 'comment_body': 'Thanks for the review\r\nCould you elaborate a bit which line should I move? Is this the `self._built = False` line?', 'comment_created': datetime.datetime(2022, 3, 22, 9, 55, 47, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 831996699, 'comment_body': ""Hey\r\nThanks for the review, I didn't run black, I'll run it on these!"", 'comment_created': datetime.datetime(2022, 3, 22, 10, 12, 36, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 832016743, 'comment_body': ""I've made these changes for both of the initializers and the activation, Thanks!"", 'comment_created': datetime.datetime(2022, 3, 22, 10, 33, 31, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 832035549, 'comment_body': 'Just to confirm this syntax also works right?\r\n\r\n```\r\n        self.assertRaises(\r\n            ValueError,\r\n            transformer_encoder.TransformerEncoder, intermediate_dim = 4, num_heads = 2, dropout = 0.5, kernel_initializer = ""Invalid""\r\n        )\r\n```', 'comment_created': datetime.datetime(2022, 3, 22, 10, 54, 16, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 832467054, 'comment_body': ""Move this to a separate test, you aren't actually testing the config here."", 'comment_created': datetime.datetime(2022, 3, 22, 17, 55, 22, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 832468996, 'comment_body': 'Also, this should look like a `with` statement, followed by the code that should raise I think.\r\n\r\n```\r\nwith self.assertRaises(ValueError):\r\n    code that raises\r\n```', 'comment_created': datetime.datetime(2022, 3, 22, 17, 57, 32, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 832469188, 'comment_body': 'Same comments as above!', 'comment_created': datetime.datetime(2022, 3, 22, 17, 57, 44, tzinfo=datetime.timezone.utc), 'commenter': 'mattdangerw', 'type': 'User'}, {'comment_id': 832495779, 'comment_body': ""Thanks for the quick review! I've added this"", 'comment_created': datetime.datetime(2022, 3, 22, 18, 26, 4, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}, {'comment_id': 832500168, 'comment_body': ""I've made some changes according to what I could gather would be great if you could have a look again!\r\nThanks!"", 'comment_created': datetime.datetime(2022, 3, 22, 18, 31, 4, tzinfo=datetime.timezone.utc), 'commenter': 'aflah02', 'type': 'User'}]","[{'commit_sha': 'd27d62e2f58543a9c15efcbfdb00475d39b5abdb', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '6f4bbc87777c55218025747e1a738e054ed71367', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e407607fa215110aca430023f9a9dd581edff9c3', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7f4b06e397f9fa263acb32e3093f6f5d2b51f4d6', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd3d15d2629ea2aacdb2ceff9cb7cb8bcee3b72f1', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '7c5d4a7cdf7724b89208323d6c736cfa847cf9f3', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'b04717c5d053745fc464d0f526fb757ea9e89981', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2cac3299ffa9a69e51fce6b6414f798f07d21768', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '15881ba35de769882343be34c7acbe656ed7ee94', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '2cf82aa20c30177b953a66f70842254a22dd5df8', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd13ac7a6f2a35282f1cff4b981db90a77af63afd', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'c754ea6a94155051349cf0e17c56d79a254efa4e', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '5b8789cf63c736740236728c89e679aee6e32796', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '680811c1c435d511eebd1b47dbbdeace5f648566', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'e18c4ff270a21af7e6be4b85ff153c4af9dcaa02', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '190404a4f27bb67fe655bdf8790d3bad0b8f5d82', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': 'd16effb3563d2a593b9fa8b7504fe2d3f790736e', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}, {'commit_sha': '065d871429a8c5f8432f6a0d2c816e285c0376af', 'committer_username': 'aflah02', 'committer_name': 'Aflah', 'committer_email': None, 'commit_date': datetime.datetime(2020, 9, 29, 16, 56, 30, tzinfo=datetime.timezone.utc)}]",Aflah,72096386,,User,,170,,338,219

Project_ID,Name,Full_name,Language,Forks,Stars,Watchers,contributors,commits,issues,branches,PRs_count,contributor pullrequests
267715375,keras-nlp,keras-team/keras-nlp,Python,223,751,30,80,1026,173,24,19,"[{'id': 1045541911, 'number': 340, 'closed': datetime.datetime(2022, 9, 7, 19, 19, 20, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 3, 19, 1, 12, tzinfo=datetime.timezone.utc), 'time_taken': 346688.0, 'time_delta': '4 days, 0:18:08', 'additions': 15, 'deletions': 14, 'state': 'closed'}, {'id': 1045522705, 'number': 339, 'closed': datetime.datetime(2022, 9, 6, 17, 29, 26, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 9, 3, 16, 55, 28, tzinfo=datetime.timezone.utc), 'time_taken': 261238.0, 'time_delta': '3 days, 0:33:58', 'additions': 44, 'deletions': 69, 'state': 'closed'}, {'id': 1024208903, 'number': 293, 'closed': datetime.datetime(2022, 8, 11, 19, 55, 9, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 11, 18, 52, 15, tzinfo=datetime.timezone.utc), 'time_taken': 3774.0, 'time_delta': '1:02:54', 'additions': 32, 'deletions': 24, 'state': 'closed'}, {'id': 1020155132, 'number': 286, 'closed': datetime.datetime(2022, 8, 8, 16, 30, 29, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 8, 11, 16, 2, tzinfo=datetime.timezone.utc), 'time_taken': 18867.0, 'time_delta': '5:14:27', 'additions': 3, 'deletions': 1, 'state': 'closed'}, {'id': 1013415216, 'number': 281, 'closed': datetime.datetime(2022, 8, 17, 17, 58, 49, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 8, 1, 6, 44, 34, tzinfo=datetime.timezone.utc), 'time_taken': 1422855.0, 'time_delta': '16 days, 11:14:15', 'additions': 243, 'deletions': 0, 'state': 'closed'}, {'id': 1002378015, 'number': 274, 'closed': None, 'created': datetime.datetime(2022, 7, 20, 15, 2, 25, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 681, 'deletions': 0, 'state': 'open'}, {'id': 983119568, 'number': 245, 'closed': datetime.datetime(2022, 6, 29, 21, 34, 58, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 6, 29, 21, 12, 31, tzinfo=datetime.timezone.utc), 'time_taken': 1347.0, 'time_delta': '0:22:27', 'additions': 1, 'deletions': 0, 'state': 'closed'}, {'id': 983054859, 'number': 243, 'closed': datetime.datetime(2022, 6, 29, 19, 59, 52, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 6, 29, 19, 54, 46, tzinfo=datetime.timezone.utc), 'time_taken': 306.0, 'time_delta': '0:05:06', 'additions': 5, 'deletions': 5, 'state': 'closed'}, {'id': 979628765, 'number': 235, 'closed': None, 'created': datetime.datetime(2022, 6, 27, 6, 22, 27, tzinfo=datetime.timezone.utc), 'time_taken': 0.0, 'time_delta': '', 'additions': 646, 'deletions': 0, 'state': 'open'}, {'id': 966757546, 'number': 224, 'closed': datetime.datetime(2022, 9, 1, 22, 11, 37, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 6, 14, 10, 41, 21, tzinfo=datetime.timezone.utc), 'time_taken': 6867016.0, 'time_delta': '79 days, 11:30:16', 'additions': 482, 'deletions': 0, 'state': 'closed'}, {'id': 951677691, 'number': 214, 'closed': datetime.datetime(2022, 7, 27, 15, 26, 40, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 5, 31, 8, 40, 9, tzinfo=datetime.timezone.utc), 'time_taken': 4949191.0, 'time_delta': '57 days, 6:46:31', 'additions': 493, 'deletions': 0, 'state': 'closed'}, {'id': 944873546, 'number': 201, 'closed': datetime.datetime(2024, 8, 15, 22, 13, 47, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 5, 23, 20, 23, 8, tzinfo=datetime.timezone.utc), 'time_taken': 70422639.0, 'time_delta': '815 days, 1:50:39', 'additions': 17, 'deletions': 77, 'state': 'closed'}, {'id': 926976950, 'number': 163, 'closed': datetime.datetime(2022, 5, 5, 6, 27, 50, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 5, 3, 20, 20, 32, tzinfo=datetime.timezone.utc), 'time_taken': 122838.0, 'time_delta': '1 day, 10:07:18', 'additions': 77, 'deletions': 0, 'state': 'closed'}, {'id': 926899570, 'number': 161, 'closed': datetime.datetime(2022, 5, 3, 19, 0, 48, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 5, 3, 18, 56, 22, tzinfo=datetime.timezone.utc), 'time_taken': 266.0, 'time_delta': '0:04:26', 'additions': 15, 'deletions': 1, 'state': 'closed'}, {'id': 926884263, 'number': 160, 'closed': datetime.datetime(2022, 5, 3, 18, 40, 35, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 5, 3, 18, 37, 8, tzinfo=datetime.timezone.utc), 'time_taken': 207.0, 'time_delta': '0:03:27', 'additions': 15, 'deletions': 0, 'state': 'closed'}, {'id': 911218503, 'number': 124, 'closed': datetime.datetime(2022, 5, 3, 17, 21, 19, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 4, 16, 7, 10, 55, tzinfo=datetime.timezone.utc), 'time_taken': 1505424.0, 'time_delta': '17 days, 10:10:24', 'additions': 81, 'deletions': 0, 'state': 'closed'}, {'id': 910607969, 'number': 119, 'closed': datetime.datetime(2022, 4, 16, 6, 58, 45, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 4, 15, 6, 1, 59, tzinfo=datetime.timezone.utc), 'time_taken': 89806.0, 'time_delta': '1 day, 0:56:46', 'additions': 709, 'deletions': 2, 'state': 'closed'}, {'id': 903651586, 'number': 100, 'closed': datetime.datetime(2022, 4, 16, 1, 49, 52, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 4, 8, 9, 41, 52, tzinfo=datetime.timezone.utc), 'time_taken': 662880.0, 'time_delta': '7 days, 16:08:00', 'additions': 594, 'deletions': 2, 'state': 'closed'}, {'id': 902416010, 'number': 92, 'closed': datetime.datetime(2022, 4, 7, 20, 20, 25, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 4, 7, 8, 3, 30, tzinfo=datetime.timezone.utc), 'time_taken': 44215.0, 'time_delta': '12:16:55', 'additions': 23, 'deletions': 0, 'state': 'closed'}, {'id': 889849530, 'number': 64, 'closed': datetime.datetime(2022, 3, 28, 18, 42, 24, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 3, 25, 19, 45, 7, tzinfo=datetime.timezone.utc), 'time_taken': 255437.0, 'time_delta': '2 days, 22:57:17', 'additions': 6, 'deletions': 0, 'state': 'closed'}, {'id': 883939607, 'number': 50, 'closed': datetime.datetime(2022, 3, 24, 0, 12, 30, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 3, 19, 8, 51, 57, tzinfo=datetime.timezone.utc), 'time_taken': 400833.0, 'time_delta': '4 days, 15:20:33', 'additions': 105, 'deletions': 10, 'state': 'closed'}]"
261861733,keras-io,keras-team/keras-io,Jupyter Notebook,2023,2723,60,335,1850,125,5,30,"[{'id': 984897941, 'number': 956, 'closed': datetime.datetime(2023, 8, 17, 6, 46, 36, tzinfo=datetime.timezone.utc), 'created': datetime.datetime(2022, 7, 1, 12, 14, 28, tzinfo=datetime.timezone.utc), 'time_taken': 35577128.0, 'time_delta': '411 days, 18:32:08', 'additions': 282, 'deletions': 0, 'state': 'closed'}]"
